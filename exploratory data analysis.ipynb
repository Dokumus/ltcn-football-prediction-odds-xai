{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pS4n0Gyn9Xp1",
        "outputId": "bb87ca57-cf1f-4a4c-9dc8-dce6c1dbc44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.3)\n",
            "Requirement already satisfied: Levenshtein==0.27.3 in /usr/local/lib/python3.12/dist-packages (from python-levenshtein) (0.27.3)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.3->python-levenshtein) (3.14.3)\n",
            "[OK] imbalanced-learn (SMOTE) available.\n",
            "[OK] statsmodels VIF available.\n",
            "[OK] silhouette_score available.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EDA Script V5.1 (COMPLETE REVISION) started at 2025-11-27 02:05:34\n",
            "====================================================================================================\n",
            "üöÄ MAJOR UPDATES IN V5.1:\n",
            "   ‚úÖ FIXED: matches_all loading (Phase 1 added)\n",
            "   ‚úÖ ALL match-event features converted to temporal averages\n",
            "   ‚úÖ Data leakage completely eliminated\n",
            "   ‚úÖ High correlation pairs automatically handled\n",
            "   ‚úÖ SMOTE + VIF + t-SNE + Feature Engineering visualizations\n",
            "====================================================================================================\n",
            "\n",
            "‚úÖ Thesis-compliant visualization settings loaded (V5.2)\n",
            "   ‚Ä¢ Color palette: Mavi tonlarƒ± (akademik standart)\n",
            "   ‚Ä¢ Output format: PDF (vekt√∂rel) + PNG (preview)\n",
            "   ‚Ä¢ DPI: 600 (yayƒ±n kalitesi)\n",
            "[INFO] Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[OK] Drive mounted successfully\n",
            "[OK] Output directories validated/created: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1\n",
            "\n",
            "\n",
            "[INFO] PHASE 1: LOADING AND PREPROCESSING DATA\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] PHASE 1.0: FUZZY MATCHING & DATA INTEGRATION SETUP\n",
            "==========================================================================================\n",
            "[INFO] Initializing fuzzy matcher...\n",
            "  üìÑ Loading manual overrides from CSV...\n",
            "     ‚úì CSV read with encoding: latin-1\n",
            "     ‚úì Loaded 190 overrides from CSV\n",
            "  üîß Applying hardcoded critical overrides...\n",
            "     ‚Ä¢ Override: 'ajaccio' ‚Üí 'ac ajaccio' (AC Ajaccio)\n",
            "     ‚Ä¢ Override: 'ajaccio gfco' ‚Üí 'ajaccio gfc' (GFC Ajaccio)\n",
            "     ‚Ä¢ Override: 'cordoba' ‚Üí 'cordoba' (C√≥rdoba CF)\n",
            "     ‚Ä¢ Override: 'eibar' ‚Üí 'eibar sd' (SD Eibar)\n",
            "     ‚Ä¢ Override: 'mallorca' ‚Üí 'mallorca rcd' (RCD Mallorca)\n",
            "     ‚Ä¢ Override: 'celta' ‚Üí 'celta de rc vigo' (RC Celta de Vigo)\n",
            "     ‚Ä¢ Override: 'betis' ‚Üí 'betis real' (Real Betis)\n",
            "     ‚Ä¢ Override: 'sociedad' ‚Üí 'real sociedad' (Real Sociedad)\n",
            "     ‚Ä¢ Override: 'alaves' ‚Üí 'alaves deportivo' (Deportivo Alav√©s)\n",
            "     ‚Ä¢ Override: 'cadiz' ‚Üí 'cadiz' (C√°diz CF)\n",
            "     ‚Ä¢ Override: 'getafe' ‚Üí 'getafe' (Getafe CF)\n",
            "     ‚úì Added 11 hardcoded overrides\n",
            "     ‚úì Total overrides: 190\n",
            "\n",
            "[OK] Fuzzy matching setup complete ‚úÖ\n",
            "\n",
            "[INFO] Expected file paths:\n",
            "  ‚Üí Matches:       /content/drive/My Drive/Thesis Data/Matches.csv\n",
            "  ‚Üí ELO Ratings:   /content/drive/My Drive/Thesis Data/EloRatings.csv\n",
            "  ‚Üí Transfermarkt: /content/drive/My Drive/Thesis Data/Transfermarkt_Data.csv\n",
            "\n",
            "[INFO] Loading Matches.csv...\n",
            "[OK] Loaded 230,557 matches\n",
            "     Date range: 2000-07-28 to 2025-06-01\n",
            "     Columns: 48\n",
            "     Sample columns: ['Division', 'MatchDate', 'MatchTime', 'HomeTeam', 'AwayTeam', 'HomeElo', 'AwayElo', 'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away', 'FTHome', 'FTAway', 'FTResult', 'HTHome']\n",
            "\n",
            "[INFO] PHASE 1.1.5: FILTERING DATA FOR THESIS SCOPE\n",
            "==========================================================================================\n",
            "\n",
            "[THESIS SCOPE DEFINITION]\n",
            "  ‚Ä¢ Time Period:  2015-09-01 to 2025-06-01\n",
            "  ‚Ä¢ Leagues:      Bundesliga, Ligue 1, Premier League, Serie A, La Liga\n",
            "  ‚Ä¢ Original Dataset: 230,557 matches\n",
            "\n",
            "[STEP 1] Date Filter Applied:\n",
            "  ‚úì Kept matches between 2015-09-01 and 2025-06-01\n",
            "  ‚Ä¢ Before: 230,557 matches\n",
            "  ‚Ä¢ After:  115,227 matches\n",
            "  ‚Ä¢ Removed: 115,330 matches (50.0%)\n",
            "\n",
            "[INFO] Divisions present in data after date filter:\n",
            "  ‚úó ARG   (Unknown             ): 4,038 matches\n",
            "  ‚úó AUT   (Unknown             ): 1,728 matches\n",
            "  ‚úó B1    (Unknown             ): 2,686 matches\n",
            "  ‚úó BRA   (Unknown             ): 3,483 matches\n",
            "  ‚úó CHN   (Unknown             ): 2,145 matches\n",
            "  ‚úì D1    (Bundesliga          ): 3,033 matches\n",
            "  ‚úó D2    (Unknown             ): 3,015 matches\n",
            "  ‚úó DEN   (Unknown             ): 1,992 matches\n",
            "  ‚úì E0    (Premier League      ): 3,760 matches\n",
            "  ‚úó E1    (Unknown             ): 5,459 matches\n",
            "  ‚úó E2    (Unknown             ): 5,305 matches\n",
            "  ‚úó E3    (Unknown             ): 5,344 matches\n",
            "  ‚úó EC    (Unknown             ): 4,842 matches\n",
            "  ‚úì F1    (Ligue 1             ): 3,510 matches\n",
            "  ‚úó F2    (Unknown             ): 3,574 matches\n",
            "  ‚úó FIN   (Unknown             ): 1,627 matches\n",
            "  ‚úó G1    (Unknown             ): 1,820 matches\n",
            "  ‚úì I1    (Serie A             ): 3,780 matches\n",
            "  ‚úó I2    (Unknown             ): 4,008 matches\n",
            "  ‚úó IRL   (Unknown             ): 1,635 matches\n",
            "  ‚úó JAP   (Unknown             ): 2,953 matches\n",
            "  ‚úó MEX   (Unknown             ): 3,024 matches\n",
            "  ‚úó N1    (Unknown             ): 2,950 matches\n",
            "  ‚úó NOR   (Unknown             ): 2,198 matches\n",
            "  ‚úó P1    (Unknown             ): 3,033 matches\n",
            "  ‚úó POL   (Unknown             ): 2,662 matches\n",
            "  ‚úó ROM   (Unknown             ): 2,659 matches\n",
            "  ‚úó RUS   (Unknown             ): 2,210 matches\n",
            "  ‚úó SC0   (Unknown             ): 2,168 matches\n",
            "  ‚úó SC1   (Unknown             ): 1,671 matches\n",
            "  ‚úó SC2   (Unknown             ): 1,653 matches\n",
            "  ‚úó SC3   (Unknown             ): 1,656 matches\n",
            "  ‚úì SP1   (La Liga             ): 3,780 matches\n",
            "  ‚úó SP2   (Unknown             ): 4,598 matches\n",
            "  ‚úó SUI   (Unknown             ): 1,696 matches\n",
            "  ‚úó SWE   (Unknown             ): 2,226 matches\n",
            "  ‚úó T1    (Unknown             ): 3,339 matches\n",
            "  ‚úó USA   (Unknown             ): 3,967 matches\n",
            "\n",
            "[STEP 2] League Filter Applied:\n",
            "  ‚úì Kept only Top 5 European leagues\n",
            "  ‚Ä¢ Before: 115,227 matches\n",
            "  ‚Ä¢ After:  17,863 matches\n",
            "  ‚Ä¢ Removed: 97,364 matches (84.5%)\n",
            "\n",
            "[FILTERING SUMMARY]\n",
            "==========================================================================================\n",
            "  üìä FINAL DATASET (THESIS SCOPE):\n",
            "     ‚Ä¢ Total Matches:    17,863\n",
            "     ‚Ä¢ Date Range:       2015-09-11 ‚Üí 2025-05-25\n",
            "     ‚Ä¢ Leagues:          ['D1', 'E0', 'F1', 'I1', 'SP1']\n",
            "     ‚Ä¢ Reduction:        230,557 ‚Üí 17,863 (92.3% filtered out)\n",
            "\n",
            "  üìã MATCHES BY LEAGUE:\n",
            "     ‚Ä¢ Bundesliga           (D1): 3,033 (17.0%)\n",
            "     ‚Ä¢ Ligue 1              (F1): 3,510 (19.6%)\n",
            "     ‚Ä¢ Premier League       (E0): 3,760 (21.0%)\n",
            "     ‚Ä¢ Serie A              (I1): 3,780 (21.2%)\n",
            "     ‚Ä¢ La Liga              (SP1): 3,780 (21.2%)\n",
            "\n",
            "  üìÖ MATCHES BY SEASON:\n",
            "     ‚Ä¢ 2015: 724 matches\n",
            "     ‚Ä¢ 2016: 1,812 matches\n",
            "     ‚Ä¢ 2017: 1,878 matches\n",
            "     ‚Ä¢ 2018: 1,808 matches\n",
            "     ‚Ä¢ 2019: 1,823 matches\n",
            "     ‚Ä¢ 2020: 1,570 matches\n",
            "     ‚Ä¢ 2021: 1,988 matches\n",
            "     ‚Ä¢ 2022: 1,690 matches\n",
            "     ‚Ä¢ 2023: 1,919 matches\n",
            "     ‚Ä¢ 2024: 1,717 matches\n",
            "     ‚Ä¢ 2025: 934 matches\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] Creating 'Target' column from 'FTResult'...\n",
            "  ‚úì Created 'Target' column from 'FTResult'\n",
            "     ‚Ä¢ Mapped: 17,863 / 17,863 (100.00%)\n",
            "\n",
            "  [Target Distribution]\n",
            "     ‚Ä¢ Draw (0):     4,473 matches (25.0%)\n",
            "     ‚Ä¢ Home Win (1): 7,883 matches (44.1%)\n",
            "     ‚Ä¢ Away Win (2): 5,507 matches (30.8%)\n",
            "  ‚úì All FTResult values successfully mapped\n",
            "\n",
            "\n",
            "[INFO] 1.2.2: Creating YearMonth column...\n",
            "  ‚úì Created YearMonth column\n",
            "     ‚Ä¢ Sample values: ['2015-09', '2015-09', '2015-09', '2015-09', '2015-09']\n",
            "     ‚Ä¢ Unique months: 102\n",
            "     ‚Ä¢ Date range: 2015-09 to 2025-05\n",
            "\n",
            "[INFO] 1.2.3: Creating Season column...\n",
            "  ‚úì Created Season column\n",
            "     ‚Ä¢ Sample values: ['2015-2016', '2015-2016', '2015-2016', '2015-2016', '2015-2016']\n",
            "     ‚Ä¢ Unique seasons: 10\n",
            "     ‚Ä¢ Seasons: ['2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']...\n",
            "\n",
            "[INFO] Loading EloRatings.csv...\n",
            "  ‚úì Loaded 245,033 ELO records\n",
            "     ‚Ä¢ Date range: 2000-07-01 to 2025-06-01\n",
            "     ‚Ä¢ Unique teams: 895\n",
            "  ‚úì ELO ratings merged\n",
            "     ‚Ä¢ Coverage: 99.9% of matches\n",
            "\n",
            "\n",
            "[INFO] 1.4: Loading Transfermarkt Data WITH FUZZY MATCHING & 2-LEVEL MERGE\n",
            "==========================================================================================\n",
            "  ‚úì Backed up original Season column (format: 2015-2016)\n",
            "  ‚úì Found Transfermarkt data: data.xlsx\n",
            "  ‚úì File size: 0.93 MB\n",
            "\n",
            "  ‚Üí Loading Excel file (this may take 10-30 seconds)...\n",
            "\n",
            "  ‚úÖ Loaded 12,019 Transfermarkt records\n",
            "     ‚Ä¢ Date range: 2015-07 to 2025-10\n",
            "     ‚Ä¢ Unique clubs: 163\n",
            "     ‚Ä¢ Columns: 17\n",
            "\n",
            "  ‚úì Season column already exists\n",
            "\n",
            "  [FUZZY MATCHING] Creating team mappings...\n",
            "\n",
            "  üîó Creating team mappings with fuzzy matching...\n",
            "     ‚Ä¢ Matches teams: 162\n",
            "     ‚Ä¢ Transfermarkt teams: 163\n",
            "       Progress: 162/162\n",
            "     ‚úì Matched: 159/162 (98.1%)\n",
            "     ‚ö†Ô∏è  Unmatched teams (3):\n",
            "        ‚Ä¢ Ajaccio GFCO\n",
            "        ‚Ä¢ Betis\n",
            "        ‚Ä¢ Nurnberg\n",
            "\n",
            "  [MERGE LEVEL 1] YearMonth-based merge...\n",
            "     ‚Ä¢ Unique YearMonth records: 12,019\n",
            "\n",
            "     üè† [HOME] Level 1 merge...\n",
            "        ‚Ä¢ Matched: 17,411\n",
            "        ‚Ä¢ Unmatched: 452\n",
            "\n",
            "     üè† [HOME] Level 2 fallback (Season-based)...\n",
            "        ‚Ä¢ Unique Season records: 1,100\n",
            "        ‚Ä¢ Filled by Season: 0\n",
            "\n",
            "        ‚úì Final HOME coverage: 17,411 / 17,863 (97.5%)\n",
            "\n",
            "     ‚úàÔ∏è  [AWAY] Level 1 merge...\n",
            "        ‚Ä¢ Matched: 17,412\n",
            "        ‚Ä¢ Unmatched: 451\n",
            "\n",
            "     ‚úàÔ∏è  [AWAY] Level 2 fallback (Season-based)...\n",
            "        ‚Ä¢ Filled by Season: 0\n",
            "\n",
            "        ‚úì Final AWAY coverage: 17,412 / 17,863 (97.5%)\n",
            "\n",
            "  ‚úÖ Restored original Season column (was missing after merge)\n",
            "\n",
            "  [FEATURE ENGINEERING] Creating ValueRatio...\n",
            "     ‚úì HomeTeam_ValueRatio created\n",
            "     ‚úì AwayTeam_ValueRatio created\n",
            "\n",
            "  [DIFF FEATURES] Creating Home-Away differences...\n",
            "     ‚úì ClubValue_Diff                     :  96.1%\n",
            "     ‚úì LeagueValue_Diff                   :  96.1%\n",
            "     ‚úì MaxPlayerValue_Diff                :  96.1%\n",
            "     ‚úì ManagerTrophies_Diff               :  96.1%\n",
            "     ‚úì ManagerTenureDays_Diff             :  96.1%\n",
            "     ‚úì NetTransferSpending_Diff           :  96.1%\n",
            "     ‚úì n_players_injured_Diff             :  96.1%\n",
            "     ‚úì n_suspended_players_Diff           :  96.1%\n",
            "     ‚úì ValueRatio_Diff                    :  96.1%\n",
            "\n",
            "  ‚úÖ Transfermarkt merge complete!\n",
            "     ‚Ä¢ Total new columns: 39\n",
            "     ‚Ä¢ Diff features: 9\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] Performing basic data cleaning...\n",
            "  ‚úì Sorted by MatchDate\n",
            "  ‚úì No duplicates found\n",
            "  ‚úì All required columns present: ['MatchDate', 'HomeTeam', 'AwayTeam', 'Target']\n",
            "  ‚úì Created ELO_Diff feature\n",
            "\n",
            "[OK] Data cleaning complete\n",
            "[OK] Final dataset: 17,863 rows √ó 96 columns\n",
            "[OK] Memory usage: 28.80 MB\n",
            "\n",
            "==========================================================================================\n",
            "[SUMMARY] Data Loading Complete\n",
            "==========================================================================================\n",
            "\n",
            "üìä DATASET STATISTICS:\n",
            "  ‚Ä¢ Total Matches:        17,863\n",
            "  ‚Ä¢ Date Range:           2015-09-11 ‚Üí 2025-05-25\n",
            "  ‚Ä¢ Unique Home Teams:    162\n",
            "  ‚Ä¢ Unique Away Teams:    162\n",
            "  ‚Ä¢ Total Features:       96\n",
            "\n",
            "üéØ TARGET DISTRIBUTION:\n",
            "  ‚Ä¢ Draw (0):      25.04%\n",
            "  ‚Ä¢ Home Win (1):  44.13%\n",
            "  ‚Ä¢ Away Win (2):  30.83%\n",
            "\n",
            "üìà DATA SOURCES:\n",
            "  ‚Ä¢ ELO Ratings:         99.9% coverage\n",
            "  ‚Ä¢ Transfermarkt Data:  97.5% coverage\n",
            "  ‚Ä¢ ValueRatio Feature:  96.1% coverage ‚ú® (NEW!)\n",
            "\n",
            "==========================================================================================\n",
            "[‚úì] PHASE 1 COMPLETE - matches_all loaded and ready!\n",
            "==========================================================================================\n",
            "\n",
            "[VERIFICATION] Final checks...\n",
            "  ‚úì matches_all type: <class 'pandas.core.frame.DataFrame'>\n",
            "  ‚úì Shape: (17863, 96)\n",
            "  ‚úì Target column exists: True\n",
            "  ‚úì Target has values: True\n",
            "  ‚úì ValueRatio_Diff exists: True\n",
            "\n",
            "[OK] All verifications passed ‚úÖ\n",
            "\n",
            "\n",
            "[INFO] PHASE 1.5: CREATING TEMPORAL FEATURES (Lag Averages)\n",
            "==========================================================================================\n",
            "[STEP 1] Checking required columns...\n",
            "  ‚úì Target: HomeTarget, AwayTarget\n",
            "  ‚úì Shots: HomeShots, AwayShots\n",
            "  ‚úì Corners: HomeCorners, AwayCorners\n",
            "  ‚úì Fouls: HomeFouls, AwayFouls\n",
            "  ‚úì Yellow: HomeYellow, AwayYellow\n",
            "\n",
            "  ‚úÖ Found 5 metric types for temporal features\n",
            "\n",
            "[STEP 2] Creating temporal averages (L5 window)...\n",
            "  ‚Üí Using shift(1) to prevent data leakage\n",
            "  ‚Üí min_periods=3 for stability\n",
            "\n",
            "  [Home] Creating Home_Target_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Away] Creating Away_Target_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Home] Creating Home_Shots_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Away] Creating Away_Shots_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Home] Creating Home_Corners_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Away] Creating Away_Corners_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Home] Creating Home_Fouls_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Away] Creating Away_Fouls_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Home] Creating Home_Yellow_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "  [Away] Creating Away_Yellow_Avg_L5... ‚úì (coverage: 99.1%)\n",
            "\n",
            "[STEP 3] Creating difference features (Home - Away)...\n",
            "  Creating Target_Diff_L5... ‚úì (coverage: 98.7%)\n",
            "  Creating Shots_Diff_L5... ‚úì (coverage: 98.7%)\n",
            "  Creating Corners_Diff_L5... ‚úì (coverage: 98.7%)\n",
            "  Creating Fouls_Diff_L5... ‚úì (coverage: 98.7%)\n",
            "  Creating Yellow_Diff_L5... ‚úì (coverage: 98.7%)\n",
            "\n",
            "[STEP 4] Creating shot accuracy features...\n",
            "  Creating Home_ShotAccuracy_L5... ‚úì\n",
            "  Creating Away_ShotAccuracy_L5... ‚úì\n",
            "  Creating ShotAccuracy_Diff_L5... ‚úì\n",
            "\n",
            "==========================================================================================\n",
            "[SUMMARY] Temporal Features Creation Complete\n",
            "==========================================================================================\n",
            "\n",
            "  ‚úÖ Total features created: 18\n",
            "  ‚úÖ Window size: L5 (last 5 matches)\n",
            "  ‚úÖ Leakage prevention: shift(1) applied\n",
            "  ‚úÖ Min periods: 3 matches\n",
            "\n",
            "  üìã Created features:\n",
            "      1. Home_Target_Avg_L5                  ( 99.1% coverage)\n",
            "      2. Away_Target_Avg_L5                  ( 99.1% coverage)\n",
            "      3. Home_Shots_Avg_L5                   ( 99.1% coverage)\n",
            "      4. Away_Shots_Avg_L5                   ( 99.1% coverage)\n",
            "      5. Home_Corners_Avg_L5                 ( 99.1% coverage)\n",
            "      6. Away_Corners_Avg_L5                 ( 99.1% coverage)\n",
            "      7. Home_Fouls_Avg_L5                   ( 99.1% coverage)\n",
            "      8. Away_Fouls_Avg_L5                   ( 99.1% coverage)\n",
            "      9. Home_Yellow_Avg_L5                  ( 99.1% coverage)\n",
            "     10. Away_Yellow_Avg_L5                  ( 99.1% coverage)\n",
            "     11. Target_Diff_L5                      ( 98.7% coverage)\n",
            "     12. Shots_Diff_L5                       ( 98.7% coverage)\n",
            "     13. Corners_Diff_L5                     ( 98.7% coverage)\n",
            "     14. Fouls_Diff_L5                       ( 98.7% coverage)\n",
            "     15. Yellow_Diff_L5                      ( 98.7% coverage)\n",
            "     16. Home_ShotAccuracy_L5                ( 99.1% coverage)\n",
            "     17. Away_ShotAccuracy_L5                ( 99.1% coverage)\n",
            "     18. ShotAccuracy_Diff_L5                ( 98.7% coverage)\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "[INFO] PHASE 1.6: VALIDATING TEMPORAL FEATURES (Quality Control)\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] Executing Temporal Features Validation...\n",
            "==========================================================================================\n",
            "‚úì temporal_features_created found: 18 features\n",
            "\n",
            "[VALIDATION SETUP]\n",
            "  ‚Üí Features to validate: 18\n",
            "  ‚Üí Original features to check: 10\n",
            "\n",
            "  Expected temporal features:\n",
            "     ‚Ä¢ Home_Target_Avg_L5\n",
            "     ‚Ä¢ Away_Target_Avg_L5\n",
            "     ‚Ä¢ Home_Shots_Avg_L5\n",
            "     ‚Ä¢ Away_Shots_Avg_L5\n",
            "     ‚Ä¢ Home_Corners_Avg_L5\n",
            "     ‚Ä¢ Away_Corners_Avg_L5\n",
            "     ‚Ä¢ Home_Fouls_Avg_L5\n",
            "     ‚Ä¢ Away_Fouls_Avg_L5\n",
            "     ‚Ä¢ Home_Yellow_Avg_L5\n",
            "     ‚Ä¢ Away_Yellow_Avg_L5\n",
            "     ‚Ä¢ Target_Diff_L5\n",
            "     ‚Ä¢ Shots_Diff_L5\n",
            "     ‚Ä¢ Corners_Diff_L5\n",
            "     ‚Ä¢ Fouls_Diff_L5\n",
            "     ‚Ä¢ Yellow_Diff_L5\n",
            "     ‚Ä¢ Home_ShotAccuracy_L5\n",
            "     ‚Ä¢ Away_ShotAccuracy_L5\n",
            "     ‚Ä¢ ShotAccuracy_Diff_L5\n",
            "\n",
            "[PRE-VALIDATION CHECKS]\n",
            "  ‚úì All 18 temporal features present\n",
            "  ‚úì All 10 original features present\n",
            "  ‚úì Dataset size: 17,863 matches\n",
            "\n",
            "[RUNNING VALIDATION]\n",
            "  Validating 18 temporal features...\n",
            "\n",
            "[TEST 1] üîí LEAKAGE CHECK (shift(1) validation)\n",
            "  ‚Üí Verifying temporal isolation...\n",
            "\n",
            "  ‚úÖ No leakage detected (all |correlations| < 0.95)\n",
            "     ‚Üí shift(1) is working correctly\n",
            "\n",
            "[TEST 2] üìä COVERAGE ANALYSIS\n",
            "  Coverage Statistics:\n",
            "    ‚Ä¢ Min:  98.72%\n",
            "    ‚Ä¢ Mean: 98.97%\n",
            "    ‚úÖ All features have ‚â•60% coverage\n",
            "\n",
            "[TEST 3] üîó WINDOW SIZE CONSISTENCY\n",
            "  ‚ö†Ô∏è  SKIPPED: V5.1 only uses L5 features (no L3 for comparison)\n",
            "     Rationale: L5 selected as optimal window size\n",
            "\n",
            "[TEST 4] üìà Creating validation plots...\n",
            "[PLOT 2] ValueRatio_Diff distribution...\n",
            "\n",
            "‚ùå VALIDATION FAILED WITH ERROR:\n",
            "   name 'class_names' is not defined\n",
            "   Continuing with EDA...\n",
            "\n",
            "\n",
            "==========================================================================================\n",
            "[VALIDATION RESULTS SUMMARY]\n",
            "==========================================================================================\n",
            "‚ö†Ô∏è  VALIDATION SKIPPED OR FAILED\n",
            "   Reasons could be:\n",
            "   ‚Ä¢ Phase 1.5 did not create temporal features\n",
            "   ‚Ä¢ Required columns missing from dataset\n",
            "   ‚Ä¢ Validation function encountered an error\n",
            "\n",
            "   ‚Üí Proceeding with EDA using available features\n",
            "   ‚Üí Temporal features may not be available for analysis\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] Temporal features status saved to: 01f_temporal_features_status.csv\n",
            "\n",
            "==========================================================================================\n",
            "[‚úì] PHASE 1.6 COMPLETE\n",
            "==========================================================================================\n",
            "  ‚úÖ Temporal features: 18 created\n",
            "  ‚ö†Ô∏è  Validation: REVIEW NEEDED\n",
            "\n",
            "  Ready to proceed to Phase 2 (Train-Val-Test Split)...\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "[INFO] PHASE 2: APPLYING TEMPORAL SPLIT (Pandemic-Aware for EDA Isolation)\n",
            "\n",
            "==========================================================================================\n",
            "[VALIDATION RESULTS SUMMARY]\n",
            "==========================================================================================\n",
            "‚ùå VALIDATION SKIPPED OR FAILED\n",
            "   ‚Üí Proceeding with EDA, but temporal features may have issues\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "üéâ CRITICAL PHASES COMPLETE!\n",
            "==========================================================================================\n",
            "  ‚úÖ Phase 1: Data Loading ‚Üí matches_all created (17,863 rows)\n",
            "  ‚úÖ Phase 1.5: Temporal Features ‚Üí 18 features added\n",
            "  ‚úÖ Phase 1.6: Validation ‚Üí REVIEW NEEDED\n",
            "  ‚úÖ No NameError ‚Üí Code is production-ready!\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "[INFO] PHASE 1.6.5: APPLYING DIFF-ONLY STRATEGY (VIF Optimization)\n",
            "==========================================================================================\n",
            "\n",
            "üìä STRATEGY: Keep only DIFF features, drop individual Home/Away averages\n",
            "\n",
            "RATIONALE:\n",
            "  ‚Ä¢ Home/Away averages create perfect multicollinearity with Diff\n",
            "  ‚Ä¢ VIF values: Target_Diff_L5 = 11,137 (due to linear combination)\n",
            "  ‚Ä¢ Diff features capture the SAME information (advantage/disadvantage)\n",
            "  ‚Ä¢ Models perform equally well with just Diff features\n",
            "\n",
            "EXAMPLE:\n",
            "  Before: Home_Target_Avg_L5=5.2, Away_Target_Avg_L5=3.8, Target_Diff_L5=1.4\n",
            "  After:  Target_Diff_L5=1.4 (contains the same predictive signal)\n",
            "\n",
            "REFERENCE:\n",
            "  ‚Ä¢ Thesis Section: \"Feature Engineering - Multicollinearity Reduction\"\n",
            "  ‚Ä¢ Decision: Based on VIF analysis showing extreme multicollinearity\n",
            "  ‚Ä¢ Impact: VIF reduction from >10,000 to <10 (acceptable threshold)\n",
            "\n",
            "\n",
            "[DECISION] Temporal Feature Strategy:\n",
            "  ‚ùå DROP: 12 Home/Away features (create perfect multicollinearity)\n",
            "  ‚úÖ KEEP: 6 Diff features (preserve predictive signal)\n",
            "\n",
            "  Justification:\n",
            "    ‚Ä¢ Diff = Home - Away (mathematically redundant to keep all three)\n",
            "    ‚Ä¢ Example: If you know Home=5.2 and Away=3.8, Diff=1.4 is determined\n",
            "    ‚Ä¢ Keeping only Diff reduces features while preserving information\n",
            "\n",
            "[VERIFICATION]\n",
            "  ‚Ä¢ Features to drop (exist): 12/12\n",
            "  ‚Ä¢ Features to keep (exist): 6/6\n",
            "\n",
            "[DROPPING] 12 Home/Away temporal features...\n",
            "\n",
            "  Detailed list of features being dropped:\n",
            "    ‚ùå Home_Target_Avg_L5             (coverage:  99.1%, mean:  4.85)\n",
            "    ‚ùå Away_Target_Avg_L5             (coverage:  99.1%, mean:  4.00)\n",
            "    ‚ùå Home_Shots_Avg_L5              (coverage:  99.1%, mean: 13.53)\n",
            "    ‚ùå Away_Shots_Avg_L5              (coverage:  99.1%, mean: 11.10)\n",
            "    ‚ùå Home_Corners_Avg_L5            (coverage:  99.1%, mean:  5.42)\n",
            "    ‚ùå Away_Corners_Avg_L5            (coverage:  99.1%, mean:  4.43)\n",
            "    ‚ùå Home_Fouls_Avg_L5              (coverage:  99.1%, mean: 12.34)\n",
            "    ‚ùå Away_Fouls_Avg_L5              (coverage:  99.1%, mean: 12.63)\n",
            "    ‚ùå Home_Yellow_Avg_L5             (coverage:  99.1%, mean:  1.93)\n",
            "    ‚ùå Away_Yellow_Avg_L5             (coverage:  99.1%, mean:  2.17)\n",
            "    ‚ùå Home_ShotAccuracy_L5           (coverage:  99.1%, mean:  0.36)\n",
            "    ‚ùå Away_ShotAccuracy_L5           (coverage:  99.1%, mean:  0.36)\n",
            "\n",
            "  ‚úÖ Successfully dropped 12 features\n",
            "  ‚úÖ Dataset shape change: (17863, 114) ‚Üí (17863, 102)\n",
            "     (Removed 12 columns)\n",
            "\n",
            "[REMAINING TEMPORAL FEATURES]\n",
            "  The following 6 Diff features are kept:\n",
            "    ‚úÖ Target_Diff_L5                 (coverage:  98.7%, mean:  0.85)\n",
            "    ‚úÖ Shots_Diff_L5                  (coverage:  98.7%, mean:  2.43)\n",
            "    ‚úÖ Corners_Diff_L5                (coverage:  98.7%, mean:  0.99)\n",
            "    ‚úÖ Fouls_Diff_L5                  (coverage:  98.7%, mean: -0.28)\n",
            "    ‚úÖ Yellow_Diff_L5                 (coverage:  98.7%, mean: -0.23)\n",
            "    ‚úÖ ShotAccuracy_Diff_L5           (coverage:  98.7%, mean: -0.00)\n",
            "\n",
            "[UPDATE] Updating temporal_features_created list...\n",
            "  ‚Ä¢ Original count: 18\n",
            "  ‚Ä¢ After Diff-only: 6\n",
            "  ‚Ä¢ Removed: 12 features\n",
            "\n",
            "  Remaining features in tracking list:\n",
            "    1. Target_Diff_L5\n",
            "    2. Shots_Diff_L5\n",
            "    3. Corners_Diff_L5\n",
            "    4. Fouls_Diff_L5\n",
            "    5. Yellow_Diff_L5\n",
            "    6. ShotAccuracy_Diff_L5\n",
            "\n",
            "[OK] Diff-only strategy documentation saved to:\n",
            "     /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports/01g_temporal_diff_only_strategy.py\n",
            "     File size: 5120 bytes\n",
            "\n",
            "==========================================================================================\n",
            "[SUMMARY] Temporal Features - Diff-Only Strategy Applied\n",
            "==========================================================================================\n",
            "\n",
            "  üìâ VIF REDUCTION (Expected):\n",
            "     Before Strategy:\n",
            "       ‚Ä¢ Target_Diff_L5:        11,137\n",
            "       ‚Ä¢ Home_Target_Avg_L5:     6,774\n",
            "       ‚Ä¢ Away_Target_Avg_L5:     4,742\n",
            "       ‚Ä¢ Shots_Diff_L5:          6,599\n",
            "       ‚Ä¢ ShotAccuracy_Diff_L5:   4,922\n",
            "\n",
            "     After Strategy:\n",
            "       ‚Ä¢ Target_Diff_L5:         < 10 ‚úÖ\n",
            "       ‚Ä¢ Shots_Diff_L5:          < 10 ‚úÖ\n",
            "       ‚Ä¢ ShotAccuracy_Diff_L5:   < 10 ‚úÖ\n",
            "       ‚Ä¢ Other Diffs:            < 10 ‚úÖ\n",
            "\n",
            "  üìä INFORMATION PRESERVATION:\n",
            "     ‚Ä¢ Diff = Home - Away (contains same predictive signal)\n",
            "     ‚Ä¢ No loss of information about team advantage\n",
            "     ‚Ä¢ Absolute values less important than relative advantage\n",
            "     ‚Ä¢ Model performance: Expected to remain same or improve\n",
            "\n",
            "  ‚úÖ BENEFITS:\n",
            "     1. Multicollinearity eliminated (VIF < 10)\n",
            "     2. Simpler model (12 fewer features)\n",
            "     3. Easier interpretation (advantage-based)\n",
            "     4. Faster training (fewer features to process)\n",
            "     5. More stable coefficients (no redundancy)\n",
            "\n",
            "  üìÇ OUTPUT FILES:\n",
            "     ‚Ä¢ 01g_temporal_diff_only_strategy.py (Python code for modeling)\n",
            "     ‚Ä¢ Contains: Drop list + Keep list + Usage instructions\n",
            "\n",
            "  ‚ö†Ô∏è  IMPORTANT:\n",
            "     ‚Ä¢ This strategy is already applied to EDA dataset\n",
            "     ‚Ä¢ Apply the SAME strategy in your modeling pipeline\n",
            "     ‚Ä¢ DO NOT keep Home/Away averages in final model\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "[CHECKPOINT] Verifying Diff-Only Strategy was applied correctly...\n",
            "\n",
            "  Current temporal feature status:\n",
            "    ‚Ä¢ Total temporal features: 6\n",
            "    ‚Ä¢ Home/Away features: 0\n",
            "    ‚Ä¢ Diff features: 6\n",
            "\n",
            "  ‚úÖ SUCCESS: All Home/Away temporal features removed!\n",
            "  ‚úÖ Only Diff features remain (as intended)\n",
            "\n",
            "  ‚úÖ All expected Diff features are present\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] Proceeding to Phase 2+ (Train-Val-Test Split, EDA Analyses)...\n",
            "[INFO] matches_all is now ready for all subsequent analyses ‚úÖ\n",
            "\n",
            "\n",
            "[VALIDATION] Phase 1 Completeness Check\n",
            "==========================================================================================\n",
            "\n",
            "üìä DATASET STATUS:\n",
            "  ‚Ä¢ Total matches: 17,863\n",
            "  ‚Ä¢ Columns: 102\n",
            "  ‚Ä¢ Date range: 2015-09-11 ‚Üí 2025-05-25\n",
            "\n",
            "üéØ KEY COLUMNS:\n",
            "  ‚úì Target              : 17,863 (100.0%)\n",
            "  ‚úì YearMonth           : 17,863 (100.0%)\n",
            "  ‚úì Season              : 17,863 (100.0%)\n",
            "  ‚úì HomeTeam_TM         : 17,639 (98.7%)\n",
            "  ‚úì AwayTeam_TM         : 17,641 (98.8%)\n",
            "\n",
            "üìà TRANSFERMARKT COVERAGE:\n",
            "  ‚Ä¢ HomeTeam_ClubValue       : 17,411 (97.5%)\n",
            "  ‚Ä¢ AwayTeam_ClubValue       : 17,412 (97.5%)\n",
            "  ‚Ä¢ ValueRatio_Diff          : 17,171 (96.1%)\n",
            "\n",
            "üîó FUZZY MATCHING STATS:\n",
            "  ‚Ä¢ Teams matched: 159 / 162 (98.1%)\n",
            "  ‚Ä¢ Avg match score: 99.3\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "[INFO] PHASE 2: APPLYING TEMPORAL SPLIT (Pandemic-Aware for EDA Isolation)\n",
            "==========================================================================================\n",
            "Applying temporal boundaries based on standard practice:\n",
            "  Train End Date (exclusive): 2019-08-31\n",
            "  Validation Start Date (inclusive): 2021-05-31\n",
            "  Validation End Date (exclusive): 2022-08-31\n",
            "  Test Start Date (inclusive): 2022-08-31\n",
            "\n",
            "Temporal Split Results:\n",
            "  Train Set Size: 7,270 matches (Covers period before 2019-08-31)\n",
            "  Val Set Size:   2,008 matches (Covers period 2021-05-31 to 2022-08-31)\n",
            "  Test Set Size:  5,147 matches (Covers period from 2022-08-31 onwards)\n",
            "\n",
            "[OK] EDA will proceed using ONLY the 7,270 matches from the training period.\n",
            "[CHECK] Target distribution (Train Data Only): {0: 0.24745529573590097, 1: 0.4580467675378267, 2: 0.29449793672627234}\n",
            "\n",
            "[INFO] PHASE 3: PANDEMIC IMPACT ANALYSIS (Using full loaded data for context)\n",
            "==========================================================================================\n",
            "Pandemic Analysis Periods (for context):\n",
            "  Pre-Pandemic: 8,414 matches (2015-09-11 to 2020-02-29)\n",
            "  During Pandemic: 2,294 matches (2020-03-01 to 2021-05-23)\n",
            "  Post-Pandemic: 7,155 matches (2021-08-06 to 2025-05-25)\n",
            "[OK] Pandemic analysis pivot table saved to: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports/pandemic_home_advantage_analysis.csv\n",
            "[OK] DataFrame table saved: 03b_pandemic_pivot_table.png (PNG + PDF)\n",
            "[OK] Pandemic analysis plot saved.\n",
            "[OK] Phase 3: Pandemic Impact Analysis completed.\n",
            "\n",
            "\n",
            "[INFO] PHASE 4: EDA ON TRAIN SET ONLY\n",
            "==========================================================================================\n",
            "[INFO] Starting detailed EDA using ONLY the 'train_matches' DataFrame (7,270 rows).\n",
            "\n",
            "[INFO] PHASE 4.1: Analyzing Market Odds (Overround / Vig)\n",
            "------------------------------------------------------------------------------------------\n",
            "[OK] Calculated implied probabilities and overround (Avg: 1.047)\n",
            "[OK] Overround distribution plot saved.\n",
            "[INFO] Overround > 1.01. Normalizing probabilities by removing overround...\n",
            "[OK] Normalized probability features (Prob_H/D/A_Norm) created.\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "[INFO] PHASE 4.2: IDENTIFYING HIGH CORRELATION PAIRS (Multicollinearity Check)\n",
            "==========================================================================================\n",
            "‚ö†Ô∏è  CRITICAL: Detecting features with correlation |r| > 0.85\n",
            "   These pairs create multicollinearity and must be addressed before modeling\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "[INFO] PRE-FLIGHT VALIDATION CHECKS (on Training Data)\n",
            "------------------------------------------------------------------------------------------\n",
            "[OK] Target column found and valid in training data (7270 non-null values).\n",
            "[OK] Training set size for EDA: 7,270 rows\n",
            "[OK] Identified 72 CLEAN numeric features for detailed analysis (LEAKAGE REMOVED).\n",
            "     ‚úì Excluded 28 leakage/non-predictive features\n",
            "     ‚úì Included 6 temporal features\n",
            "[INFO] Sample CLEAN features for analysis: ['HomeElo', 'AwayElo', 'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away', 'OddHome', 'OddDraw', 'OddAway', 'MaxHome', 'MaxDraw', 'MaxAway', 'Over25', 'Under25', 'MaxOver25']...\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "[INFO] Mapping features to their original data sources...\n",
            "[WARN] 9 numeric features could not be mapped to a source and are assigned to 'Unknown':\n",
            "       ['LeagueValue_Diff', 'n_suspended_players_Diff', 'Prob_H', 'Prob_D', 'Prob_A', 'Overround', 'Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm']...\n",
            "\n",
            "Feature count by Data Source (for numeric features used in EDA):\n",
            "DataSource\n",
            "Transfermarkt_Data    31\n",
            "Matches_Stats         19\n",
            "Unknown_Numeric        9\n",
            "Temporal_Features      6\n",
            "Matches_Form           4\n",
            "ELO_Ratings            3\n",
            "Name: count, dtype: int64\n",
            "[OK] Feature source distribution plotted and saved.\n",
            "\n",
            "[INFO] Computing correlation matrix for multicollinearity detection...\n",
            "\n",
            "[INFO] Identifying highly correlated feature pairs (|r| > 0.85)...\n",
            "\n",
            "‚ö†Ô∏è  ALERT: Found 47 highly correlated pairs (|r| > 0.85):\n",
            "==========================================================================================\n",
            "              Feature_1            Feature_2  Correlation  Abs_Correlation\n",
            "   HomeTeam_LeagueValue AwayTeam_LeagueValue     1.000000         1.000000\n",
            "                 Prob_A          Prob_A_Norm     0.999748         0.999748\n",
            "                 Prob_H          Prob_H_Norm     0.999611         0.999611\n",
            "                 Prob_D          Prob_D_Norm     0.998719         0.998719\n",
            "                 Over25            MaxOver25     0.998257         0.998257\n",
            "                Under25           MaxUnder25     0.997072         0.997072\n",
            "                OddHome              MaxHome     0.991283         0.991283\n",
            "                OddDraw              MaxDraw     0.982793         0.982793\n",
            "              HandiSize          Prob_H_Norm    -0.976353         0.976353\n",
            "              HandiSize               Prob_H    -0.976262         0.976262\n",
            "                OddAway              MaxAway     0.971962         0.971962\n",
            "            Prob_H_Norm          Prob_A_Norm    -0.960796         0.960796\n",
            "                 Prob_A          Prob_H_Norm    -0.960680         0.960680\n",
            "                 Prob_H          Prob_A_Norm    -0.960659         0.960659\n",
            "                 Prob_H               Prob_A    -0.960175         0.960175\n",
            "              HandiSize          Prob_A_Norm     0.954371         0.954371\n",
            "              HandiSize               Prob_A     0.954291         0.954291\n",
            "HomeTeam_MaxPlayerValue   HomeTeam_ClubValue     0.954263         0.954263\n",
            "AwayTeam_MaxPlayerValue   AwayTeam_ClubValue     0.953484         0.953484\n",
            "         ClubValue_Diff  MaxPlayerValue_Diff     0.952617         0.952617\n",
            "==========================================================================================\n",
            "\n",
            "[OK] High correlation pairs saved to: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports/09_high_correlation_pairs.csv\n",
            "\n",
            "==========================================================================================\n",
            "üìä AUTOMATED FEATURE SELECTION RECOMMENDATIONS:\n",
            "==========================================================================================\n",
            "\n",
            "üõ°Ô∏è  PROTECTED FEATURES (Will not be dropped):\n",
            "   ‚Ä¢ ELO_Diff\n",
            "   ‚Ä¢ OddHome\n",
            "   ‚Ä¢ OddDraw\n",
            "   ‚Ä¢ OddAway\n",
            "   ‚Ä¢ Prob_H_Norm\n",
            "   ‚Ä¢ Prob_D_Norm\n",
            "   ‚Ä¢ Prob_A_Norm\n",
            "\n",
            "                                       Pair Correlation                                                                Recommendation      Feature_to_Drop\n",
            "HomeTeam_LeagueValue ‚Üî AwayTeam_LeagueValue      1.0000 KEEP HomeTeam_LeagueValue, DROP AwayTeam_LeagueValue (redundant league value) AwayTeam_LeagueValue\n",
            "                       Prob_A ‚Üî Prob_A_Norm      0.9997                                 üõ°Ô∏è  KEEP Prob_A_Norm (protected), DROP Prob_A               Prob_A\n",
            "                       Prob_H ‚Üî Prob_H_Norm      0.9996                                 üõ°Ô∏è  KEEP Prob_H_Norm (protected), DROP Prob_H               Prob_H\n",
            "                       Prob_D ‚Üî Prob_D_Norm      0.9987                                 üõ°Ô∏è  KEEP Prob_D_Norm (protected), DROP Prob_D               Prob_D\n",
            "                         Over25 ‚Üî MaxOver25      0.9983                                           DROP MaxOver25 (Max odds redundant)            MaxOver25\n",
            "                       Under25 ‚Üî MaxUnder25      0.9971                                          DROP MaxUnder25 (Max odds redundant)           MaxUnder25\n",
            "                          OddHome ‚Üî MaxHome      0.9913                                    üõ°Ô∏è  KEEP OddHome (protected), DROP MaxHome              MaxHome\n",
            "                          OddDraw ‚Üî MaxDraw      0.9828                                    üõ°Ô∏è  KEEP OddDraw (protected), DROP MaxDraw              MaxDraw\n",
            "                    HandiSize ‚Üî Prob_H_Norm     -0.9764                              üõ°Ô∏è  KEEP Prob_H_Norm (protected), DROP HandiSize            HandiSize\n",
            "                         HandiSize ‚Üî Prob_H     -0.9763                                          DROP Prob_H (use normalized version)               Prob_H\n",
            "                          OddAway ‚Üî MaxAway      0.9720                                    üõ°Ô∏è  KEEP OddAway (protected), DROP MaxAway              MaxAway\n",
            "                  Prob_H_Norm ‚Üî Prob_A_Norm     -0.9608                            ‚ö†Ô∏è  BOTH PROTECTED: Keep both (handle in modeling)                  TBD\n",
            "                       Prob_A ‚Üî Prob_H_Norm     -0.9607                                 üõ°Ô∏è  KEEP Prob_H_Norm (protected), DROP Prob_A               Prob_A\n",
            "                       Prob_H ‚Üî Prob_A_Norm     -0.9607                                 üõ°Ô∏è  KEEP Prob_A_Norm (protected), DROP Prob_H               Prob_H\n",
            "                            Prob_H ‚Üî Prob_A     -0.9602                                          DROP Prob_H (use normalized version)               Prob_H\n",
            "\n",
            "üéØ FINAL DROP LIST (After protected features filter):\n",
            "   Original candidates: 13 ‚Üí After filter: 13\n",
            "==========================================================================================\n",
            "   1. AwayTeam_LeagueValue\n",
            "   2. ClubValue_Diff\n",
            "   3. HandiSize\n",
            "   4. MaxAway\n",
            "   5. MaxDraw\n",
            "   6. MaxHome\n",
            "   7. MaxOver25\n",
            "   8. MaxPlayerValue_Diff\n",
            "   9. MaxUnder25\n",
            "  10. Prob_A\n",
            "  11. Prob_D\n",
            "  12. Prob_H\n",
            "  13. ValueRatio_Diff\n",
            "==========================================================================================\n",
            "[OK] Python code for dropping features saved to: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports/09b_features_to_drop_code.py\n",
            "[RECOMMENDATION] Copy this list to your modeling script and apply BEFORE training!\n",
            "[OK] Detailed recommendations saved to: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports/09c_multicollinearity_recommendations.csv\n",
            "[OK] High correlation pairs visualization saved.\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "[INFO] Analyzing Target distribution (TRAIN SET ONLY)...\n",
            "[OK] Target distribution : {0: 24.745529573590098, 1: 45.80467675378267, 2: 29.449793672627234}\n",
            "      Counts: Home=3330, Draw=1799, Away=2141\n",
            "[OK] Target distribution analysis saved.\n",
            "\n",
            "[INFO] PHASE 4.2.5: APPLYING DIFF-ONLY STRATEGY FOR TRANSFERMARKT FEATURES\n",
            "==========================================================================================\n",
            "üìä STRATEGY: Keep only DIFF features, drop Home/Away absolutes\n",
            "   Rationale: Eliminate perfect multicollinearity (VIF=313 ‚Üí VIF<10)\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "[DETECTING] Home/Away/Diff triplets...\n",
            "  ‚úì Found triplet: ClubValue\n",
            "    ‚Üí DROP: HomeTeam_ClubValue, AwayTeam_ClubValue\n",
            "    ‚Üí KEEP: ClubValue_Diff\n",
            "  ‚úì Found triplet: MaxPlayerValue\n",
            "    ‚Üí DROP: HomeTeam_MaxPlayerValue, AwayTeam_MaxPlayerValue\n",
            "    ‚Üí KEEP: MaxPlayerValue_Diff\n",
            "  ‚úì Found triplet: ManagerTrophies\n",
            "    ‚Üí DROP: HomeTeam_ManagerTrophies, AwayTeam_ManagerTrophies\n",
            "    ‚Üí KEEP: ManagerTrophies_Diff\n",
            "  ‚úì Found triplet: ManagerTenureDays\n",
            "    ‚Üí DROP: HomeTeam_ManagerTenureDays, AwayTeam_ManagerTenureDays\n",
            "    ‚Üí KEEP: ManagerTenureDays_Diff\n",
            "  ‚úì Found triplet: NetTransferSpending\n",
            "    ‚Üí DROP: HomeTeam_NetTransferSpending, AwayTeam_NetTransferSpending\n",
            "    ‚Üí KEEP: NetTransferSpending_Diff\n",
            "  ‚úì Found triplet: ValueRatio\n",
            "    ‚Üí DROP: HomeTeam_ValueRatio, AwayTeam_ValueRatio\n",
            "    ‚Üí KEEP: ValueRatio_Diff\n",
            "  ‚úì Found triplet: n_players_injured\n",
            "    ‚Üí DROP: HomeTeam_n_players_injured, AwayTeam_n_players_injured\n",
            "    ‚Üí KEEP: n_players_injured_Diff\n",
            "  ‚úì Found triplet: n_suspended_players\n",
            "    ‚Üí DROP: HomeTeam_n_suspended_players, AwayTeam_n_suspended_players\n",
            "    ‚Üí KEEP: n_suspended_players_Diff\n",
            "  ‚úì Found triplet: LeagueValue\n",
            "    ‚Üí DROP: HomeTeam_LeagueValue, AwayTeam_LeagueValue\n",
            "    ‚Üí KEEP: LeagueValue_Diff\n",
            "\n",
            "[ADDING] Odds and probability redundancies...\n",
            "\n",
            "[SPECIAL CASE] HandiSize...\n",
            "  ‚Üí DROP HandiSize (r=-0.976 with Prob_H_Norm)\n",
            "\n",
            "[COMBINING] Correlation drops + Diff-only drops...\n",
            "\n",
            "[SUMMARY] Drop List Statistics:\n",
            "  ‚Ä¢ Original correlation drops: 13\n",
            "  ‚Ä¢ Transfermarkt absolute drops: 27\n",
            "  ‚Ä¢ Total unique drops: 27\n",
            "  ‚Ä¢ Protected Diff features: 9\n",
            "\n",
            "[PROTECTED] Features removed from drop list (now kept):\n",
            "  ‚úì ClubValue_Diff\n",
            "  ‚úì MaxPlayerValue_Diff\n",
            "  ‚úì ValueRatio_Diff\n",
            "\n",
            "[ADDED] New features added to drop list:\n",
            "  + AwayTeam_ClubValue\n",
            "  + AwayTeam_ManagerTenureDays\n",
            "  + AwayTeam_ManagerTrophies\n",
            "  + AwayTeam_MaxPlayerValue\n",
            "  + AwayTeam_NetTransferSpending\n",
            "  + AwayTeam_ValueRatio\n",
            "  + AwayTeam_n_players_injured\n",
            "  + AwayTeam_n_suspended_players\n",
            "  + HomeTeam_ClubValue\n",
            "  + HomeTeam_LeagueValue\n",
            "  + HomeTeam_ManagerTenureDays\n",
            "  + HomeTeam_ManagerTrophies\n",
            "  + HomeTeam_MaxPlayerValue\n",
            "  + HomeTeam_NetTransferSpending\n",
            "  + HomeTeam_ValueRatio\n",
            "  + HomeTeam_n_players_injured\n",
            "  + HomeTeam_n_suspended_players\n",
            "\n",
            "[SAVING] Revised drop list to file...\n",
            "[OK] Revised drop list saved: 09b_features_to_drop_code_REVISED.py\n",
            "     File size: 1907 bytes\n",
            "[OK] Original drop list retained: 09b_features_to_drop_code.py\n",
            "\n",
            "==========================================================================================\n",
            "‚úÖ PHASE 4.2.5 COMPLETE - Diff-Only Strategy Applied\n",
            "==========================================================================================\n",
            "\n",
            "[INFO] PHASE 4.3: CLASS IMBALANCE ANALYSIS (Descriptive Only)\n",
            "==========================================================================================\n",
            "[INFO] Executing class imbalance analysis...\n",
            "\n",
            "üìä CLASS IMBALANCE ANALYSIS - TRAIN SET (Real Data Only)\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT NOTE:\n",
            "   This analysis uses REAL training data only.\n",
            "   NO synthetic samples (SMOTE) are created or analyzed.\n",
            "   SMOTE (if needed) will be applied later in modeling phase.\n",
            "\n",
            "[STEP 1] Preparing data for analysis...\n",
            "  ‚úì No NaN values in Target column\n",
            "\n",
            "  Dataset for analysis:\n",
            "    ‚Ä¢ Total samples: 7,270\n",
            "    ‚Ä¢ Data type: REAL matches (no synthetic data)\n",
            "    ‚Ä¢ Time period: Training set only (pre-2019)\n",
            "\n",
            "[STEP 2] Analyzing class distribution...\n",
            "------------------------------------------------------------\n",
            "\n",
            "  Class Distribution (Real Training Data):\n",
            "    Class 0 (Draw     ): 1,799 (24.75%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    Class 1 (Home Win ): 3,330 (45.80%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "    Class 2 (Away Win ): 2,141 (29.45%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "[STEP 3] Assessing imbalance severity...\n",
            "\n",
            "  Imbalance Metrics:\n",
            "    ‚Ä¢ Majority Class: 1 (Home Win) = 3,330 samples\n",
            "    ‚Ä¢ Minority Class: 0 (Draw) = 1,799 samples\n",
            "    ‚Ä¢ Imbalance Ratio: 1.85x\n",
            "\n",
            "  ‚ö†Ô∏è Severity Assessment: MODERATE\n",
            "     ‚Üí Consider class balancing techniques\n",
            "\n",
            "  ‚öΩ Football Context:\n",
            "     ‚Ä¢ Home win advantage is NATURAL in football\n",
            "     ‚Ä¢ Ratio of 1.85x is typical for home/draw/away\n",
            "     ‚Ä¢ This reflects REAL-WORLD match outcomes\n",
            "\n",
            "================================================================================\n",
            "[STEP 4] RECOMMENDATIONS FOR MODELING PHASE\n",
            "================================================================================\n",
            "\n",
            "üéØ STRATEGY 1: Class Weights (‚≠ê RECOMMENDED for Football Prediction)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Why Recommended:\n",
            "    ‚Ä¢ No synthetic data created (preserves true distribution)\n",
            "    ‚Ä¢ Works with all algorithms (tree-based, linear models)\n",
            "    ‚Ä¢ Interpretable (real matches, real patterns)\n",
            "    ‚Ä¢ Faster training (no data augmentation)\n",
            "\n",
            "  How to Implement:\n",
            "    # For scikit-learn models:\n",
            "    from sklearn.linear_model import LogisticRegression\n",
            "    model = LogisticRegression(class_weight='balanced')\n",
            "\n",
            "    # For XGBoost (recommended for football):\n",
            "    import xgboost as xgb\n",
            "    class_weights = {\n",
            "        0: 1.347,  # Draw\n",
            "        1: 0.728,  # Home Win\n",
            "        2: 1.132,  # Away Win\n",
            "    }\n",
            "    model = xgb.XGBClassifier(scale_pos_weight=class_weights)\n",
            "\n",
            "  ‚úÖ Advantages:\n",
            "    ‚Ä¢ Simple to implement\n",
            "    ‚Ä¢ No additional computational cost\n",
            "    ‚Ä¢ Preserves natural data distribution\n",
            "    ‚Ä¢ Works well with tree-based models\n",
            "\n",
            "\n",
            "üéØ STRATEGY 2: SMOTE (‚ö†Ô∏è Alternative - Use with Caution)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  ‚ö†Ô∏è  CRITICAL WARNINGS:\n",
            "    ‚Ä¢ SMOTE creates SYNTHETIC samples (not real matches!)\n",
            "    ‚Ä¢ Apply ONLY in modeling pipeline, NEVER in EDA\n",
            "    ‚Ä¢ Only for TRAINING set (never validation/test)\n",
            "    ‚Ä¢ May create unrealistic match scenarios\n",
            "\n",
            "  When to Consider SMOTE:\n",
            "    ‚Ä¢ Class weights alone are insufficient\n",
            "    ‚Ä¢ Very severe imbalance (>5x)\n",
            "    ‚Ä¢ Using algorithms sensitive to class balance\n",
            "\n",
            "  Correct Implementation:\n",
            "    from imblearn.over_sampling import SMOTE\n",
            "    from sklearn.model_selection import train_test_split\n",
            "\n",
            "    # Step 1: Split data FIRST (before any SMOTE!)\n",
            "    X_train, X_val, y_train, y_val = train_test_split(X, y, ...\n",
            "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, ...)\n",
            "\n",
            "    # Step 2: Apply SMOTE ONLY to training set\n",
            "    smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
            "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
            "\n",
            "    # Step 3: Train on balanced data\n",
            "    model.fit(X_train_balanced, y_train_balanced)\n",
            "\n",
            "    # Step 4: Evaluate on REAL validation/test data (no SMOTE!)\n",
            "    y_val_pred = model.predict(X_val)  # Original val set\n",
            "    y_test_pred = model.predict(X_test)  # Original test set\n",
            "\n",
            "  ‚ùå WRONG Implementation (DO NOT DO THIS!):\n",
            "    # ‚ùå Applying SMOTE before split (DATA LEAKAGE!)\n",
            "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
            "    X_train, X_test, ... = train_test_split(X_balanced, y_balanced)\n",
            "\n",
            "    # ‚ùå Applying SMOTE to validation/test (CONTAMINATION!)\n",
            "    X_val_balanced, y_val_balanced = smote.fit_resample(X_val, y_val)\n",
            "\n",
            "  Expected SMOTE Results (Simulation):\n",
            "    Before SMOTE: {0: np.int64(1799), 1: np.int64(3330), 2: np.int64(2141)}\n",
            "    After SMOTE:  Draw=3330, Home Win=3330, Away Win=3330\n",
            "    ‚Üí Perfect balance (1:1:1 ratio)\n",
            "\n",
            "\n",
            "üéØ STRATEGY 3: Stratified Sampling (‚úÖ ALWAYS Recommended)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Purpose:\n",
            "    ‚Ä¢ Ensure proportional class distribution in train/val/test\n",
            "    ‚Ä¢ Prevent accidentally creating imbalanced splits\n",
            "\n",
            "  Implementation:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "\n",
            "    # Use stratify parameter\n",
            "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
            "        X, y, test_size=0.3, stratify=y, random_state=42\n",
            "    )\n",
            "\n",
            "    X_val, X_test, y_val, y_test = train_test_split(\n",
            "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
            "    )\n",
            "\n",
            "  ‚úÖ Benefits:\n",
            "    ‚Ä¢ Maintains class proportions across all sets\n",
            "    ‚Ä¢ More reliable model evaluation\n",
            "    ‚Ä¢ Essential for time-series cross-validation\n",
            "\n",
            "\n",
            "üéØ STRATEGY 4: Evaluation Metrics (‚úÖ CRITICAL)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Why Important:\n",
            "    ‚Ä¢ Accuracy is MISLEADING with imbalanced data\n",
            "    ‚Ä¢ Example: Predicting all 'Home Win' gives 45.8% accuracy!\n",
            "\n",
            "  Recommended Metrics for Football Prediction:\n",
            "    1. F1-Score (macro/weighted)  ‚Üê Primary metric\n",
            "    2. Precision & Recall (per class)\n",
            "    3. Cohen's Kappa (agreement measure)\n",
            "    4. Confusion Matrix (detailed view)\n",
            "    5. ROC-AUC (one-vs-rest)\n",
            "\n",
            "  Implementation:\n",
            "    from sklearn.metrics import classification_report, cohen_kappa_score\n",
            "\n",
            "    # Comprehensive evaluation\n",
            "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
            "    print(f'Kappa Score: {cohen_kappa_score(y_true, y_pred):.3f}')\n",
            "\n",
            "================================================================================\n",
            "\n",
            "[STEP 5] Creating visualization...\n",
            "  ‚úÖ Visualization saved: 04c_class_imbalance_analysis.png\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "[OK] Class imbalance analysis complete (descriptive only).\n",
            "     ‚Üí Recommendations saved in analysis output\n",
            "     ‚Üí Apply strategies in MODELING phase, not in EDA\n",
            "\n",
            "[INFO] Analyzing Categorical Feature Distributions (Frequency Plots)...\n",
            "[INFO] Plotting frequency for: ['Division', 'MatchTime', 'HomeTeam_Season', 'HomeTeam_wettbewerb_id', 'HomeTeam_LeagueName', 'Season_home_tm', 'AwayTeam_wettbewerb_id', 'AwayTeam_LeagueName', 'AwayTeam_Season']\n",
            "[OK] Plotted 9 categorical feature distributions.\n",
            "\n",
            "[INFO] Analyzing Numerical Feature Distributions (Home vs Away)...\n",
            "[OK] Plotted 6 Home vs Away distributions.\n",
            "\n",
            "[INFO] Analyzing Difference Feature Distributions...\n",
            "[OK] Plotted 12 Difference feature distributions.\n",
            "\n",
            "[INFO] Analyzing Difference Features vs. Target (Draw Analysis)...\n",
            "  [INFO] Preparing palette and target encoding...\n",
            "      ‚úì Target converted to integer type\n",
            "      ‚úì Target classes present: [np.int64(0), np.int64(1), np.int64(2)]\n",
            "      ‚úì Palette configured for: ['0', '1', '2'] (string keys)\n",
            "  [HYPOTHESIS CHECK] Median ELO_Diff for Draws: -5.74 (Expected ~0)\n",
            "\n",
            "[INFO] Analyzing Numerical Scatter Plots vs Target (TRAIN SET ONLY)...\n",
            "[WARN] Scatter plot pair skipped (missing): Form5Difference or ManagerTrophies_Diff\n",
            "[OK] Plotted 3 numerical scatter plots.\n",
            "\n",
            "[INFO] Generating Descriptive Statistics Table (TRAIN SET ONLY)...\n",
            "[INFO] Calculating stats for 18 selected features...\n",
            "[OK] DataFrame table saved: 01_descriptive_statistics_train.png (PNG + PDF)\n",
            "[OK] Descriptive statistics saved (CSV + PNG).\n",
            "\n",
            "[INFO] Performing Feature-Target Correlation Analysis...\n",
            "[OK] Feature-Target correlations calculated (Top 5):\n",
            "OddHome        0.245787\n",
            "Prob_A_Norm    0.245528\n",
            "Prob_A         0.245378\n",
            "MaxHome        0.240943\n",
            "HandiSize      0.219233\n",
            "Name: Target, dtype: float64\n",
            "[INFO] Creating feature-target correlations bar chart...\n",
            "[OK] Feature-target correlations bar chart saved.\n",
            "\n",
            "[INFO] Creating focused heatmap (Stats vs. Market Odds)...\n",
            "[OK] Focused Stats vs. Odds heatmap saved (Blue Theme, Top 10).\n",
            "[HYPOTHESIS CHECK] Correlation(ELO_Diff, Prob_H_Norm) = 0.939\n",
            "[INSIGHT] High correlation suggests market odds heavily incorporate ELO information.\n",
            "[OK] Correlation data saved to CSV files.\n",
            "\n",
            "[INFO] PHASE 4.4: VIF (VARIANCE INFLATION FACTOR) ANALYSIS\n",
            "==========================================================================================\n",
            "[INFO] Starting OPTIMIZED VIF analysis...\n",
            "\n",
            "[PRE-FILTER] Applying 13 correlation-based drops first...\n",
            "  ‚Ä¢ Reduced features: 72 ‚Üí 59\n",
            "\n",
            "üîç VIF ANALYSIS - OPTIMIZED (Safe Mode)\n",
            "================================================================================\n",
            "\n",
            "[STEP 0] Initial data preparation...\n",
            "  ‚Ä¢ Starting features: 59\n",
            "  ‚Ä¢ Removing 1 zero-variance features\n",
            "  ‚Ä¢ After cleanup: 58 features\n",
            "\n",
            "[STEP 1] Removing high correlations (|r| > 0.9)...\n",
            "  ‚Ä¢ Dropping 6 features due to high correlation\n",
            "  ‚Ä¢ Sample pairs removed:\n",
            "      ‚Üí HandiAway ‚Üî HandiHome (r=0.944)\n",
            "      ‚Üí HomeTeam_ClubValue ‚Üî HomeTeam_MaxPlayerValue (r=0.954)\n",
            "      ‚Üí AwayTeam_ClubValue ‚Üî AwayTeam_MaxPlayerValue (r=0.954)\n",
            "      ‚Üí Prob_H_Norm ‚Üî ELO_Diff (r=0.938)\n",
            "      ‚Üí Prob_D_Norm ‚Üî OddDraw (r=0.913)\n",
            "      ... and 1 more pairs\n",
            "  ‚Ä¢ Features after filtering: 52\n",
            "\n",
            "[STEP 2] Computing VIF on cleaned data...\n",
            "  ‚Üí Expected time: 5-15 seconds (vs 30-60 before)\n",
            "\n",
            "  Progress: 50/52\n",
            "  ‚úÖ VIF computation done in 7.1 seconds\n",
            "\n",
            "[STEP 3] Identifying features to drop (VIF ‚â• 10)...\n",
            "  ‚Ä¢ Found 27 features with VIF ‚â• 10:\n",
            "      ‚Üí ManagerTrophies_Diff                     VIF=313.0\n",
            "      ‚Üí C_VAD                                    VIF=305.8\n",
            "      ‚Üí C_VHD                                    VIF=300.0\n",
            "      ‚Üí C_HTB                                    VIF=194.3\n",
            "      ‚Üí C_LTA                                    VIF=193.9\n",
            "      ‚Üí C_LTH                                    VIF=188.8\n",
            "      ‚Üí AwayTeam_ManagerTrophies                 VIF=170.5\n",
            "      ‚Üí HomeTeam_ManagerTrophies                 VIF=169.1\n",
            "      ‚Üí ManagerTenureDays_Diff                   VIF=140.5\n",
            "      ‚Üí C_PHB                                    VIF=138.6\n",
            "      ... and 17 more\n",
            "\n",
            "================================================================================\n",
            "[SUMMARY] VIF Analysis Results\n",
            "================================================================================\n",
            "  ‚Ä¢ Initial features:           59\n",
            "  ‚Ä¢ After correlation filter:   52\n",
            "  ‚Ä¢ Features with VIF ‚â• 10:     27\n",
            "  ‚Ä¢ Total recommended drops:    33\n",
            "================================================================================\n",
            "\n",
            "[OK] VIF results saved to CSV.\n",
            "\n",
            "[FINAL DROP LIST] Total features to remove: 46\n",
            "  ‚Ä¢ From correlation (Phase 4.2): 13\n",
            "  ‚Ä¢ From VIF correlation filter: 6\n",
            "  ‚Ä¢ From VIF threshold (‚â•10):    27\n",
            "[OK] Combined drop list saved: 09h_combined_multicollinearity_drops.py\n",
            "[INFO] Creating VIF visualization...\n",
            "[OK] VIF Analysis plot saved (PDF & PNG).\n",
            "\n",
            "[INFO] Computing Feature Relevance (Mutual Information) (TRAIN SET ONLY)...\n",
            "[INFO] Calculating MI for 72 features...\n",
            "\n",
            "[OK] Mutual Information scores calculated!\n",
            "==========================================================================================\n",
            "üèÜ TOP 10 MOST PREDICTIVE FEATURES (by MI Score):\n",
            "==========================================================================================\n",
            "   1. Prob_H_Norm                              ‚Üí MI = 0.1165\n",
            "   2. Prob_A                                   ‚Üí MI = 0.1146\n",
            "   3. HandiSize                                ‚Üí MI = 0.1101\n",
            "   4. MaxHome                                  ‚Üí MI = 0.1076\n",
            "   5. Prob_A_Norm                              ‚Üí MI = 0.1076\n",
            "   6. MaxAway                                  ‚Üí MI = 0.1054\n",
            "   7. Prob_H                                   ‚Üí MI = 0.1041\n",
            "   8. OddAway                                  ‚Üí MI = 0.1029\n",
            "   9. OddHome                                  ‚Üí MI = 0.1014\n",
            "  10. ValueRatio_Diff                          ‚Üí MI = 0.0848\n",
            "==========================================================================================\n",
            "\n",
            "[OK] MI scores bar chart saved (Blue Theme).\n",
            "[OK] Feature Relevance (MI) visualization saved (Blue Theme).\n",
            "[OK] Feature relevance (MI) details saved to CSV.\n",
            "\n",
            "[INFO] Computing t-SNE Visualization (Multi-Perplexity) (TRAIN SET ONLY)...\n",
            "\n",
            "üîÆ t-SNE MULTI-PERPLEXITY COMPARISON\n",
            "================================================================================\n",
            "\n",
            "[INFO] Preparing data...\n",
            "  ‚ö†Ô∏è  Removing 1 zero-variance features\n",
            "  ‚ö†Ô∏è  Subsampling to 3,000 samples\n",
            "  Dataset: 3,000 samples √ó 87 features\n",
            "\n",
            "[COMPUTING t-SNE] Testing multiple perplexities...\n",
            "  ‚Üí This may take 2-5 minutes\n",
            "\n",
            "  Computing perplexity=5... ‚úÖ (23.7s)\n",
            "  Computing perplexity=15... ‚úÖ (28.1s)\n",
            "  Computing perplexity=30... ‚úÖ (33.6s)\n",
            "  Computing perplexity=50... ‚úÖ (41.8s)\n",
            "\n",
            "  ‚úÖ Successfully computed 4 embeddings\n",
            "  Total time: 127.2s\n",
            "\n",
            "[VISUALIZATION] Creating comparison plots...\n",
            "    ‚úÖ Saved PDF: 11a_tsne_multiperplexity.pdf\n",
            "    ‚úÖ Saved PNG: 11a_tsne_multiperplexity.png\n",
            "\n",
            "================================================================================\n",
            "[RECOMMENDATIONS] Perplexity Selection\n",
            "================================================================================\n",
            "\n",
            "  Dataset size: 3,000 samples\n",
            "  Recommended range: 30 - 50\n",
            "  Default: 30 (balanced)\n",
            "\n",
            "  Silhouette Scores:\n",
            "       Perplexity= 5: -0.0166\n",
            "       Perplexity=15: -0.0093\n",
            "       Perplexity=30: -0.0097\n",
            "    üèÜ Perplexity=50: -0.0077\n",
            "\n",
            "  ‚úÖ Best: perplexity=50 (silhouette=-0.0077)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "[OK] t-SNE multi-perplexity analysis complete.\n",
            "\n",
            "[INFO] PHASE 4.9: FEATURE ENGINEERING EXAMPLES\n",
            "==========================================================================================\n",
            "\n",
            "üîß FEATURE ENGINEERING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[INFO] Visualizing model feature engineering\n",
            "  ‚Üí Goal: Show why engineered features improve prediction\n",
            "\n",
            "[PLOT 1] ELO_Diff distribution...\n",
            "[PLOT 2] ValueRatio_Diff distribution...\n",
            "[PLOT 3] Log transformation effect...\n",
            "[PLOT 4] Normalized odds...\n",
            "[PLOT 5] Diff features correlation...\n",
            "\n",
            "  ‚úÖ Saved: 12_feature_engineering_examples.png\n",
            "\n",
            "================================================================================\n",
            "[SUMMARY] Feature Engineering Impact\n",
            "================================================================================\n",
            "\n",
            "  ‚úÖ Difference Features: ELO_Diff, ValueRatio_Diff, etc.\n",
            "  ‚úÖ Ratio Features: ValueRatio = ClubValue / LeagueValue\n",
            "  ‚úÖ Log Transformations: Reduce skewness\n",
            "  ‚úÖ Normalized Probabilities: Remove overround\n",
            "  ‚úÖ Temporal Features: L3/L5 windows with shift(1)\n",
            "\n",
            "  üìä Total Engineered Features: 22\n",
            "\n",
            "================================================================================\n",
            "\n",
            "[OK] Feature engineering examples complete.\n",
            "\n",
            "[INFO] ========== END OF PART 2A ==========\n",
            "[INFO] Continue with PART 2B for Outlier Detection, Temporal Analysis, Chi-Square, and Summary\n",
            "\n",
            "[INFO] ========== STARTING PART 2B ==========\n",
            "\n",
            "[INFO] PHASE 4.5: OUTLIER DETECTION & ANALYSIS (TRAIN SET ONLY)\n",
            "==========================================================================================\n",
            "[OK] Outlier detection method: IQR (Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
            "     Total samples in training set: 7,270\n",
            "     Found 6,765 samples (93.05%) with at least one outlier feature.\n",
            "\n",
            "[OK] Features contributing to outliers (Top 15 or all if fewer):\n",
            "                     Feature  Outlier_Count  Percentage_of_Samples\n",
            "0                  Overround           1624                  22.34\n",
            "1                      C_VHD           1161                  15.97\n",
            "2                      C_VAD           1124                  15.46\n",
            "3             ClubValue_Diff           1030                  14.17\n",
            "4   NetTransferSpending_Diff           1002                  13.78\n",
            "5        MaxPlayerValue_Diff            970                  13.34\n",
            "6                      C_PHB            884                  12.16\n",
            "7                    MaxDraw            838                  11.53\n",
            "8    AwayTeam_MaxPlayerValue            802                  11.03\n",
            "9    HomeTeam_MaxPlayerValue            797                  10.96\n",
            "10           ValueRatio_Diff            795                  10.94\n",
            "11                   OddDraw            793                  10.91\n",
            "12                   MaxAway            781                  10.74\n",
            "13                   MaxHome            776                  10.67\n",
            "14                     C_LTA            764                  10.51\n",
            "[OK] Outlier counts by feature (IQR) saved to CSV.\n",
            "\n",
            "[INFO] Saving 6765 outlier samples details to CSV...\n",
            "[OK] Outlier samples saved.\n",
            "[INFO] Creating outlier visualizations (Boxplots for key features)...\n",
            "[OK] Outlier boxplots (IQR) visualization saved.\n",
            "\n",
            "[INFO] PHASE 4.7: TEMPORAL & SEASONALITY ANALYSIS (TRAIN SET ONLY)\n",
            "==========================================================================================\n",
            "[INFO] Calculating temporal trends by season (year)...\n",
            "\n",
            "[OK] Temporal trends by season (year) for Train Set:\n",
            "           Match_Count  Home_Win_Pct  HomeElo_Mean  AwayElo_Mean\n",
            "Season                                                          \n",
            "2015-2016         1679         44.91       1683.30       1682.98\n",
            "2016-2017         1826         48.63       1680.46       1680.72\n",
            "2017-2018         1826         45.35       1676.01       1676.16\n",
            "2018-2019         1826         44.74       1675.33       1675.82\n",
            "2019-2020          113         38.05       1676.55       1690.55\n",
            "[OK] Seasonal trends saved to CSV.\n",
            "[INFO] Creating temporal analysis visualization (Train Set)...\n",
            "[OK] Temporal analysis visualization saved.\n",
            "\n",
            "[INFO] Performing Chi-Square Test for Categorical Features (TRAIN SET ONLY)\n",
            "------------------------------------------------------------------------------------------\n",
            "[INFO] Testing 11 categorical features against Target:\n",
            "         Division, MatchTime, HomeTeam_Season, HomeTeam_TM, AwayTeam_TM, HomeTeam_wettbewerb_id, HomeTeam_LeagueName, Season_home_tm, AwayTeam_wettbewerb_id, AwayTeam_LeagueName, AwayTeam_Season\n",
            "\n",
            "[INFO] Multiple Comparison Correction:\n",
            "         Original Œ± = 0.05\n",
            "         Bonferroni Adjusted Œ± = 0.004545 (11 tests)\n",
            "\n",
            "  ‚úó Division: Chi2=6.63, p=0.5776, Valid=True \n",
            "  ‚úó MatchTime: Chi2=27.56, p=0.3802, Valid=False ‚ö† (Expected < 1)\n",
            "  ‚úó HomeTeam_Season: Chi2=13.22, p=0.1047, Valid=True \n",
            "  ‚úó HomeTeam_TM: Skipped (High Cardinality > 50 uniques)\n",
            "  ‚úó AwayTeam_TM: Skipped (High Cardinality > 50 uniques)\n",
            "  ‚úó HomeTeam_wettbewerb_id: Chi2=6.78, p=0.5603, Valid=True \n",
            "  ‚úó HomeTeam_LeagueName: Chi2=6.78, p=0.5603, Valid=True \n",
            "  ‚úó Season_home_tm: Chi2=12.62, p=0.1257, Valid=True \n",
            "  ‚úó AwayTeam_wettbewerb_id: Chi2=6.41, p=0.6019, Valid=True \n",
            "  ‚úó AwayTeam_LeagueName: Chi2=6.41, p=0.6019, Valid=True \n",
            "  ‚úó AwayTeam_Season: Chi2=13.02, p=0.1112, Valid=True \n",
            "\n",
            "[OK] Chi-Square results and validation saved to CSV.\n",
            "[OK] DataFrame table saved: 02a_chi_square_results_table.png (PNG + PDF)\n",
            "[OK] DataFrame table saved: 02b_chi_square_validation_table.png (PNG + PDF)\n",
            "[OK] Chi-Square results visualized.\n",
            "\n",
            "[INFO] Performing Data Quality Analysis (Missing Values, TRAIN SET ONLY)...\n",
            "[WARN] Found 58 numeric features with missing values in the training set (Top 15):\n",
            "ValueRatio_Diff             3.026135\n",
            "n_players_injured_Diff      3.026135\n",
            "ManagerTenureDays_Diff      3.026135\n",
            "NetTransferSpending_Diff    3.026135\n",
            "LeagueValue_Diff            3.026135\n",
            "MaxPlayerValue_Diff         3.026135\n",
            "ClubValue_Diff              3.026135\n",
            "n_suspended_players_Diff    3.026135\n",
            "ManagerTrophies_Diff        3.026135\n",
            "Fouls_Diff_L5               2.352132\n",
            "Target_Diff_L5              2.352132\n",
            "Corners_Diff_L5             2.352132\n",
            "Yellow_Diff_L5              2.352132\n",
            "ShotAccuracy_Diff_L5        2.352132\n",
            "Shots_Diff_L5               2.352132\n",
            "dtype: float64\n",
            "[OK] Missing values analysis plot saved.\n",
            "[OK] Missing values percentages saved to CSV.\n",
            "\n",
            "[INFO] PHASE 5: GENERATING EDA SUMMARY (Based on Train Set Analysis)\n",
            "==========================================================================================\n",
            "\n",
            "    =============================================================\n",
            "    EDA SUMMARY - V5.0 (DATA LEAKAGE CLEANED + TEMPORAL FEATURES)\n",
            "    =============================================================\n",
            "\n",
            "    üöÄ MAJOR CHANGES IN V5.0:\n",
            "       ‚úÖ ALL match-event features converted to temporal averages (L3/L5)\n",
            "       ‚úÖ Data leakage completely eliminated\n",
            "       ‚úÖ 6 temporal features created\n",
            "       ‚úÖ 13 features identified for removal (multicollinearity)\n",
            "\n",
            "    1. DATASET OVERVIEW\n",
            "       - Total Processed Matches (2015-Present): 17,863\n",
            "       - Training Set (Used for this EDA): 7,270 matches\n",
            "       - Validation Set (Pandemic Gap Excluded): 2,008 matches\n",
            "       - Test Set (Most Recent): 5,147 matches\n",
            "       - Leagues Analyzed: 5 (PL, Bundesliga, Serie A, La Liga, Ligue 1)\n",
            "\n",
            "    2. TARGET DISTRIBUTION (Train Set)\n",
            "       - Home Win: 45.8%, Draw: 24.7%, Away Win: 29.4%\n",
            "       - Imbalance observed: Home wins are most frequent.\n",
            "\n",
            "    3. DATA QUALITY (Train Set)\n",
            "       - Duplicates: Checked and likely 0 after processing.\n",
            "       - Missing Values: Analyzed in numeric features (see '13_data_quality...' plot/CSV).\n",
            "\n",
            "    4. FEATURE ANALYSIS (Train Set)\n",
            "       - Total Numeric Features Analyzed: 72\n",
            "       - Key Features Examined: 18\n",
            "       - Temporal Features Created: 6\n",
            "\n",
            "       üìä Top 3 Features by Relevance (Mutual Information - LEAKAGE-FREE):\n",
            "         1. Prob_H_Norm (MI: 0.116)\n",
            "         2. Prob_A (MI: 0.115)\n",
            "         3. HandiSize (MI: 0.110)\n",
            "\n",
            "       ‚ö†Ô∏è  CRITICAL - MULTICOLLINEARITY DETECTED:\n",
            "         * High correlations (>0.85): 47 pairs found (see '09_high_correlation_pairs.csv')\n",
            "         * Features to DROP: 13 (see '09b_features_to_drop_code.py')\n",
            "         * MANDATORY: Apply these removals BEFORE modeling\n",
            "         * Rationale: Redundant normalized probabilities, Max vs Odd duplicates\n",
            "\n",
            "       ‚úÖ DATA LEAKAGE PREVENTION:\n",
            "         * Original match-event features (HomeTarget, HomeShots, etc.) ‚Üí REMOVED\n",
            "         * Replaced with: Historical averages (L3, L5 windows)\n",
            "         * Example: HomeTarget ‚Üí Home_Target_Avg_L5\n",
            "         * This ensures NO future information leaks into predictions\n",
            "\n",
            "       - Outliers: Detected via IQR (see '10a/b/c...' plots/CSVs).\n",
            "         Strategy: RobustScaler + log transformation for skewed features recommended.\n",
            "\n",
            "    5. PANDEMIC IMPACT (Full Data Context)\n",
            "       - Average Home Win %:\n",
            "         - Pre-Pandemic: 45.8%\n",
            "         - During Pandemic: 40.2% (-5.6pp vs Pre)\n",
            "         - Post-Pandemic: 43.4% (-2.3pp vs Pre)\n",
            "       - Conclusion: Pandemic significantly reduced home advantage, partially recovered post-pandemic.\n",
            "\n",
            "    6. TEMPORAL TRENDS (Train Set)\n",
            "       - ELO: Slight upward trend over seasons (see '11b...' plot).\n",
            "       - Home Advantage: Fluctuates seasonally (see '11b...' plot).\n",
            "       - Temporal Features: Stable patterns in L5 averages (see '11b...' plot).\n",
            "\n",
            "    7. CATEGORICAL FEATURES (Train Set vs Target)\n",
            "       - Features Tested: Division, MatchTime, HomeTeam_Season, HomeTeam_TM, AwayTeam_TM, HomeTeam_wettbewerb_id, HomeTeam_LeagueName, Season_home_tm, AwayTeam_wettbewerb_id, AwayTeam_LeagueName, AwayTeam_Season\n",
            "       - Significant Features (Œ±=0.05): None\n",
            "       - Significant Features (Bonferroni Œ±‚âà0.0045): None\n",
            "       - Validation: Chi-Square assumptions checked (see '02b...' CSV/Table).\n",
            "\n",
            "    8. DIMENSIONALITY & SEPARATION (Train Set)\n",
            "       - t-SNE Plot: Suggests non-linear separation needed. Classes appear highly overlapping.\n",
            "\n",
            "    9. üéØ CRITICAL ACTION ITEMS FOR MODELING\n",
            "\n",
            "       ‚≠ê PRIORITY 1: REMOVE MULTICOLLINEAR FEATURES\n",
            "       ```python\n",
            "       # Load the auto-generated drop list\n",
            "       exec(open('09b_features_to_drop_code.py').read())\n",
            "       X_train = X_train.drop(columns=features_to_drop_multicollinearity)\n",
            "       X_val = X_val.drop(columns=features_to_drop_multicollinearity)\n",
            "       X_test = X_test.drop(columns=features_to_drop_multicollinearity)\n",
            "       ```\n",
            "\n",
            "       ‚≠ê PRIORITY 2: VERIFY NO LEAKAGE FEATURES REMAIN\n",
            "       ```python\n",
            "       leakage_check = ['FTHome', 'FTAway', 'HTHome', 'HTAway',\n",
            "                        'HomeTarget', 'AwayTarget', 'HomeShots', 'AwayShots',\n",
            "                        'HomeCorners', 'AwayCorners', 'HomeFouls', 'AwayFouls']\n",
            "       assert not any(feat in X_train.columns for feat in leakage_check), \"LEAKAGE DETECTED!\"\n",
            "       ```\n",
            "\n",
            "       ‚≠ê PRIORITY 3: FEATURE SELECTION (MI-Based)\n",
            "       ```python\n",
            "       # Use features with MI > 0.04 (see '02_feature_relevance...' CSV)\n",
            "       selected_features = mi_scores[mi_scores > 0.04].index.tolist()\n",
            "       ```\n",
            "\n",
            "       ‚≠ê PRIORITY 4: HANDLE OUTLIERS & SKEWNESS\n",
            "       ```python\n",
            "       from sklearn.preprocessing import RobustScaler\n",
            "       import numpy as np\n",
            "\n",
            "       # Log transform skewed financial features\n",
            "       skewed_features = ['ClubValue', 'MaxPlayerValue', 'ManagerTrophies']\n",
            "       for feat in skewed_features:\n",
            "           for prefix in ['HomeTeam_', 'AwayTeam_']:\n",
            "               col = f'{prefix}{feat}'\n",
            "               if col in X_train.columns:\n",
            "                   X_train[f'{col}_log'] = np.log1p(X_train[col])\n",
            "\n",
            "       # Apply RobustScaler (IQR-based, outlier-resistant)\n",
            "       scaler = RobustScaler()\n",
            "       X_train_scaled = scaler.fit_transform(X_train)\n",
            "       ```\n",
            "\n",
            "       OTHER RECOMMENDATIONS:\n",
            "       - Address Class Imbalance: Use class_weight='balanced' (PREFERRED)\n",
            "       - SMOTE (if needed): Apply ONLY in modeling pipeline, NEVER in EDA\n",
            "       - Model Selection: Non-linear models (XGBoost, LGBM, RF) recommended\n",
            "       - Reason: EDA analyzes REAL data, SMOTE creates SYNTHETIC data\n",
            "       - Evaluation Metrics: F1-weighted, Kappa, Brier Score, RPS, ECE\n",
            "       - TEMPORAL SPLIT IS MANDATORY: NO random shuffling allowed\n",
            "\n",
            "    10. FILES GENERATED (V5.0)\n",
            "        - PNG Visualizations: ~27+ (in /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Images)\n",
            "        - CSV Data Exports: ~15+ (in /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports)\n",
            "        - PDF Report(s): (in /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/compact_report)\n",
            "        - Python Code: features_to_drop_multicollinearity list\n",
            "\n",
            "        **CRITICAL FILES FOR MODELING:**\n",
            "        1. ‚≠ê 09b_features_to_drop_code.py ‚Üí MANDATORY: Remove multicollinear features\n",
            "        2. ‚≠ê 02_feature_relevance_mutual_information_train.csv ‚Üí Feature selection guide\n",
            "        3. 09_high_correlation_pairs.csv ‚Üí Review correlation issues\n",
            "        4. 09c_multicollinearity_recommendations.csv ‚Üí Detailed removal rationale\n",
            "        5. 01_descriptive_statistics_train.csv ‚Üí Understand feature scales\n",
            "        6. 10a_outliers_by_feature_train_IQR.csv ‚Üí Confirm robust scaling needs\n",
            "\n",
            "    11. ‚úÖ VALIDATION CHECKLIST BEFORE MODELING:\n",
            "        ‚ñ° Loaded '09b_features_to_drop_code.py' and applied removals\n",
            "        ‚ñ° Verified NO leakage features (FTHome, HomeTarget, etc.) in X_train\n",
            "        ‚ñ° Confirmed temporal features (L3/L5 averages) are present\n",
            "        ‚ñ° Applied log transformation to skewed financial features\n",
            "        ‚ñ° Used RobustScaler for outlier-resistant scaling\n",
            "        ‚ñ° Selected top N features based on MI scores (>0.04 threshold)\n",
            "        ‚ñ° Used temporal split (NO random shuffling)\n",
            "        ‚ñ° Set appropriate class weights for imbalance\n",
            "    \n",
            "\n",
            "[OK] EDA Summary saved to: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/eda_summary_v5_0_leakage_cleaned.txt\n",
            "\n",
            "==========================================================================================\n",
            "[DONE] FULL EDA COMPLETE - V5.0 (DATA LEAKAGE CLEANED + TEMPORAL FEATURES)\n",
            "Updates: All match-event features converted to temporal averages\n",
            "         Multicollinearity automatically detected and drop list generated\n",
            "         Complete leakage prevention implemented\n",
            "==========================================================================================\n",
            "\n",
            "[FINAL OUTPUT STATS]\n",
            "  PNG Images: 25\n",
            "  CSV Files: 17\n",
            "  HTML Files: 0\n",
            "  PDF Files: 1\n",
            "  Text Files: 1\n",
            "  Python Code Files: 4\n",
            "\n",
            "Output directories:\n",
            "  - Images: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Images\n",
            "  - Data Exports: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/Data_Exports\n",
            "  - PDF Reports: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1/compact_report\n",
            "  - Summary Text: /content/drive/My Drive/Thesis Data/EDA_Outputs_27.11.25 V1\n",
            "\n",
            "[OK] Final PDF file closed successfully.\n",
            "\n",
            "[TIMER] Total EDA execution time: 353.89 seconds (5.90 minutes)\n",
            "[INFO] Script V5.1 ended at 2025-11-27 02:11:28\n",
            "\n",
            "====================================================================================================\n",
            "üìã EKLEME KONTROL√ú (V5.1 New Features)\n",
            "====================================================================================================\n",
            "\n",
            "[K√úT√úPHANE DURUMU]\n",
            "  ‚úÖ SMOTE Available          : True\n",
            "  ‚úÖ VIF Available            : True\n",
            "  ‚úÖ Silhouette Available     : True\n",
            "\n",
            "[ANALƒ∞Z SONU√áLARI]\n",
            "  ‚ùå Temporal Validation      : False\n",
            "  ‚ùå SMOTE Metrics            : False\n",
            "  ‚úÖ VIF Results              : True\n",
            "  ‚úÖ t-SNE Results            : True\n",
            "\n",
            "====================================================================================================\n",
            "‚ö†Ô∏è  BAZI EKLEMELER √áALI≈ûMADI - KONTROL GEREKLƒ∞\n",
            "====================================================================================================\n",
            "\n",
            "[SORUN TESPƒ∞Tƒ∞]\n",
            "  ‚ùå Temporal Validation √ßalƒ±≈ümadƒ±\n",
            "     ‚Üí Kontrol: Phase 1.6 kodu doƒüru eklenmi≈ü mi?\n",
            "     ‚Üí validation_results deƒüi≈ükeni olu≈üturulmu≈ü mu?\n",
            "  ‚ùå SMOTE Metrics √ßalƒ±≈ümadƒ±\n",
            "     ‚Üí Kontrol: Phase 4.3 kodu doƒüru eklenmi≈ü mi?\n",
            "     ‚Üí SMOTE_AVAILABLE = True mu?\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "[DOSYA √áIKTI ƒ∞STATƒ∞STƒ∞KLERƒ∞]\n",
            "  üìÅ PNG Images     :  25 dosya\n",
            "  üìÅ CSV Files      :  17 dosya\n",
            "  üìÅ PDF Files      :   1 dosya\n",
            "  üìÅ Text Files     :   1 dosya\n",
            "\n",
            "[YENƒ∞ EKLEME DOSYALARI (V5.1)]\n",
            "  ‚ùå Temporal Validation           : 01d_temporal_features_validation.png\n",
            "  ‚ùå SMOTE Analysis                : 04c_class_imbalance_smote.png\n",
            "  ‚úÖ VIF Analysis                  : 09f_vif_analysis.png\n",
            "  ‚úÖ VIF Results CSV               : 09g_vif_results.csv\n",
            "  ‚úÖ t-SNE Multi-Perplexity        : 11a_tsne_multiperplexity.png\n",
            "  ‚úÖ Feature Engineering           : 12_feature_engineering_examples.png\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "[INFO] Cleanup complete. EDA V5.1 Finished.\n",
            "\n",
            "==========================================================================================\n",
            "üéâ THANK YOU FOR USING EDA V5.1!\n",
            "   ‚ú® New Features Added:\n",
            "      ‚Ä¢ Temporal Features Validation (Phase 1.6)\n",
            "      ‚Ä¢ SMOTE Class Imbalance Analysis (Phase 4.3)\n",
            "      ‚Ä¢ VIF Multicollinearity Detection (Phase 4.4)\n",
            "      ‚Ä¢ t-SNE Multi-Perplexity Comparison (Enhanced)\n",
            "      ‚Ä¢ Feature Engineering Examples (Phase 4.9)\n",
            "\n",
            "   Next steps: Review generated files and apply recommendations to your modeling pipeline.\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# FINAL INTEGRATED EDA ‚Äî PRODUCTION-READY V5.1 (COMPLETE REVISION)\n",
        "# √ñZELLƒ∞KLER:\n",
        "#   ‚úÖ Data Leakage Tamamen Temizlendi + Temporal Features Eklendi\n",
        "#   ‚úÖ matches_all Loading Problemi √á√∂z√ºld√º (Phase 1 Eklendi)\n",
        "#   ‚úÖ Multicollinearity Detection & Auto Drop List\n",
        "#   ‚úÖ SMOTE + VIF + t-SNE + Feature Engineering Visualizations\n",
        "#\n",
        "# YENƒ∞ (V5.1):\n",
        "#   - Phase 1: CSV Loading + Data Merging + Verification\n",
        "#   - matches_all NameError √ß√∂z√ºld√º\n",
        "#   - Comprehensive error handling\n",
        "#   - 15+ validation checkpoints\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import glob\n",
        "import warnings\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import seaborn as sns\n",
        "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_object_dtype\n",
        "from scipy.stats import chi2_contingency, zscore\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import statsmodels\n",
        "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import plotly.graph_objects as go\n",
        "import plotly.offline as py\n",
        "import textwrap\n",
        "from scipy import stats\n",
        "from scipy.stats import skew\n",
        "!pip install fuzzywuzzy\n",
        "!pip install unidecode python-levenshtein\n",
        "# üÜï YENƒ∞ EKLEMELER\n",
        "from fuzzywuzzy import fuzz\n",
        "from unidecode import unidecode\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï V5.1 ƒ∞√áƒ∞N YENƒ∞ K√úT√úPHANELER (Kontroll√º Import)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# SMOTE i√ßin\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    SMOTE_AVAILABLE = True\n",
        "    print(\"[OK] imbalanced-learn (SMOTE) available.\")\n",
        "except ImportError:\n",
        "    SMOTE_AVAILABLE = False\n",
        "    print(\"[WARN] imbalanced-learn not installed. SMOTE analysis will be skipped.\")\n",
        "    print(\"       Install with: pip install imbalanced-learn\")\n",
        "\n",
        "# VIF i√ßin\n",
        "try:\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    VIF_AVAILABLE = True\n",
        "    print(\"[OK] statsmodels VIF available.\")\n",
        "except ImportError:\n",
        "    VIF_AVAILABLE = False\n",
        "    print(\"[WARN] statsmodels VIF not available. VIF analysis will be skipped.\")\n",
        "\n",
        "# Silhouette Score i√ßin (t-SNE)\n",
        "try:\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    SILHOUETTE_AVAILABLE = True\n",
        "    print(\"[OK] silhouette_score available.\")\n",
        "except ImportError:\n",
        "    SILHOUETTE_AVAILABLE = False\n",
        "    print(\"[WARN] silhouette_score not available.\")\n",
        "\n",
        "try:\n",
        "    py.init_notebook_mode(connected=True)\n",
        "except ImportError:\n",
        "    print(\"[WARN] Plotly 'init_notebook_mode' failed. (Colab dƒ±≈üƒ±nda normaldir)\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =======================\n",
        "# TIMING & LOGGING\n",
        "# =======================\n",
        "SCRIPT_START = time.time()\n",
        "print(f\"[INFO] EDA Script V5.1 (COMPLETE REVISION) started at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 100)\n",
        "print(\"üöÄ MAJOR UPDATES IN V5.1:\")\n",
        "print(\"   ‚úÖ FIXED: matches_all loading (Phase 1 added)\")\n",
        "print(\"   ‚úÖ ALL match-event features converted to temporal averages\")\n",
        "print(\"   ‚úÖ Data leakage completely eliminated\")\n",
        "print(\"   ‚úÖ High correlation pairs automatically handled\")\n",
        "print(\"   ‚úÖ SMOTE + VIF + t-SNE + Feature Engineering visualizations\")\n",
        "print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "# =======================\n",
        "# REPRODUCIBILITY & PUBLICATION STYLE\n",
        "# =======================\n",
        "RNG_SEED = 42\n",
        "np.random.seed(RNG_SEED)\n",
        "random.seed(RNG_SEED)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üé® THESIS-COMPLIANT VISUALIZATION SETTINGS (V5.2)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1. THESIS COLOR PALETTE (Mavi Tonlarƒ± - Akademik Standart)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "THESIS_COLORS = {\n",
        "    'primary': {\n",
        "        'dark_blue': '#1f3a93',      # Ana mavi (en koyu) - Ba≈ülƒ±klar, vurgu\n",
        "        'medium_blue': '#2e59d9',    # Orta mavi - Ana veriler\n",
        "        'light_blue': '#6c8cd5',     # A√ßƒ±k mavi - ƒ∞kincil veriler\n",
        "        'sky_blue': '#a4c2f4',       # √áok a√ßƒ±k mavi - Arka plan, dolgu\n",
        "    },\n",
        "    'accent': {\n",
        "        'highlight': '#ffa500',      # Turuncu (vurgu i√ßin)\n",
        "        'warning': '#ff6b6b',        # A√ßƒ±k kƒ±rmƒ±zƒ± (uyarƒ±)\n",
        "        'success': '#27ae60',        # Ye≈üil (ba≈üarƒ±)\n",
        "    },\n",
        "    'neutral': {\n",
        "        'dark_gray': '#2c3e50',      # Koyu gri (text)\n",
        "        'medium_gray': '#7f8c8d',    # Orta gri\n",
        "        'light_gray': '#ecf0f1',     # A√ßƒ±k gri (background)\n",
        "    },\n",
        "    # Geriye uyumluluk i√ßin eski isimler\n",
        "    'main_blue': '#1f3a93',\n",
        "    'light_blue': '#6c8cd5',\n",
        "    'home_green': '#27ae60',\n",
        "    'away_red': '#ff6b6b',\n",
        "    'draw_orange': '#ffa500',\n",
        "    'gray': '#7f8c8d'\n",
        "}\n",
        "\n",
        "# Kƒ±sa eri≈üim i√ßin alias\n",
        "TC = THESIS_COLORS\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üîÑ BACKWARD COMPATIBILITY - Eski kod i√ßin UNIFIED_COLORS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "UNIFIED_COLORS = {\n",
        "    'main_blue': TC['primary']['dark_blue'],      # '#1f3a93'\n",
        "    'light_blue': TC['primary']['light_blue'],    # '#6c8cd5'\n",
        "    'home_green': TC['accent']['success'],        # '#27ae60'\n",
        "    'away_red': TC['accent']['warning'],          # '#ff6b6b'\n",
        "    'draw_orange': TC['accent']['highlight'],     # '#ffa500'\n",
        "    'gray': TC['neutral']['medium_gray'],         # '#7f8c8d'\n",
        "}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 2. MATCH OUTCOME COLORS (Futbol Analizi i√ßin)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "OUTCOME_COLORS = {\n",
        "    0: TC['accent']['highlight'],    # Draw - Turuncu\n",
        "    1: TC['primary']['medium_blue'], # Home Win - Mavi\n",
        "    2: TC['accent']['warning'],      # Away Win - Kƒ±rmƒ±zƒ±\n",
        "}\n",
        "OUTCOME_NAMES = {0: 'Draw', 1: 'Home Win', 2: 'Away Win'}\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 3. CUSTOM COLORMAPS (Heatmaps i√ßin)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Mavi tonlarƒ± (correlation, feature importance)\n",
        "CMAP_BLUES = LinearSegmentedColormap.from_list(\n",
        "    'thesis_blues',\n",
        "    [TC['neutral']['light_gray'], TC['primary']['sky_blue'],\n",
        "     TC['primary']['light_blue'], TC['primary']['medium_blue'],\n",
        "     TC['primary']['dark_blue']],\n",
        "    N=256\n",
        ")\n",
        "\n",
        "# Diverging (correlation matrix - negatif/pozitif)\n",
        "CMAP_DIVERGING = LinearSegmentedColormap.from_list(\n",
        "    'thesis_diverging',\n",
        "    [TC['accent']['warning'], TC['neutral']['light_gray'], TC['primary']['dark_blue']],\n",
        "    N=256\n",
        ")\n",
        "\n",
        "# Performance colormap (k√∂t√º ‚Üí iyi)\n",
        "CMAP_PERFORMANCE = LinearSegmentedColormap.from_list(\n",
        "    'thesis_performance',\n",
        "    [TC['accent']['warning'], TC['accent']['highlight'],\n",
        "     TC['primary']['light_blue'], TC['primary']['dark_blue']],\n",
        "    N=256\n",
        ")\n",
        "\n",
        "# Matplotlib'e kaydet\n",
        "mpl.colormaps.register(CMAP_BLUES)\n",
        "mpl.colormaps.register(CMAP_DIVERGING)\n",
        "mpl.colormaps.register(CMAP_PERFORMANCE)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 4. GLOBAL MATPLOTLIB SETTINGS (Y√ºksek Kalite PDF √áƒ±ktƒ±sƒ±)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "plt.rcParams.update({\n",
        "    # Font ayarlarƒ±\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\", \"Liberation Serif\"],\n",
        "    \"font.size\": 12,\n",
        "    \"font.weight\": \"normal\",\n",
        "\n",
        "    # Eksen ayarlarƒ±\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"axes.labelweight\": \"bold\",\n",
        "    \"axes.titleweight\": \"bold\",\n",
        "    \"axes.linewidth\": 1.5,\n",
        "    \"axes.edgecolor\": TC['neutral']['dark_gray'],\n",
        "    \"axes.facecolor\": \"white\",\n",
        "    \"axes.grid\": True,\n",
        "    \"axes.axisbelow\": True,\n",
        "\n",
        "    # Tick ayarlarƒ±\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"xtick.major.width\": 1.2,\n",
        "    \"ytick.major.width\": 1.2,\n",
        "\n",
        "    # Grid ayarlarƒ±\n",
        "    \"grid.alpha\": 0.4,\n",
        "    \"grid.linestyle\": \"--\",\n",
        "    \"grid.linewidth\": 0.8,\n",
        "    \"grid.color\": TC['neutral']['medium_gray'],\n",
        "\n",
        "    # √áizgi ayarlarƒ±\n",
        "    \"lines.linewidth\": 2.5,\n",
        "    \"lines.markersize\": 8,\n",
        "\n",
        "    # Legend ayarlarƒ±\n",
        "    \"legend.fontsize\": 10,\n",
        "    \"legend.framealpha\": 0.9,\n",
        "    \"legend.edgecolor\": TC['neutral']['medium_gray'],\n",
        "\n",
        "    # Figure ayarlarƒ±\n",
        "    \"figure.facecolor\": \"white\",\n",
        "    \"figure.edgecolor\": \"white\",\n",
        "    \"figure.autolayout\": True,\n",
        "    \"figure.dpi\": 100,\n",
        "\n",
        "    # Kaydetme ayarlarƒ± (KRƒ∞Tƒ∞K!)\n",
        "    \"savefig.dpi\": 600,\n",
        "    \"savefig.format\": \"pdf\",\n",
        "    \"savefig.facecolor\": \"white\",\n",
        "    \"savefig.edgecolor\": \"white\",\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "    \"savefig.pad_inches\": 0.1,\n",
        "\n",
        "    # PDF vekt√∂rel √ßƒ±ktƒ± i√ßin\n",
        "    \"pdf.fonttype\": 42,\n",
        "    \"ps.fonttype\": 42,\n",
        "\n",
        "    # LaTeX desteƒüi (opsiyonel)\n",
        "    \"text.usetex\": False,\n",
        "    \"mathtext.fontset\": \"dejavuserif\",\n",
        "})\n",
        "\n",
        "# Seaborn tema ayarlarƒ±\n",
        "sns.set_style(\"whitegrid\", {\n",
        "    \"axes.facecolor\": \"white\",\n",
        "    \"grid.color\": TC['neutral']['medium_gray'],\n",
        "    \"grid.linestyle\": \"--\",\n",
        "})\n",
        "\n",
        "# Seaborn palette (thesis renkleriyle)\n",
        "sns.set_palette([\n",
        "    TC['primary']['dark_blue'],\n",
        "    TC['primary']['medium_blue'],\n",
        "    TC['primary']['light_blue'],\n",
        "    TC['accent']['highlight'],\n",
        "    TC['accent']['warning'],\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Thesis-compliant visualization settings loaded (V5.2)\")\n",
        "print(f\"   ‚Ä¢ Color palette: Mavi tonlarƒ± (akademik standart)\")\n",
        "print(f\"   ‚Ä¢ Output format: PDF (vekt√∂rel) + PNG (preview)\")\n",
        "print(f\"   ‚Ä¢ DPI: 600 (yayƒ±n kalitesi)\")\n",
        "\n",
        "# =======================\n",
        "# GOOGLE DRIVE SETUP\n",
        "# =======================\n",
        "print(\"[INFO] Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', timeout_ms=120000)\n",
        "    print(\"[OK] Drive mounted successfully\")\n",
        "    BASE_PATH = \"/content/drive/My Drive/Thesis Data/\"\n",
        "except ImportError:\n",
        "    print(\"[INFO] Not in Colab environment, assuming local BASE_PATH='./Thesis Data/'\")\n",
        "    BASE_PATH = \"./Thesis Data/\"\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Drive mount failed: {e}. Assuming local BASE_PATH='./Thesis Data/'\")\n",
        "    BASE_PATH = \"./Thesis Data/\"\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "# =======================\n",
        "# CONFIG & PATHS & OUTPUT DIRS\n",
        "# =======================\n",
        "OUTPUT_BASE_PATH = os.path.join(BASE_PATH, \"EDA_Outputs_27.11.25 V1\")\n",
        "OUT_DIR = OUTPUT_BASE_PATH\n",
        "\n",
        "try:\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    COMPACT_DIR = os.path.join(OUT_DIR, \"compact_report\")\n",
        "    os.makedirs(COMPACT_DIR, exist_ok=True)\n",
        "    IMAGES_DIR = os.path.join(OUT_DIR, \"Images\")\n",
        "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "    CSV_DIR = os.path.join(OUT_DIR, \"Data_Exports\")\n",
        "    os.makedirs(CSV_DIR, exist_ok=True)\n",
        "    print(f\"[OK] Output directories validated/created: {OUT_DIR}\\n\")\n",
        "except OSError as e:\n",
        "    print(f\"[ERROR] Cannot create output directories: {e}\")\n",
        "    OUT_DIR = os.getcwd()\n",
        "    COMPACT_DIR = os.path.join(OUT_DIR, \"compact_report\")\n",
        "    os.makedirs(COMPACT_DIR, exist_ok=True)\n",
        "    IMAGES_DIR = os.path.join(OUT_DIR, \"Images\")\n",
        "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "    CSV_DIR = os.path.join(OUT_DIR, \"Data_Exports\")\n",
        "    os.makedirs(CSV_DIR, exist_ok=True)\n",
        "    print(f\"[WARN] Using current working directory for outputs: {OUT_DIR}\\n\")\n",
        "\n",
        "# PDF Raporlama i√ßin ayarlar\n",
        "_pdf = PdfPages(os.path.join(COMPACT_DIR, \"EDA_Outputs_24.11.25 V1.pdf\"))\n",
        "_pdf_page_count = 0\n",
        "_pdf_part = 1\n",
        "PAGES_PER_PDF = 100\n",
        "\n",
        "# =======================\n",
        "# UTILITY FUNCTIONS\n",
        "# =======================\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üìê THESIS-COMPLIANT SAVE FUNCTIONS (V5.2)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def savefig_report(page_name_hint=None, also_png=True, also_pdf_standalone=True):\n",
        "    \"\"\"\n",
        "    Enhanced save function - saves to report PDF + standalone PNG + standalone PDF.\n",
        "\n",
        "    V5.2 Updates:\n",
        "        - Always saves both PNG and standalone PDF (vekt√∂rel)\n",
        "        - Thesis-compliant metadata\n",
        "        - High DPI (600) for publication quality\n",
        "        - White background enforced\n",
        "\n",
        "    Args:\n",
        "        page_name_hint: Filename hint (without extension)\n",
        "        also_png: Save standalone PNG (default: True)\n",
        "        also_pdf_standalone: Save standalone PDF (default: True)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (png_path, pdf_path) or None if failed\n",
        "    \"\"\"\n",
        "    global _pdf, _pdf_page_count, _pdf_part, PAGES_PER_PDF, COMPACT_DIR, IMAGES_DIR\n",
        "\n",
        "    current_fig = plt.gcf()\n",
        "\n",
        "    # Bo≈ü fig√ºr kontrol√º\n",
        "    if not current_fig.get_axes():\n",
        "        print(f\"[WARN] Attempted to save an empty figure (Hint: {page_name_hint}). Skipping.\")\n",
        "        plt.close(current_fig)\n",
        "        return None, None\n",
        "\n",
        "    # Tight layout uygula\n",
        "    try:\n",
        "        current_fig.tight_layout()\n",
        "    except Exception as e_layout:\n",
        "        print(f\"[WARN] tight_layout failed for '{page_name_hint}': {e_layout}\")\n",
        "\n",
        "    # Kaydetme parametreleri (thesis-compliant)\n",
        "    save_params_common = {\n",
        "        'bbox_inches': 'tight',\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'none',\n",
        "        'pad_inches': 0.1,\n",
        "    }\n",
        "\n",
        "    # PDF metadata\n",
        "    pdf_metadata = {\n",
        "        'Title': page_name_hint or 'EDA Figure',\n",
        "        'Author': 'Dokumus - Tilburg University',\n",
        "        'Subject': 'Football Match Prediction - EDA Analysis',\n",
        "        'Keywords': 'Machine Learning, XAI, Football Prediction, LTCN',\n",
        "        'Creator': 'Matplotlib / EDA V5.2',\n",
        "        'Producer': 'Thesis Research - Data Science & Society'\n",
        "    }\n",
        "\n",
        "    png_path = None\n",
        "    pdf_path = None\n",
        "\n",
        "    def _safe_savefig(path, format_type='pdf', dpi=600):\n",
        "        \"\"\"Internal save function with error handling.\"\"\"\n",
        "        try:\n",
        "            if format_type == 'report_pdf':\n",
        "                # Ana PDF raporuna ekle\n",
        "                _pdf.savefig(current_fig, **save_params_common)\n",
        "            elif format_type == 'png':\n",
        "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "                current_fig.savefig(\n",
        "                    path,\n",
        "                    format='png',\n",
        "                    dpi=dpi,\n",
        "                    **save_params_common,\n",
        "                    pil_kwargs={\"optimize\": True, \"compress_level\": 6}\n",
        "                )\n",
        "            elif format_type == 'pdf':\n",
        "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "                current_fig.savefig(\n",
        "                    path,\n",
        "                    format='pdf',\n",
        "                    dpi=600,  # PDF i√ßin 300 yeterli (vekt√∂rel)\n",
        "                    metadata=pdf_metadata,\n",
        "                    **save_params_common\n",
        "                )\n",
        "            return True\n",
        "        except Exception as e_save:\n",
        "            print(f\"[ERROR] Failed to save figure ({format_type}): {e_save}\")\n",
        "            return False\n",
        "\n",
        "    try:\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # 1. ANA PDF RAPORUNA EKLE\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if _safe_savefig(None, format_type='report_pdf'):\n",
        "            _pdf_page_count += 1\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # 2. STANDALONE DOSYALAR (PNG + PDF)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if page_name_hint:\n",
        "            # G√ºvenli dosya adƒ± olu≈ütur\n",
        "            safe_hint = re.sub(r'[\\\\/*?:\"<>|]+', \"_\", page_name_hint)\n",
        "            safe_hint = safe_hint.replace('.png', '').replace('.pdf', '')\n",
        "\n",
        "            base_path = os.path.join(IMAGES_DIR, safe_hint)\n",
        "\n",
        "            # PNG kaydet (preview ve web i√ßin)\n",
        "            if also_png:\n",
        "                png_path = f\"{base_path}.png\"\n",
        "                if not _safe_savefig(png_path, format_type='png', dpi=600):\n",
        "                    png_path = None\n",
        "\n",
        "            # Standalone PDF kaydet (thesis i√ßin - vekt√∂rel!)\n",
        "            if also_pdf_standalone:\n",
        "                pdf_path = f\"{base_path}.pdf\"\n",
        "                if not _safe_savefig(pdf_path, format_type='pdf'):\n",
        "                    pdf_path = None\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # 3. PDF SAYFA KONTROL√ú (√áok sayfalƒ± raporlar i√ßin)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if _pdf_page_count >= PAGES_PER_PDF:\n",
        "            print(f\"[INFO] Closing PDF part {_pdf_part} with {_pdf_page_count} pages...\")\n",
        "            _pdf.close()\n",
        "            _pdf_part += 1\n",
        "            new_pdf_path = os.path.join(COMPACT_DIR, f\"EDA_Report_V5_2_part{_pdf_part:02d}.pdf\")\n",
        "            _pdf = PdfPages(new_pdf_path)\n",
        "            _pdf_page_count = 0\n",
        "            print(f\"[INFO] Starting new PDF part: {os.path.basename(new_pdf_path)}\")\n",
        "\n",
        "    except Exception as e_report:\n",
        "        print(f\"[ERROR] Error occurred in savefig_report for '{page_name_hint}': {e_report}\")\n",
        "\n",
        "    finally:\n",
        "        plt.close(current_fig)\n",
        "\n",
        "    return png_path, pdf_path\n",
        "\n",
        "\n",
        "def save_df_as_image(df_to_save, filename_hint, title=\"\"):\n",
        "    \"\"\"\n",
        "    Saves a pandas DataFrame as a styled image table (PNG + PDF).\n",
        "\n",
        "    V5.2 Updates:\n",
        "        - Thesis-compliant color scheme (mavi tonlarƒ±)\n",
        "        - Both PNG and PDF output\n",
        "        - Professional table styling\n",
        "        - Better readability\n",
        "\n",
        "    Args:\n",
        "        df_to_save: pandas DataFrame to save\n",
        "        filename_hint: Filename hint (without extension)\n",
        "        title: Optional title for the table\n",
        "    \"\"\"\n",
        "    if df_to_save is None or df_to_save.empty:\n",
        "        print(f\"[WARN] DataFrame to save as image '{filename_hint}' is empty. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # THESIS COLORS (Tablo i√ßin)\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    TABLE_COLORS = {\n",
        "        'header_bg': '#1f3a93',           # Koyu mavi (header)\n",
        "        'header_text': 'white',           # Beyaz text\n",
        "        'row_label_bg': '#a4c2f4',        # A√ßƒ±k mavi (row labels)\n",
        "        'row_label_text': '#2c3e50',      # Koyu gri text\n",
        "        'cell_even': '#f8f9fa',           # √áok a√ßƒ±k gri (√ßift satƒ±rlar)\n",
        "        'cell_odd': 'white',              # Beyaz (tek satƒ±rlar)\n",
        "        'cell_text': '#2c3e50',           # Koyu gri text\n",
        "        'border': '#d1d5db',              # A√ßƒ±k gri border\n",
        "        'title': '#1f3a93',               # Koyu mavi ba≈ülƒ±k\n",
        "    }\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # FIG√úR BOYUTU HESAPLAMA\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    nrows, ncols = df_to_save.shape\n",
        "\n",
        "    # Dinamik boyutlandƒ±rma\n",
        "    fig_w = max(10, 1.4 * ncols + 3)\n",
        "    fig_h = max(5, 0.35 * nrows + 2.5)\n",
        "\n",
        "    # Maksimum sƒ±nƒ±rlar\n",
        "    fig_w = min(fig_w, 45)\n",
        "    fig_h = min(fig_h, 55)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Beyaz arka plan\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # S√úTUN ETƒ∞KETLERƒ∞\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if isinstance(df_to_save.columns, pd.MultiIndex):\n",
        "        col_labels = ['_'.join(map(str, col)).strip() for col in df_to_save.columns.values]\n",
        "    else:\n",
        "        col_labels = [str(c) for c in df_to_save.columns]\n",
        "\n",
        "    # Uzun s√ºtun isimlerini wrap et\n",
        "    WRAP_WIDTH = 18\n",
        "    wrapped_col_labels = []\n",
        "    for c in col_labels:\n",
        "        if len(c) > WRAP_WIDTH:\n",
        "            wrapped_col_labels.append(textwrap.fill(c, width=WRAP_WIDTH))\n",
        "        else:\n",
        "            wrapped_col_labels.append(c)\n",
        "    col_labels = wrapped_col_labels\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # SATIR ETƒ∞KETLERƒ∞\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if isinstance(df_to_save.index, pd.MultiIndex):\n",
        "        row_labels = ['_'.join(map(str, idx)).strip() for idx in df_to_save.index.values]\n",
        "    else:\n",
        "        row_labels = [str(i) for i in df_to_save.index]\n",
        "\n",
        "    # H√ºcre i√ßeriƒüi\n",
        "    cell_text = df_to_save.fillna('‚Äî').astype(str).values  # NaN yerine em-dash\n",
        "\n",
        "    try:\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # TABLO OLU≈ûTUR\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        table = ax.table(\n",
        "            cellText=cell_text,\n",
        "            colLabels=col_labels,\n",
        "            rowLabels=row_labels,\n",
        "            cellLoc='center',\n",
        "            loc='center',\n",
        "        )\n",
        "\n",
        "        # Font boyutu (dinamik)\n",
        "        base_fs = 10\n",
        "        if ncols > 20 or nrows > 50:\n",
        "            base_fs = 7\n",
        "        elif ncols > 15 or nrows > 35:\n",
        "            base_fs = 8\n",
        "        elif ncols > 10 or nrows > 20:\n",
        "            base_fs = 9\n",
        "\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(base_fs)\n",
        "        table.scale(1.2, 1.5)\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # H√úCRE STƒ∞LLERƒ∞ (Thesis-Compliant)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        for (row_idx, col_idx), cell in table.get_celld().items():\n",
        "            cell.set_edgecolor(TABLE_COLORS['border'])\n",
        "            cell.set_linewidth(0.8)\n",
        "\n",
        "            if row_idx == 0:\n",
        "                # HEADER ROW (Koyu mavi arka plan, beyaz text)\n",
        "                cell.set_text_props(\n",
        "                    weight='bold',\n",
        "                    color=TABLE_COLORS['header_text'],\n",
        "                    fontsize=base_fs\n",
        "                )\n",
        "                cell.set_facecolor(TABLE_COLORS['header_bg'])\n",
        "                cell.set_height(cell.get_height() * 1.3)\n",
        "\n",
        "            elif col_idx == -1:\n",
        "                # ROW LABELS (A√ßƒ±k mavi arka plan)\n",
        "                cell.set_text_props(\n",
        "                    weight='bold',\n",
        "                    color=TABLE_COLORS['row_label_text'],\n",
        "                    fontsize=base_fs - 1\n",
        "                )\n",
        "                cell.set_facecolor(TABLE_COLORS['row_label_bg'])\n",
        "\n",
        "            else:\n",
        "                # DATA CELLS (Alternatif renklendirme)\n",
        "                cell.set_text_props(\n",
        "                    color=TABLE_COLORS['cell_text'],\n",
        "                    fontsize=base_fs - 1\n",
        "                )\n",
        "                if (row_idx - 1) % 2 == 0:\n",
        "                    cell.set_facecolor(TABLE_COLORS['cell_even'])\n",
        "                else:\n",
        "                    cell.set_facecolor(TABLE_COLORS['cell_odd'])\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # BA≈ûLIK\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if title:\n",
        "            fig.suptitle(\n",
        "                title,\n",
        "                fontsize=14,\n",
        "                fontweight='bold',\n",
        "                color=TABLE_COLORS['title'],\n",
        "                y=0.98\n",
        "            )\n",
        "\n",
        "        plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0.05)\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # KAYDET (PNG + PDF)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        savefig_report(filename_hint, also_png=True, also_pdf_standalone=True)\n",
        "\n",
        "        print(f\"[OK] DataFrame table saved: {filename_hint} (PNG + PDF)\")\n",
        "\n",
        "    except Exception as e_table:\n",
        "        print(f\"[ERROR] Failed to save DataFrame as image ({filename_hint}): {e_table}\")\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def save_figure_dual_format(fig, base_path, dpi=600, close_fig=True):\n",
        "    \"\"\"\n",
        "    Standalone function to save any figure in both PNG and PDF formats.\n",
        "\n",
        "    Use this for custom figures that don't go through savefig_report.\n",
        "\n",
        "    Args:\n",
        "        fig: matplotlib figure object\n",
        "        base_path: Path without extension (e.g., '/path/to/figure_01')\n",
        "        dpi: Resolution for PNG (PDF is always vector)\n",
        "        close_fig: Whether to close figure after saving\n",
        "\n",
        "    Returns:\n",
        "        tuple: (png_path, pdf_path)\n",
        "\n",
        "    Example:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.plot(x, y)\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, '/output/my_plot')\n",
        "    \"\"\"\n",
        "\n",
        "    save_params = {\n",
        "        'bbox_inches': 'tight',\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'none',\n",
        "        'pad_inches': 0.1,\n",
        "    }\n",
        "\n",
        "    # Dizin olu≈ütur\n",
        "    os.makedirs(os.path.dirname(base_path), exist_ok=True)\n",
        "\n",
        "    png_path = None\n",
        "    pdf_path = None\n",
        "\n",
        "    # PNG kaydet\n",
        "    try:\n",
        "        png_path = f\"{base_path}.png\"\n",
        "        fig.savefig(png_path, dpi=dpi, format='png', **save_params)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] PNG save failed for {base_path}: {e}\")\n",
        "        png_path = None\n",
        "\n",
        "    # PDF kaydet (vekt√∂rel)\n",
        "    try:\n",
        "        pdf_path = f\"{base_path}.pdf\"\n",
        "        pdf_metadata = {\n",
        "            'Title': os.path.basename(base_path),\n",
        "            'Author': 'Dokumus - Tilburg University',\n",
        "            'Subject': 'Football Match Prediction - EDA',\n",
        "            'Creator': 'Matplotlib / EDA V5.2',\n",
        "        }\n",
        "        fig.savefig(pdf_path, format='pdf', dpi=600,\n",
        "                   metadata=pdf_metadata, **save_params)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] PDF save failed for {base_path}: {e}\")\n",
        "        pdf_path = None\n",
        "\n",
        "    if close_fig:\n",
        "        plt.close(fig)\n",
        "\n",
        "    return png_path, pdf_path\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 1: DATA LOADING & PREPROCESSING (COMPLETE REVISION)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 1: LOADING AND PREPROCESSING DATA\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 1.0: FUZZY MATCHING SETUP\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 1.0: FUZZY MATCHING & DATA INTEGRATION SETUP\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "class DataIntegration:\n",
        "    \"\"\"Advanced data integration with fuzzy matching\"\"\"\n",
        "\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = base_path\n",
        "        self.df_matches = None\n",
        "        self.df_elo = None\n",
        "        self.df_tm = None\n",
        "        self.team_mapping = {}\n",
        "        self.team_scores = {}\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üî• MODEL KODUNDAN: NORMALIZATION HELPER (AYNI KALIYOR)\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        def _helper_normalize_for_key(s):\n",
        "            \"\"\"MODEL KODUNDAN - %97 E≈ûLE≈ûMENƒ∞N ANAHTARI!\"\"\"\n",
        "            if pd.isna(s):\n",
        "                return None\n",
        "            s = str(s).strip()\n",
        "            s = unidecode(s)\n",
        "\n",
        "            suffixes = [\n",
        "                ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc',\n",
        "                ' SK', ' sk', ' Sk', ' FK', ' fk', ' Fk',\n",
        "                ' AC', ' ac', ' Ac', ' AS', ' as', ' As',\n",
        "                ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf',\n",
        "                ' HSC', ' hsc', ' Hsc', ' BC', ' bc', ' Bc',\n",
        "                ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',\n",
        "                ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',\n",
        "                ' 1909', ' 1846', ' 1860', ' 2010', ' 1913',\n",
        "                ' 1907', ' 1936', ' 1919', ' 2025',\n",
        "                ' spor', ' kul√ºb√º', ' klub',\n",
        "            ]\n",
        "\n",
        "            for suffix in suffixes:\n",
        "                s = s.replace(suffix, '')\n",
        "\n",
        "            s = s.replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            words = sorted(s.lower().split())\n",
        "            s = ' '.join(words)\n",
        "            s = re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "            return s if s else None\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üî• DEƒûƒ∞≈ûƒ∞KLƒ∞K 1: CSV Y√úKLEME - MODEL KODUNDAN APPLY YAKLA≈ûIMI\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        self.MANUAL_OVERRIDES = {}\n",
        "        team_list_csv = os.path.join(base_path, \"Takim_Listesi_Temiz_1.csv\")\n",
        "\n",
        "        try:\n",
        "            print(f\"  üìÑ Loading manual overrides from CSV...\")\n",
        "\n",
        "            df_temp = None\n",
        "            for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:\n",
        "                try:\n",
        "                    df_temp = pd.read_csv(\n",
        "                        team_list_csv,\n",
        "                        header=None,\n",
        "                        skiprows=1,\n",
        "                        quotechar='\"',\n",
        "                        sep='|',\n",
        "                        encoding=encoding\n",
        "                    )\n",
        "                    print(f\"     ‚úì CSV read with encoding: {encoding}\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "                except FileNotFoundError:\n",
        "                    raise\n",
        "\n",
        "            if df_temp is None:\n",
        "                raise Exception(\"CSV could not be read with any encoding!\")\n",
        "\n",
        "            df_team_list = df_temp[0].str.split('\\t', expand=True)\n",
        "            df_team_list.columns = ['Matches', 'Transfermarkt']\n",
        "\n",
        "            # üî• YENƒ∞: MODEL KODUNDAN - _process_team_row FONKSIYONU\n",
        "            def _process_team_row(row):\n",
        "                \"\"\"MODEL KODUNDAN: Her iki tarafƒ± da normalize et\"\"\"\n",
        "                matches_name = row['Matches']\n",
        "                tm_name = row['Transfermarkt']\n",
        "\n",
        "                if pd.notna(matches_name) and pd.notna(tm_name):\n",
        "                    # üî• KRƒ∞Tƒ∞K: ƒ∞Kƒ∞ TARAF DA NORMALIZE EDƒ∞Lƒ∞YOR\n",
        "                    normalized_key = _helper_normalize_for_key(matches_name)\n",
        "                    normalized_value = _helper_normalize_for_key(tm_name)\n",
        "\n",
        "                    if normalized_key and normalized_value:\n",
        "                        return normalized_key, normalized_value\n",
        "                return None\n",
        "\n",
        "            # üî• YENƒ∞: APPLY ƒ∞LE T√úM SATIRLARI ƒ∞≈ûLE (MODEL KODUNDAN)\n",
        "            for result in df_team_list.apply(_process_team_row, axis=1):\n",
        "                if result is not None:\n",
        "                    key, value = result\n",
        "                    self.MANUAL_OVERRIDES[key] = value\n",
        "\n",
        "            print(f\"     ‚úì Loaded {len(self.MANUAL_OVERRIDES)} overrides from CSV\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"     ‚ö†Ô∏è  CSV not found: {team_list_csv}\")\n",
        "            print(\"        Using hardcoded overrides only...\")\n",
        "        except Exception as e:\n",
        "            print(f\"     ‚ö†Ô∏è  Error loading CSV: {e}\")\n",
        "            print(\"        Using hardcoded overrides only...\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üî• DEƒûƒ∞≈ûƒ∞KLƒ∞K 2: HARDCODED OVERRIDES - MODEL KODUNDAN TAM MANTIK\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"  üîß Applying hardcoded critical overrides...\")\n",
        "\n",
        "        # üî• YENƒ∞: √ñNCE TRANSFERMARKT TARGET ƒ∞Sƒ∞MLERƒ∞Nƒ∞ NORMALIZE ET\n",
        "        tm_ac_ajaccio_normalized = _helper_normalize_for_key(\"AC Ajaccio\")\n",
        "        tm_gfc_ajaccio_normalized = _helper_normalize_for_key(\"GFC Ajaccio\")\n",
        "        tm_cordoba_normalized = _helper_normalize_for_key(\"C√≥rdoba CF\")\n",
        "        tm_eibar_normalized = _helper_normalize_for_key(\"SD Eibar\")\n",
        "        tm_mallorca_normalized = _helper_normalize_for_key(\"RCD Mallorca\")\n",
        "        tm_celta_normalized = _helper_normalize_for_key(\"RC Celta de Vigo\")\n",
        "        tm_betis_normalized = _helper_normalize_for_key(\"Real Betis\")\n",
        "        tm_sociedad_normalized = _helper_normalize_for_key(\"Real Sociedad\")\n",
        "        tm_alaves_normalized = _helper_normalize_for_key(\"Deportivo Alav√©s\")\n",
        "        tm_cadiz_normalized = _helper_normalize_for_key(\"C√°diz CF\")\n",
        "        tm_getafe_normalized = _helper_normalize_for_key(\"Getafe CF\")\n",
        "\n",
        "        # üî• YENƒ∞: SONRA MATCHES SOURCE ƒ∞Sƒ∞MLERƒ∞Nƒ∞ NORMALIZE ET VE OVERRIDE'LARI OLU≈ûTUR\n",
        "\n",
        "        # Ajaccio ‚Üí AC Ajaccio\n",
        "        key_aja = _helper_normalize_for_key(\"Ajaccio\")\n",
        "        if key_aja and tm_ac_ajaccio_normalized:\n",
        "            self.MANUAL_OVERRIDES.pop(key_aja, None)  # Eski override'ƒ± sil\n",
        "            self.MANUAL_OVERRIDES[key_aja] = tm_ac_ajaccio_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_aja}' ‚Üí '{tm_ac_ajaccio_normalized}' (AC Ajaccio)\")\n",
        "\n",
        "        # Ajaccio GFCO ‚Üí GFC Ajaccio\n",
        "        key_gfco = _helper_normalize_for_key(\"Ajaccio GFCO\")\n",
        "        if key_gfco and tm_gfc_ajaccio_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_gfco] = tm_gfc_ajaccio_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_gfco}' ‚Üí '{tm_gfc_ajaccio_normalized}' (GFC Ajaccio)\")\n",
        "\n",
        "        # Cordoba ‚Üí C√≥rdoba CF\n",
        "        key_cor = _helper_normalize_for_key(\"Cordoba\")\n",
        "        if key_cor and tm_cordoba_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_cor] = tm_cordoba_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_cor}' ‚Üí '{tm_cordoba_normalized}' (C√≥rdoba CF)\")\n",
        "\n",
        "        # Eibar ‚Üí SD Eibar\n",
        "        key_eibar = _helper_normalize_for_key(\"Eibar\")\n",
        "        if key_eibar and tm_eibar_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_eibar] = tm_eibar_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_eibar}' ‚Üí '{tm_eibar_normalized}' (SD Eibar)\")\n",
        "\n",
        "        # Mallorca ‚Üí RCD Mallorca\n",
        "        key_mallorca = _helper_normalize_for_key(\"Mallorca\")\n",
        "        if key_mallorca and tm_mallorca_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_mallorca] = tm_mallorca_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_mallorca}' ‚Üí '{tm_mallorca_normalized}' (RCD Mallorca)\")\n",
        "\n",
        "        # Celta ‚Üí RC Celta de Vigo\n",
        "        key_celta = _helper_normalize_for_key(\"Celta\")\n",
        "        if key_celta and tm_celta_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_celta] = tm_celta_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_celta}' ‚Üí '{tm_celta_normalized}' (RC Celta de Vigo)\")\n",
        "\n",
        "        # Betis ‚Üí Real Betis\n",
        "        key_betis = _helper_normalize_for_key(\"Betis\")\n",
        "        if key_betis and tm_betis_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_betis] = tm_betis_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_betis}' ‚Üí '{tm_betis_normalized}' (Real Betis)\")\n",
        "\n",
        "        # Sociedad ‚Üí Real Sociedad\n",
        "        key_sociedad = _helper_normalize_for_key(\"Sociedad\")\n",
        "        if key_sociedad and tm_sociedad_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_sociedad] = tm_sociedad_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_sociedad}' ‚Üí '{tm_sociedad_normalized}' (Real Sociedad)\")\n",
        "\n",
        "        # Alaves ‚Üí Deportivo Alav√©s\n",
        "        key_alaves = _helper_normalize_for_key(\"Alaves\")\n",
        "        if key_alaves and tm_alaves_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_alaves] = tm_alaves_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_alaves}' ‚Üí '{tm_alaves_normalized}' (Deportivo Alav√©s)\")\n",
        "\n",
        "        # Cadiz ‚Üí C√°diz CF\n",
        "        key_cadiz = _helper_normalize_for_key(\"Cadiz\")\n",
        "        if key_cadiz and tm_cadiz_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_cadiz] = tm_cadiz_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_cadiz}' ‚Üí '{tm_cadiz_normalized}' (C√°diz CF)\")\n",
        "\n",
        "        # Getafe ‚Üí Getafe CF\n",
        "        key_getafe = _helper_normalize_for_key(\"Getafe\")\n",
        "        if key_getafe and tm_getafe_normalized:\n",
        "            self.MANUAL_OVERRIDES[key_getafe] = tm_getafe_normalized\n",
        "            print(f\"     ‚Ä¢ Override: '{key_getafe}' ‚Üí '{tm_getafe_normalized}' (Getafe CF)\")\n",
        "\n",
        "        print(f\"     ‚úì Added 11 hardcoded overrides\")\n",
        "        print(f\"     ‚úì Total overrides: {len(self.MANUAL_OVERRIDES)}\\n\")\n",
        "\n",
        "    def normalize_team_name(self, s):\n",
        "        \"\"\"Normalize team name\"\"\"\n",
        "        if pd.isna(s):\n",
        "            return None\n",
        "\n",
        "        s = str(s).strip()\n",
        "        s = unidecode(s)\n",
        "\n",
        "        suffixes = [\n",
        "            ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc',\n",
        "            ' SK', ' sk', ' Sk', ' FK', ' fk', ' Fk',\n",
        "            ' AC', ' ac', ' Ac', ' AS', ' as', ' As',\n",
        "            ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf',\n",
        "            ' HSC', ' hsc', ' Hsc', ' BC', ' bc', ' Bc',\n",
        "            ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',\n",
        "            ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',\n",
        "            ' 1909', ' 1846', ' 1860', ' 2010', ' 1913',\n",
        "            ' 1907', ' 1936', ' 1919', ' 2025',\n",
        "            ' spor', ' kul√ºb√º', ' klub',\n",
        "        ]\n",
        "\n",
        "        for suffix in suffixes:\n",
        "            s = s.replace(suffix, '')\n",
        "\n",
        "        s = s.replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "\n",
        "        # üî• KRƒ∞Tƒ∞K: Kelime sƒ±ralama\n",
        "        words = sorted(s.lower().split())\n",
        "        s = ' '.join(words)\n",
        "        s = re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "        # Manuel override uygula\n",
        "        if s in self.MANUAL_OVERRIDES:\n",
        "            s = self.MANUAL_OVERRIDES[s]\n",
        "\n",
        "        return s if s else None\n",
        "\n",
        "    def find_best_match(self, match_team, tm_teams, threshold=80):\n",
        "        \"\"\"Find best match using fuzzy matching\"\"\"\n",
        "        if pd.isna(match_team):\n",
        "            return None, 0\n",
        "\n",
        "        match_norm = self.normalize_team_name(match_team)\n",
        "\n",
        "        if not match_norm:\n",
        "            return None, 0\n",
        "\n",
        "        # tm_teams = [(original_name, normalized_name), ...]\n",
        "        tm_norm_dict = {norm: orig for orig, norm in tm_teams if norm}\n",
        "\n",
        "        # Exact match kontrol√º\n",
        "        if match_norm in tm_norm_dict:\n",
        "            return tm_norm_dict[match_norm], 100\n",
        "\n",
        "        # Fuzzy match\n",
        "        best_score = 0\n",
        "        best_match = None\n",
        "\n",
        "        for tm_orig, tm_norm in tm_teams:\n",
        "            if not tm_norm:\n",
        "                continue\n",
        "\n",
        "            score = fuzz.token_sort_ratio(match_norm, tm_norm)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = tm_orig\n",
        "\n",
        "        if best_score >= threshold:\n",
        "            return best_match, best_score\n",
        "\n",
        "        return None, best_score\n",
        "\n",
        "    def create_team_mapping(self, df_matches, df_tm):\n",
        "        \"\"\"Create mapping between Matches and Transfermarkt\"\"\"\n",
        "        print(\"\\n  üîó Creating team mappings with fuzzy matching...\")\n",
        "\n",
        "        # Unique takƒ±mlar\n",
        "        matches_teams = sorted(\n",
        "            set(df_matches['HomeTeam'].dropna().unique()) |\n",
        "            set(df_matches['AwayTeam'].dropna().unique())\n",
        "        )\n",
        "\n",
        "        tm_teams_orig = sorted(df_tm['ClubName'].dropna().unique())\n",
        "\n",
        "        # Normalize et\n",
        "        tm_teams_normalized = [\n",
        "            (team, self.normalize_team_name(team))\n",
        "            for team in tm_teams_orig\n",
        "        ]\n",
        "\n",
        "        print(f\"     ‚Ä¢ Matches teams: {len(matches_teams)}\")\n",
        "        print(f\"     ‚Ä¢ Transfermarkt teams: {len(tm_teams_orig)}\")\n",
        "\n",
        "        # Mapping\n",
        "        for idx, team in enumerate(matches_teams):\n",
        "            if idx % 50 == 0 and idx > 0:\n",
        "                print(f\"       Progress: {idx}/{len(matches_teams)}\", end='\\r')\n",
        "\n",
        "            mapped, score = self.find_best_match(team, tm_teams_normalized, threshold=80)\n",
        "            self.team_mapping[team] = mapped\n",
        "            self.team_scores[team] = score\n",
        "\n",
        "        print(f\"       Progress: {len(matches_teams)}/{len(matches_teams)}\")\n",
        "\n",
        "        matched = sum(1 for v in self.team_mapping.values() if v is not None)\n",
        "        match_rate = (matched / len(self.team_mapping)) * 100\n",
        "\n",
        "        print(f\"     ‚úì Matched: {matched}/{len(self.team_mapping)} ({match_rate:.1f}%)\")\n",
        "\n",
        "        # Unmatched g√∂ster\n",
        "        unmatched = [k for k, v in self.team_mapping.items() if v is None]\n",
        "        if unmatched:\n",
        "            print(f\"     ‚ö†Ô∏è  Unmatched teams ({len(unmatched)}):\")\n",
        "            for team in unmatched[:10]:\n",
        "                print(f\"        ‚Ä¢ {team}\")\n",
        "            if len(unmatched) > 10:\n",
        "                print(f\"        ... and {len(unmatched)-10} more\")\n",
        "\n",
        "        # Low score warnings\n",
        "        low_scores = {k: v for k, v in self.team_scores.items()\n",
        "                      if v is not None and v < 85 and k in self.team_mapping and self.team_mapping[k] is not None}\n",
        "        if low_scores:\n",
        "            print(f\"\\n     ‚ö†Ô∏è  Low confidence matches (<85 score):\")\n",
        "            for team, score in sorted(low_scores.items(), key=lambda x: x[1])[:10]:\n",
        "                mapped = self.team_mapping[team]\n",
        "                print(f\"        ‚Ä¢ {team:30s} ‚Üí {mapped:30s} ({score})\")\n",
        "\n",
        "        return self.team_mapping\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# Initialize Fuzzy Matcher\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"[INFO] Initializing fuzzy matcher...\")\n",
        "fuzzy_matcher = DataIntegration(base_path=BASE_PATH)\n",
        "print(\"[OK] Fuzzy matching setup complete ‚úÖ\\n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.1: Define file paths\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "matches_csv = os.path.join(BASE_PATH, \"Matches.csv\")\n",
        "elo_csv = os.path.join(BASE_PATH, \"EloRatings.csv\")\n",
        "transfermarkt_csv = os.path.join(BASE_PATH, \"Transfermarkt_Data.csv\")\n",
        "\n",
        "print(f\"[INFO] Expected file paths:\")\n",
        "print(f\"  ‚Üí Matches:       {matches_csv}\")\n",
        "print(f\"  ‚Üí ELO Ratings:   {elo_csv}\")\n",
        "print(f\"  ‚Üí Transfermarkt: {transfermarkt_csv}\\n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.2: Load main dataset (CRITICAL!)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if not os.path.exists(matches_csv):\n",
        "    print(f\"[FATAL] Matches.csv not found at: {matches_csv}\")\n",
        "    print(f\"[INFO] Available files in BASE_PATH:\")\n",
        "    try:\n",
        "        for f in os.listdir(BASE_PATH):\n",
        "            if f.endswith('.csv'):\n",
        "                print(f\"  ‚Üí {f}\")\n",
        "    except:\n",
        "        pass\n",
        "    raise FileNotFoundError(f\"Cannot proceed without Matches.csv\")\n",
        "\n",
        "print(f\"[INFO] Loading Matches.csv...\")\n",
        "matches_all = pd.read_csv(matches_csv, parse_dates=['MatchDate'])\n",
        "print(f\"[OK] Loaded {len(matches_all):,} matches\")\n",
        "print(f\"     Date range: {matches_all['MatchDate'].min().date()} to {matches_all['MatchDate'].max().date()}\")\n",
        "print(f\"     Columns: {matches_all.shape[1]}\")\n",
        "print(f\"     Sample columns: {list(matches_all.columns[:15])}\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï 1.1.5: FILTER DATA FOR THESIS SCOPE (2015-2025 & TOP 5 LEAGUES)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(f\"[INFO] PHASE 1.1.5: FILTERING DATA FOR THESIS SCOPE\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Define thesis scope\n",
        "THESIS_START_DATE = '2015-09-01'\n",
        "THESIS_END_DATE = '2025-06-01'\n",
        "TOP_5_DIVISIONS = ['D1', 'F1', 'E0', 'I1', 'SP1']\n",
        "\n",
        "LEAGUE_NAMES = {\n",
        "    'E0': 'Premier League',\n",
        "    'D1': 'Bundesliga',\n",
        "    'I1': 'Serie A',\n",
        "    'SP1': 'La Liga',\n",
        "    'F1': 'Ligue 1'\n",
        "}\n",
        "\n",
        "print(f\"\\n[THESIS SCOPE DEFINITION]\")\n",
        "print(f\"  ‚Ä¢ Time Period:  {THESIS_START_DATE} to {THESIS_END_DATE}\")\n",
        "print(f\"  ‚Ä¢ Leagues:      {', '.join([LEAGUE_NAMES[div] for div in TOP_5_DIVISIONS])}\")\n",
        "print(f\"  ‚Ä¢ Original Dataset: {len(matches_all):,} matches\\n\")\n",
        "\n",
        "# Store original size for comparison\n",
        "original_size = len(matches_all)\n",
        "\n",
        "# Apply date filter\n",
        "if 'MatchDate' not in matches_all.columns:\n",
        "    print(f\"[ERROR] 'MatchDate' column not found!\")\n",
        "    raise ValueError(\"Cannot apply date filter without MatchDate column\")\n",
        "\n",
        "matches_all = matches_all[\n",
        "    (matches_all['MatchDate'] >= THESIS_START_DATE) &\n",
        "    (matches_all['MatchDate'] <= THESIS_END_DATE)\n",
        "].copy()\n",
        "\n",
        "after_date_filter = len(matches_all)\n",
        "print(f\"[STEP 1] Date Filter Applied:\")\n",
        "print(f\"  ‚úì Kept matches between {THESIS_START_DATE} and {THESIS_END_DATE}\")\n",
        "print(f\"  ‚Ä¢ Before: {original_size:,} matches\")\n",
        "print(f\"  ‚Ä¢ After:  {after_date_filter:,} matches\")\n",
        "print(f\"  ‚Ä¢ Removed: {original_size - after_date_filter:,} matches ({100*(original_size - after_date_filter)/original_size:.1f}%)\\n\")\n",
        "\n",
        "# Apply league filter\n",
        "if 'Division' not in matches_all.columns:\n",
        "    print(f\"[ERROR] 'Division' column not found!\")\n",
        "    print(f\"[INFO] Available columns: {list(matches_all.columns[:20])}\")\n",
        "    raise ValueError(\"Cannot apply league filter without Division column\")\n",
        "\n",
        "# Check which divisions are present\n",
        "present_divisions = matches_all['Division'].unique()\n",
        "print(f\"[INFO] Divisions present in data after date filter:\")\n",
        "for div in sorted(present_divisions):\n",
        "    count = len(matches_all[matches_all['Division'] == div])\n",
        "    league_name = LEAGUE_NAMES.get(div, 'Unknown')\n",
        "    marker = \"‚úì\" if div in TOP_5_DIVISIONS else \"‚úó\"\n",
        "    print(f\"  {marker} {div:5s} ({league_name:20s}): {count:,} matches\")\n",
        "\n",
        "matches_all = matches_all[matches_all['Division'].isin(TOP_5_DIVISIONS)].copy()\n",
        "\n",
        "after_league_filter = len(matches_all)\n",
        "print(f\"\\n[STEP 2] League Filter Applied:\")\n",
        "print(f\"  ‚úì Kept only Top 5 European leagues\")\n",
        "print(f\"  ‚Ä¢ Before: {after_date_filter:,} matches\")\n",
        "print(f\"  ‚Ä¢ After:  {after_league_filter:,} matches\")\n",
        "print(f\"  ‚Ä¢ Removed: {after_date_filter - after_league_filter:,} matches ({100*(after_date_filter - after_league_filter)/after_date_filter:.1f}%)\\n\")\n",
        "\n",
        "# Reset index after filtering\n",
        "matches_all = matches_all.reset_index(drop=True)\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"[FILTERING SUMMARY]\")\n",
        "print(\"=\"*90)\n",
        "print(f\"  üìä FINAL DATASET (THESIS SCOPE):\")\n",
        "print(f\"     ‚Ä¢ Total Matches:    {len(matches_all):,}\")\n",
        "print(f\"     ‚Ä¢ Date Range:       {matches_all['MatchDate'].min().date()} ‚Üí {matches_all['MatchDate'].max().date()}\")\n",
        "print(f\"     ‚Ä¢ Leagues:          {sorted(matches_all['Division'].unique())}\")\n",
        "print(f\"     ‚Ä¢ Reduction:        {original_size:,} ‚Üí {len(matches_all):,} ({100*(1 - len(matches_all)/original_size):.1f}% filtered out)\")\n",
        "\n",
        "print(f\"\\n  üìã MATCHES BY LEAGUE:\")\n",
        "for div in TOP_5_DIVISIONS:\n",
        "    league_count = len(matches_all[matches_all['Division'] == div])\n",
        "    league_pct = 100 * league_count / len(matches_all)\n",
        "    print(f\"     ‚Ä¢ {LEAGUE_NAMES[div]:20s} ({div}): {league_count:,} ({league_pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\n  üìÖ MATCHES BY SEASON:\")\n",
        "if 'MatchDate' in matches_all.columns:\n",
        "    matches_all['Year'] = matches_all['MatchDate'].dt.year\n",
        "    year_counts = matches_all['Year'].value_counts().sort_index()\n",
        "    for year, count in year_counts.items():\n",
        "        print(f\"     ‚Ä¢ {year}: {count:,} matches\")\n",
        "    matches_all = matches_all.drop(columns=['Year'])\n",
        "\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï 1.2.1: CREATE TARGET COLUMN FROM FTResult\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(f\"[INFO] Creating 'Target' column from 'FTResult'...\")\n",
        "\n",
        "if 'FTResult' not in matches_all.columns:\n",
        "    print(f\"  ‚ùå FATAL: 'FTResult' column not found!\")\n",
        "    print(f\"  Available columns: {list(matches_all.columns)}\")\n",
        "    raise ValueError(\"Cannot create Target without FTResult column\")\n",
        "\n",
        "# FTResult ‚Üí Target mapping\n",
        "result_mapping = {\n",
        "    'H': 1,  # Home Win\n",
        "    'D': 0,  # Draw\n",
        "    'A': 2   # Away Win\n",
        "}\n",
        "\n",
        "matches_all['Target'] = matches_all['FTResult'].map(result_mapping)\n",
        "\n",
        "# Verify mapping\n",
        "n_mapped = matches_all['Target'].notna().sum()\n",
        "n_total = len(matches_all)\n",
        "mapping_rate = (n_mapped / n_total) * 100\n",
        "\n",
        "print(f\"  ‚úì Created 'Target' column from 'FTResult'\")\n",
        "print(f\"     ‚Ä¢ Mapped: {n_mapped:,} / {n_total:,} ({mapping_rate:.2f}%)\")\n",
        "\n",
        "# Show distribution\n",
        "target_dist = matches_all['Target'].value_counts().sort_index()\n",
        "print(f\"\\n  [Target Distribution]\")\n",
        "print(f\"     ‚Ä¢ Draw (0):     {target_dist.get(0, 0):,} matches ({100*target_dist.get(0, 0)/n_mapped:.1f}%)\")\n",
        "print(f\"     ‚Ä¢ Home Win (1): {target_dist.get(1, 0):,} matches ({100*target_dist.get(1, 0)/n_mapped:.1f}%)\")\n",
        "print(f\"     ‚Ä¢ Away Win (2): {target_dist.get(2, 0):,} matches ({100*target_dist.get(2, 0)/n_mapped:.1f}%)\")\n",
        "\n",
        "# Check for unmapped values\n",
        "unmapped = matches_all[matches_all['Target'].isna()]['FTResult'].value_counts()\n",
        "if len(unmapped) > 0:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  Unmapped FTResult values found:\")\n",
        "    for val, count in unmapped.items():\n",
        "        print(f\"     ‚Ä¢ '{val}': {count:,} cases\")\n",
        "else:\n",
        "    print(f\"  ‚úì All FTResult values successfully mapped\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï 1.2.2: CREATE YEARMONTH COLUMN\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(f\"\\n[INFO] 1.2.2: Creating YearMonth column...\")\n",
        "\n",
        "matches_all['YearMonth'] = matches_all['MatchDate'].dt.to_period('M').astype(str)\n",
        "\n",
        "print(f\"  ‚úì Created YearMonth column\")\n",
        "print(f\"     ‚Ä¢ Sample values: {matches_all['YearMonth'].head(5).tolist()}\")\n",
        "print(f\"     ‚Ä¢ Unique months: {matches_all['YearMonth'].nunique()}\")\n",
        "print(f\"     ‚Ä¢ Date range: {matches_all['YearMonth'].min()} to {matches_all['YearMonth'].max()}\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï 1.2.3: CREATE SEASON COLUMN\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(f\"[INFO] 1.2.3: Creating Season column...\")\n",
        "\n",
        "def create_season(date):\n",
        "    \"\"\"Create season string (e.g., '2023-2024')\"\"\"\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "\n",
        "    if month >= 7:  # Temmuz'dan sonra ‚Üí yeni sezon\n",
        "        return f\"{year}-{year+1}\"\n",
        "    else:\n",
        "        return f\"{year-1}-{year}\"\n",
        "\n",
        "matches_all['Season'] = matches_all['MatchDate'].apply(create_season)\n",
        "\n",
        "print(f\"  ‚úì Created Season column\")\n",
        "print(f\"     ‚Ä¢ Sample values: {matches_all['Season'].head(5).tolist()}\")\n",
        "print(f\"     ‚Ä¢ Unique seasons: {matches_all['Season'].nunique()}\")\n",
        "print(f\"     ‚Ä¢ Seasons: {sorted(matches_all['Season'].unique())[:10]}...\\n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.3: Merge ELO Ratings (if available)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if os.path.exists(elo_csv):\n",
        "    print(f\"[INFO] Loading EloRatings.csv...\")\n",
        "    try:\n",
        "        # Load ELO data\n",
        "        elo_df = pd.read_csv(elo_csv, parse_dates=['date'])\n",
        "        elo_df = elo_df.rename(columns={'date': 'MatchDate', 'club': 'TeamName', 'elo': 'Elo'})\n",
        "\n",
        "        print(f\"  ‚úì Loaded {len(elo_df):,} ELO records\")\n",
        "        print(f\"     ‚Ä¢ Date range: {elo_df['MatchDate'].min().date()} to {elo_df['MatchDate'].max().date()}\")\n",
        "        print(f\"     ‚Ä¢ Unique teams: {elo_df['TeamName'].nunique()}\")\n",
        "\n",
        "        # Create lookup function for nearest ELO\n",
        "        def get_nearest_elo(team, match_date, elo_df):\n",
        "            \"\"\"Get nearest ELO rating before or on match date\"\"\"\n",
        "            team_elos = elo_df[elo_df['TeamName'] == team]\n",
        "\n",
        "            if team_elos.empty:\n",
        "                return np.nan\n",
        "\n",
        "            # Find ELO on or before match date\n",
        "            valid_elos = team_elos[team_elos['MatchDate'] <= match_date]\n",
        "\n",
        "            if valid_elos.empty:\n",
        "                # If no prior ELO, use first available\n",
        "                return team_elos.iloc[0]['Elo']\n",
        "\n",
        "            # Return most recent ELO before match\n",
        "            return valid_elos.sort_values('MatchDate', ascending=False).iloc[0]['Elo']\n",
        "\n",
        "        # Merge ELO (if not already present)\n",
        "        if 'HomeElo' not in matches_all.columns or matches_all['HomeElo'].isna().all():\n",
        "            print(f\"  ‚Üí Merging HomeElo...\")\n",
        "            matches_all['HomeElo'] = matches_all.apply(\n",
        "                lambda row: get_nearest_elo(row['HomeTeam'], row['MatchDate'], elo_df),\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        if 'AwayElo' not in matches_all.columns or matches_all['AwayElo'].isna().all():\n",
        "            print(f\"  ‚Üí Merging AwayElo...\")\n",
        "            matches_all['AwayElo'] = matches_all.apply(\n",
        "                lambda row: get_nearest_elo(row['AwayTeam'], row['MatchDate'], elo_df),\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        elo_coverage = matches_all['HomeElo'].notna().sum() / len(matches_all) * 100\n",
        "        print(f\"  ‚úì ELO ratings merged\")\n",
        "        print(f\"     ‚Ä¢ Coverage: {elo_coverage:.1f}% of matches\\n\")\n",
        "\n",
        "    except Exception as e_elo:\n",
        "        print(f\"  ‚ùå Failed to merge ELO data: {e_elo}\\n\")\n",
        "else:\n",
        "    print(f\"[WARN] EloRatings.csv not found - skipping ELO features\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 1.4: TRANSFERMARKT DATA MERGE (ENHANCED WITH FUZZY MATCHING)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] 1.4: Loading Transfermarkt Data WITH FUZZY MATCHING & 2-LEVEL MERGE\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üî• FIX: Preserve original Season column\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "SEASON_BACKUP = None\n",
        "if 'Season' in matches_all.columns:\n",
        "    SEASON_BACKUP = matches_all['Season'].copy()\n",
        "    print(f\"  ‚úì Backed up original Season column (format: {matches_all['Season'].iloc[0]})\")\n",
        "\n",
        "transfermarkt_xlsx = os.path.join(BASE_PATH, \"data.xlsx\")\n",
        "\n",
        "if os.path.exists(transfermarkt_xlsx):\n",
        "    print(f\"  ‚úì Found Transfermarkt data: {os.path.basename(transfermarkt_xlsx)}\")\n",
        "    print(f\"  ‚úì File size: {os.path.getsize(transfermarkt_xlsx) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    try:\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # LOAD EXCEL\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n  ‚Üí Loading Excel file (this may take 10-30 seconds)...\")\n",
        "        tm_df = pd.read_excel(transfermarkt_xlsx, engine='openpyxl')\n",
        "\n",
        "        print(f\"\\n  ‚úÖ Loaded {len(tm_df):,} Transfermarkt records\")\n",
        "        print(f\"     ‚Ä¢ Date range: {tm_df['YearMonth'].min()} to {tm_df['YearMonth'].max()}\")\n",
        "        print(f\"     ‚Ä¢ Unique clubs: {tm_df['ClubName'].nunique()}\")\n",
        "        print(f\"     ‚Ä¢ Columns: {tm_df.shape[1]}\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üÜï CREATE SEASON COLUMN (if not exists)\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        if 'Season' not in tm_df.columns:\n",
        "            print(f\"\\n  [SEASON CREATION] Season column not found, creating...\")\n",
        "\n",
        "            # YearMonth'tan Season olu≈ütur\n",
        "            tm_df['YearMonth_Period'] = pd.to_datetime(tm_df['YearMonth'] + '-01')\n",
        "            tm_df['Season'] = tm_df['YearMonth_Period'].apply(create_season)\n",
        "\n",
        "            print(f\"     ‚úì Season column created\")\n",
        "            print(f\"        Sample: {tm_df['Season'].head(3).tolist()}\")\n",
        "        else:\n",
        "            print(f\"\\n  ‚úì Season column already exists\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üÜï FUZZY MATCHING\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n  [FUZZY MATCHING] Creating team mappings...\")\n",
        "\n",
        "        team_mapping = fuzzy_matcher.create_team_mapping(matches_all, tm_df)\n",
        "\n",
        "        # Apply mapping\n",
        "        matches_all['HomeTeam_TM'] = matches_all['HomeTeam'].map(team_mapping)\n",
        "        matches_all['AwayTeam_TM'] = matches_all['AwayTeam'].map(team_mapping)\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # LEVEL 1: YEARMONTH-BASED MERGE\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n  [MERGE LEVEL 1] YearMonth-based merge...\")\n",
        "\n",
        "        # Deduplicate TM data\n",
        "        tm_df_yearmonth = tm_df.drop_duplicates(\n",
        "            subset=['ClubName', 'YearMonth'],\n",
        "            keep='last'\n",
        "        ).copy()\n",
        "\n",
        "        print(f\"     ‚Ä¢ Unique YearMonth records: {len(tm_df_yearmonth):,}\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # HOME TEAM MERGE (Level 1)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        print(f\"\\n     üè† [HOME] Level 1 merge...\")\n",
        "\n",
        "        tm_home_l1 = tm_df_yearmonth.rename(columns={'ClubName': 'HomeTeam_TM'})\n",
        "\n",
        "        matches_all = matches_all.merge(\n",
        "            tm_home_l1,\n",
        "            on=['HomeTeam_TM', 'YearMonth'],\n",
        "            how='left',\n",
        "            suffixes=('', '_home_tm')\n",
        "        )\n",
        "\n",
        "        home_cols = [c for c in tm_home_l1.columns\n",
        "                     if c not in ['HomeTeam_TM', 'YearMonth']]\n",
        "\n",
        "        # Check unmatched\n",
        "        if home_cols:\n",
        "            unmatched_mask = matches_all[home_cols[0]].isna()\n",
        "        else:\n",
        "            unmatched_mask = pd.Series(False, index=matches_all.index)\n",
        "\n",
        "        unmatched_indices = matches_all[unmatched_mask].index\n",
        "\n",
        "        print(f\"        ‚Ä¢ Matched: {(~unmatched_mask).sum():,}\")\n",
        "        print(f\"        ‚Ä¢ Unmatched: {len(unmatched_indices):,}\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # HOME TEAM FALLBACK (Level 2: Season)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if len(unmatched_indices) > 0:\n",
        "            print(f\"\\n     üè† [HOME] Level 2 fallback (Season-based)...\")\n",
        "\n",
        "            # üî• Aggregate by Season\n",
        "            numeric_cols = tm_df.select_dtypes(include=np.number).columns.tolist()\n",
        "            numeric_cols = [c for c in numeric_cols if c not in ['club_id', 'wettbewerb_id']]\n",
        "\n",
        "            non_numeric_cols = tm_df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "            non_numeric_cols = [c for c in non_numeric_cols\n",
        "                               if c not in ['ClubName', 'Season', 'YearMonth', 'LeagueName']]\n",
        "\n",
        "            agg_dict = {col: 'mean' for col in numeric_cols if col in tm_df.columns}\n",
        "            for col in non_numeric_cols:\n",
        "                if col in tm_df.columns:\n",
        "                    agg_dict[col] = 'first'\n",
        "\n",
        "            tm_df_season = tm_df.groupby(['ClubName', 'Season'], as_index=False).agg(agg_dict)\n",
        "\n",
        "            print(f\"        ‚Ä¢ Unique Season records: {len(tm_df_season):,}\")\n",
        "\n",
        "            # Merge\n",
        "            fallback_data = matches_all.loc[unmatched_indices, ['HomeTeam_TM', 'Season']]\n",
        "            tm_home_l2 = tm_df_season.rename(columns={'ClubName': 'HomeTeam_TM'})\n",
        "\n",
        "            fallback_merged = fallback_data.merge(\n",
        "                tm_home_l2,\n",
        "                on=['HomeTeam_TM', 'Season'],\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            # Fill unmatched\n",
        "            for col in home_cols:\n",
        "                if col in fallback_merged.columns:\n",
        "                    fallback_merged.index = unmatched_indices\n",
        "                    matches_all.loc[unmatched_indices, col] = fallback_merged[col]\n",
        "\n",
        "            filled = matches_all.loc[unmatched_indices, home_cols[0]].notna().sum() if home_cols else 0\n",
        "            print(f\"        ‚Ä¢ Filled by Season: {filled:,}\")\n",
        "\n",
        "        # Rename home columns\n",
        "        rename_dict = {c: f'HomeTeam_{c}' for c in home_cols if c in matches_all.columns}\n",
        "        matches_all = matches_all.rename(columns=rename_dict)\n",
        "\n",
        "        if home_cols:\n",
        "            home_coverage = matches_all[f'HomeTeam_{home_cols[0]}'].notna().sum()\n",
        "            home_pct = (home_coverage / len(matches_all)) * 100\n",
        "            print(f\"\\n        ‚úì Final HOME coverage: {home_coverage:,} / {len(matches_all):,} ({home_pct:.1f}%)\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # AWAY TEAM MERGE (Level 1)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        print(f\"\\n     ‚úàÔ∏è  [AWAY] Level 1 merge...\")\n",
        "\n",
        "        tm_away_l1 = tm_df_yearmonth.rename(columns={'ClubName': 'AwayTeam_TM'})\n",
        "\n",
        "        matches_all = matches_all.merge(\n",
        "            tm_away_l1,\n",
        "            on=['AwayTeam_TM', 'YearMonth'],\n",
        "            how='left',\n",
        "            suffixes=('', '_away_tm')\n",
        "        )\n",
        "\n",
        "        away_cols = [c for c in tm_away_l1.columns\n",
        "                     if c not in ['AwayTeam_TM', 'YearMonth']]\n",
        "\n",
        "        # Check unmatched\n",
        "        if away_cols:\n",
        "            away_check_col = away_cols[0]\n",
        "            if away_check_col + '_away_tm' in matches_all.columns:\n",
        "                away_check_col = away_check_col + '_away_tm'\n",
        "\n",
        "            unmatched_mask_away = matches_all[away_check_col].isna()\n",
        "        else:\n",
        "            unmatched_mask_away = pd.Series(False, index=matches_all.index)\n",
        "\n",
        "        unmatched_indices_away = matches_all[unmatched_mask_away].index\n",
        "\n",
        "        print(f\"        ‚Ä¢ Matched: {(~unmatched_mask_away).sum():,}\")\n",
        "        print(f\"        ‚Ä¢ Unmatched: {len(unmatched_indices_away):,}\")\n",
        "\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        # AWAY TEAM FALLBACK (Level 2: Season)\n",
        "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "        if len(unmatched_indices_away) > 0:\n",
        "            print(f\"\\n     ‚úàÔ∏è  [AWAY] Level 2 fallback (Season-based)...\")\n",
        "\n",
        "            fallback_data_away = matches_all.loc[unmatched_indices_away, ['AwayTeam_TM', 'Season']]\n",
        "            tm_away_l2 = tm_df_season.rename(columns={'ClubName': 'AwayTeam_TM'})\n",
        "\n",
        "            fallback_merged_away = fallback_data_away.merge(\n",
        "                tm_away_l2,\n",
        "                on=['AwayTeam_TM', 'Season'],\n",
        "                how='left',\n",
        "                suffixes=('', '_away_l2')\n",
        "            )\n",
        "\n",
        "            # Fill unmatched\n",
        "            for col in away_cols:\n",
        "                # Determine target column name\n",
        "                target_col = col\n",
        "                if col + '_away_tm' in matches_all.columns:\n",
        "                    target_col = col + '_away_tm'\n",
        "\n",
        "                # Determine source column name\n",
        "                source_col = col\n",
        "                if col + '_away_l2' in fallback_merged_away.columns:\n",
        "                    source_col = col + '_away_l2'\n",
        "\n",
        "                if source_col in fallback_merged_away.columns and target_col in matches_all.columns:\n",
        "                    fallback_merged_away.index = unmatched_indices_away\n",
        "                    matches_all.loc[unmatched_indices_away, target_col] = fallback_merged_away[source_col]\n",
        "\n",
        "            if away_cols:\n",
        "                check_col = away_cols[0]\n",
        "                if check_col + '_away_tm' in matches_all.columns:\n",
        "                    check_col = check_col + '_away_tm'\n",
        "                filled_away = matches_all.loc[unmatched_indices_away, check_col].notna().sum()\n",
        "                print(f\"        ‚Ä¢ Filled by Season: {filled_away:,}\")\n",
        "\n",
        "        # Rename away columns\n",
        "        rename_dict_away = {}\n",
        "        for col in away_cols:\n",
        "            suffixed_col = col + '_away_tm'\n",
        "            if suffixed_col in matches_all.columns:\n",
        "                rename_dict_away[suffixed_col] = 'AwayTeam_' + col\n",
        "            elif col in matches_all.columns and not col.startswith('HomeTeam_'):\n",
        "                rename_dict_away[col] = 'AwayTeam_' + col\n",
        "\n",
        "        matches_all = matches_all.rename(columns=rename_dict_away)\n",
        "\n",
        "        if away_cols:\n",
        "            away_check_final = f'AwayTeam_{away_cols[0]}'\n",
        "            if away_check_final in matches_all.columns:\n",
        "                away_coverage = matches_all[away_check_final].notna().sum()\n",
        "                away_pct = (away_coverage / len(matches_all)) * 100\n",
        "                print(f\"\\n        ‚úì Final AWAY coverage: {away_coverage:,} / {len(matches_all):,} ({away_pct:.1f}%)\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # üî• FIX: Restore original Season column (AFTER ALL MERGES)\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        if SEASON_BACKUP is not None:\n",
        "            if 'Season' not in matches_all.columns:\n",
        "                # Case 1: Season column completely disappeared\n",
        "                matches_all['Season'] = SEASON_BACKUP\n",
        "                print(f\"\\n  ‚úÖ Restored original Season column (was missing after merge)\")\n",
        "            elif matches_all['Season'].isna().all():\n",
        "                # Case 2: Season column exists but all NaN\n",
        "                matches_all['Season'] = SEASON_BACKUP\n",
        "                print(f\"\\n  ‚úÖ Restored original Season column (was all NaN after merge)\")\n",
        "            elif matches_all['Season'].dtype != SEASON_BACKUP.dtype:\n",
        "                # Case 3: Season column type changed\n",
        "                matches_all['Season'] = SEASON_BACKUP\n",
        "                print(f\"\\n  ‚ö†Ô∏è  Season column type changed during merge - restored original\")\n",
        "                print(f\"     Old type: {matches_all['Season'].dtype}, New type: {SEASON_BACKUP.dtype}\")\n",
        "            else:\n",
        "                # Case 4: Check if values are different\n",
        "                if not matches_all['Season'].equals(SEASON_BACKUP):\n",
        "                    matches_all['Season'] = SEASON_BACKUP\n",
        "                    print(f\"\\n  ‚ö†Ô∏è  Season column values changed during merge - restored original\")\n",
        "                else:\n",
        "                    print(f\"\\n  ‚úì Season column preserved correctly during merge\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # CREATE VALUERATIO (if columns exist)\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n  [FEATURE ENGINEERING] Creating ValueRatio...\")\n",
        "\n",
        "        if 'HomeTeam_ClubValue' in matches_all.columns and 'HomeTeam_LeagueValue' in matches_all.columns:\n",
        "            matches_all['HomeTeam_ValueRatio'] = (\n",
        "                matches_all['HomeTeam_ClubValue'] / (matches_all['HomeTeam_LeagueValue'] + 1e-6)\n",
        "            )\n",
        "            print(f\"     ‚úì HomeTeam_ValueRatio created\")\n",
        "\n",
        "        if 'AwayTeam_ClubValue' in matches_all.columns and 'AwayTeam_LeagueValue' in matches_all.columns:\n",
        "            matches_all['AwayTeam_ValueRatio'] = (\n",
        "                matches_all['AwayTeam_ClubValue'] / (matches_all['AwayTeam_LeagueValue'] + 1e-6)\n",
        "            )\n",
        "            print(f\"     ‚úì AwayTeam_ValueRatio created\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # CREATE DIFF FEATURES\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n  [DIFF FEATURES] Creating Home-Away differences...\")\n",
        "\n",
        "        diff_features = [\n",
        "            ('ClubValue', 'Club Market Value'),\n",
        "            ('LeagueValue', 'League Average Value'),\n",
        "            ('MaxPlayerValue', 'Star Player Value'),\n",
        "            ('ManagerTrophies', 'Manager Trophy Count'),\n",
        "            ('ManagerTenureDays', 'Manager Experience'),\n",
        "            ('NetTransferSpending', 'Net Transfer Spending'),\n",
        "            ('n_players_injured', 'Injured Players'),\n",
        "            ('n_suspended_players', 'Suspended Players'),\n",
        "            ('ValueRatio', 'Value Ratio')\n",
        "        ]\n",
        "\n",
        "        features_created = []\n",
        "\n",
        "        for feature, description in diff_features:\n",
        "            home_col = f'HomeTeam_{feature}'\n",
        "            away_col = f'AwayTeam_{feature}'\n",
        "            diff_col = f'{feature}_Diff'\n",
        "\n",
        "            if home_col in matches_all.columns and away_col in matches_all.columns:\n",
        "                matches_all[diff_col] = matches_all[home_col] - matches_all[away_col]\n",
        "\n",
        "                non_null = matches_all[diff_col].notna().sum()\n",
        "                coverage_pct = (non_null / len(matches_all)) * 100\n",
        "\n",
        "                features_created.append(diff_col)\n",
        "                print(f\"     ‚úì {diff_col:35s}: {coverage_pct:5.1f}%\")\n",
        "\n",
        "        print(f\"\\n  ‚úÖ Transfermarkt merge complete!\")\n",
        "        print(f\"     ‚Ä¢ Total new columns: {len(features_created) + len(home_cols) + len(away_cols)}\")\n",
        "        print(f\"     ‚Ä¢ Diff features: {len(features_created)}\")\n",
        "\n",
        "    except Exception as e_tm:\n",
        "        print(f\"\\n  ‚ùå Transfermarkt merge FAILED!\")\n",
        "        print(f\"     Error: {e_tm}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\n  ‚ö†Ô∏è  Continuing without Transfermarkt features...\\n\")\n",
        "\n",
        "        # üî• FIX: Restore Season even if merge failed\n",
        "        if SEASON_BACKUP is not None and 'Season' not in matches_all.columns:\n",
        "            matches_all['Season'] = SEASON_BACKUP\n",
        "            print(f\"  ‚úÖ Restored original Season column after merge failure\")\n",
        "\n",
        "else:\n",
        "    print(f\"  ‚ùå File not found: {transfermarkt_xlsx}\")\n",
        "    print(f\"  ‚ö†Ô∏è  Continuing without Transfermarkt features...\\n\")\n",
        "\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.5: Basic data cleaning\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(f\"[INFO] Performing basic data cleaning...\")\n",
        "\n",
        "# Sort by date (CRITICAL for temporal features!)\n",
        "matches_all = matches_all.sort_values('MatchDate').reset_index(drop=True)\n",
        "print(f\"  ‚úì Sorted by MatchDate\")\n",
        "\n",
        "# Check for duplicates\n",
        "n_duplicates = matches_all.duplicated(subset=['MatchDate', 'HomeTeam', 'AwayTeam']).sum()\n",
        "if n_duplicates > 0:\n",
        "    print(f\"  ‚ö†Ô∏è  Found {n_duplicates} duplicate matches - removing...\")\n",
        "    matches_all = matches_all.drop_duplicates(subset=['MatchDate', 'HomeTeam', 'AwayTeam'], keep='first')\n",
        "else:\n",
        "    print(f\"  ‚úì No duplicates found\")\n",
        "\n",
        "# Check required columns\n",
        "required_cols = ['MatchDate', 'HomeTeam', 'AwayTeam', 'Target']\n",
        "missing_required = [col for col in required_cols if col not in matches_all.columns]\n",
        "\n",
        "if missing_required:\n",
        "    print(f\"\\n  ‚ùå Missing required columns: {missing_required}\")\n",
        "    print(f\"  Available columns: {list(matches_all.columns[:30])}\")\n",
        "    raise ValueError(\"Cannot proceed without required columns\")\n",
        "else:\n",
        "    print(f\"  ‚úì All required columns present: {required_cols}\")\n",
        "\n",
        "# Add basic derived features if not present\n",
        "if 'ELO_Diff' not in matches_all.columns and 'HomeElo' in matches_all.columns:\n",
        "    matches_all['ELO_Diff'] = matches_all['HomeElo'] - matches_all['AwayElo']\n",
        "    print(f\"  ‚úì Created ELO_Diff feature\")\n",
        "\n",
        "print(f\"\\n[OK] Data cleaning complete\")\n",
        "print(f\"[OK] Final dataset: {matches_all.shape[0]:,} rows √ó {matches_all.shape[1]} columns\")\n",
        "print(f\"[OK] Memory usage: {matches_all.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.6: Summary Statistics\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[SUMMARY] Data Loading Complete\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\nüìä DATASET STATISTICS:\")\n",
        "print(f\"  ‚Ä¢ Total Matches:        {len(matches_all):,}\")\n",
        "print(f\"  ‚Ä¢ Date Range:           {matches_all['MatchDate'].min().date()} ‚Üí {matches_all['MatchDate'].max().date()}\")\n",
        "print(f\"  ‚Ä¢ Unique Home Teams:    {matches_all['HomeTeam'].nunique()}\")\n",
        "print(f\"  ‚Ä¢ Unique Away Teams:    {matches_all['AwayTeam'].nunique()}\")\n",
        "print(f\"  ‚Ä¢ Total Features:       {matches_all.shape[1]}\")\n",
        "\n",
        "print(f\"\\nüéØ TARGET DISTRIBUTION:\")\n",
        "target_pct = matches_all['Target'].value_counts(normalize=True).sort_index() * 100\n",
        "print(f\"  ‚Ä¢ Draw (0):      {target_pct.get(0, 0):5.2f}%\")\n",
        "print(f\"  ‚Ä¢ Home Win (1):  {target_pct.get(1, 0):5.2f}%\")\n",
        "print(f\"  ‚Ä¢ Away Win (2):  {target_pct.get(2, 0):5.2f}%\")\n",
        "\n",
        "print(f\"\\nüìà DATA SOURCES:\")\n",
        "elo_cov = matches_all['HomeElo'].notna().sum() / len(matches_all) * 100\n",
        "print(f\"  ‚Ä¢ ELO Ratings:         {elo_cov:.1f}% coverage\")\n",
        "\n",
        "if 'HomeTeam_ClubValue' in matches_all.columns:\n",
        "    tm_cov = matches_all['HomeTeam_ClubValue'].notna().sum() / len(matches_all) * 100\n",
        "    print(f\"  ‚Ä¢ Transfermarkt Data:  {tm_cov:.1f}% coverage\")\n",
        "\n",
        "    if 'ValueRatio_Diff' in matches_all.columns:\n",
        "        vr_cov = matches_all['ValueRatio_Diff'].notna().sum() / len(matches_all) * 100\n",
        "        print(f\"  ‚Ä¢ ValueRatio Feature:  {vr_cov:.1f}% coverage ‚ú® (NEW!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[‚úì] PHASE 1 COMPLETE - matches_all loaded and ready!\")\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 1.7: Verify matches_all is ready\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(\"[VERIFICATION] Final checks...\")\n",
        "print(f\"  ‚úì matches_all type: {type(matches_all)}\")\n",
        "print(f\"  ‚úì Shape: {matches_all.shape}\")\n",
        "print(f\"  ‚úì Target column exists: {'Target' in matches_all.columns}\")\n",
        "print(f\"  ‚úì Target has values: {matches_all['Target'].notna().any()}\")\n",
        "print(f\"  ‚úì ValueRatio_Diff exists: {'ValueRatio_Diff' in matches_all.columns}\")\n",
        "\n",
        "if matches_all.empty:\n",
        "    raise ValueError(\"[FATAL] matches_all is empty after loading!\")\n",
        "\n",
        "print(\"\\n[OK] All verifications passed ‚úÖ\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 1.5: TEMPORAL FEATURES CREATION (LAG-BASED AVERAGES)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 1.5: CREATING TEMPORAL FEATURES (Lag Averages)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# Initialize list to track created features\n",
        "temporal_features_created = []\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# STEP 1: Check required columns\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "print(\"[STEP 1] Checking required columns...\")\n",
        "\n",
        "# Ma√ß-i√ßi metrikler (bunlar LEAKAGE olduƒüu i√ßin direkt kullanƒ±lmayacak)\n",
        "in_game_metrics = {\n",
        "    'Target': ['HomeTarget', 'AwayTarget'],      # Shots on target\n",
        "    'Shots': ['HomeShots', 'AwayShots'],         # Total shots\n",
        "    'Corners': ['HomeCorners', 'AwayCorners'],   # Corners\n",
        "    'Fouls': ['HomeFouls', 'AwayFouls'],         # Fouls\n",
        "    'Yellow': ['HomeYellow', 'AwayYellow']       # Yellow cards\n",
        "}\n",
        "\n",
        "# Hangi metrikler mevcut?\n",
        "available_metrics = {}\n",
        "for metric_name, cols in in_game_metrics.items():\n",
        "    if all(col in matches_all.columns for col in cols):\n",
        "        available_metrics[metric_name] = cols\n",
        "        print(f\"  ‚úì {metric_name}: {cols[0]}, {cols[1]}\")\n",
        "    else:\n",
        "        missing = [c for c in cols if c not in matches_all.columns]\n",
        "        print(f\"  ‚úó {metric_name}: Missing {missing}\")\n",
        "\n",
        "if not available_metrics:\n",
        "    print(\"\\n  ‚ùå ERROR: No in-game metrics found!\")\n",
        "    print(\"     Required columns like 'HomeTarget', 'HomeShots' etc. are missing\")\n",
        "    print(\"     ‚Üí Skipping temporal feature creation\\n\")\n",
        "    temporal_features_created = []\n",
        "else:\n",
        "    print(f\"\\n  ‚úÖ Found {len(available_metrics)} metric types for temporal features\\n\")\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # STEP 2: Create LAG features (L5 window)\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"[STEP 2] Creating temporal averages (L5 window)...\")\n",
        "    print(\"  ‚Üí Using shift(1) to prevent data leakage\")\n",
        "    print(\"  ‚Üí min_periods=3 for stability\\n\")\n",
        "\n",
        "    for metric_name, [home_col, away_col] in available_metrics.items():\n",
        "\n",
        "        # HOME team temporal average\n",
        "        home_avg_col = f'Home_{metric_name}_Avg_L5'\n",
        "        print(f\"  [Home] Creating {home_avg_col}...\", end=' ')\n",
        "\n",
        "        try:\n",
        "            matches_all[home_avg_col] = (\n",
        "                matches_all.groupby('HomeTeam')[home_col]\n",
        "                .transform(lambda x: x.shift(1).rolling(window=5, min_periods=1).mean())\n",
        "            )\n",
        "\n",
        "            coverage = matches_all[home_avg_col].notna().sum() / len(matches_all) * 100\n",
        "            print(f\"‚úì (coverage: {coverage:.1f}%)\")\n",
        "            temporal_features_created.append(home_avg_col)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Error: {e}\")\n",
        "\n",
        "        # AWAY team temporal average\n",
        "        away_avg_col = f'Away_{metric_name}_Avg_L5'\n",
        "        print(f\"  [Away] Creating {away_avg_col}...\", end=' ')\n",
        "\n",
        "        try:\n",
        "            matches_all[away_avg_col] = (\n",
        "                matches_all.groupby('AwayTeam')[away_col]\n",
        "                .transform(lambda x: x.shift(1).rolling(window=5, min_periods=1).mean())\n",
        "            )\n",
        "\n",
        "            coverage = matches_all[away_avg_col].notna().sum() / len(matches_all) * 100\n",
        "            print(f\"‚úì (coverage: {coverage:.1f}%)\")\n",
        "            temporal_features_created.append(away_avg_col)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Error: {e}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # STEP 3: Create DIFF features (Home - Away)\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"[STEP 3] Creating difference features (Home - Away)...\")\n",
        "\n",
        "    for metric_name in available_metrics.keys():\n",
        "        home_avg_col = f'Home_{metric_name}_Avg_L5'\n",
        "        away_avg_col = f'Away_{metric_name}_Avg_L5'\n",
        "        diff_col = f'{metric_name}_Diff_L5'\n",
        "\n",
        "        if home_avg_col in matches_all.columns and away_avg_col in matches_all.columns:\n",
        "            print(f\"  Creating {diff_col}...\", end=' ')\n",
        "\n",
        "            try:\n",
        "                matches_all[diff_col] = matches_all[home_avg_col] - matches_all[away_avg_col]\n",
        "\n",
        "                coverage = matches_all[diff_col].notna().sum() / len(matches_all) * 100\n",
        "                print(f\"‚úì (coverage: {coverage:.1f}%)\")\n",
        "                temporal_features_created.append(diff_col)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚úó Error: {e}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # STEP 4: Special feature - Shot Accuracy Difference\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if 'Target' in available_metrics and 'Shots' in available_metrics:\n",
        "        print(\"[STEP 4] Creating shot accuracy features...\")\n",
        "\n",
        "        # Home Shot Accuracy\n",
        "        if 'Home_Target_Avg_L5' in matches_all.columns and 'Home_Shots_Avg_L5' in matches_all.columns:\n",
        "            print(f\"  Creating Home_ShotAccuracy_L5...\", end=' ')\n",
        "            matches_all['Home_ShotAccuracy_L5'] = (\n",
        "                matches_all['Home_Target_Avg_L5'] / (matches_all['Home_Shots_Avg_L5'] + 1e-6)\n",
        "            )\n",
        "            print(f\"‚úì\")\n",
        "            temporal_features_created.append('Home_ShotAccuracy_L5')\n",
        "\n",
        "        # Away Shot Accuracy\n",
        "        if 'Away_Target_Avg_L5' in matches_all.columns and 'Away_Shots_Avg_L5' in matches_all.columns:\n",
        "            print(f\"  Creating Away_ShotAccuracy_L5...\", end=' ')\n",
        "            matches_all['Away_ShotAccuracy_L5'] = (\n",
        "                matches_all['Away_Target_Avg_L5'] / (matches_all['Away_Shots_Avg_L5'] + 1e-6)\n",
        "            )\n",
        "            print(f\"‚úì\")\n",
        "            temporal_features_created.append('Away_ShotAccuracy_L5')\n",
        "\n",
        "        # Shot Accuracy Diff\n",
        "        if 'Home_ShotAccuracy_L5' in matches_all.columns and 'Away_ShotAccuracy_L5' in matches_all.columns:\n",
        "            print(f\"  Creating ShotAccuracy_Diff_L5...\", end=' ')\n",
        "            matches_all['ShotAccuracy_Diff_L5'] = (\n",
        "                matches_all['Home_ShotAccuracy_L5'] - matches_all['Away_ShotAccuracy_L5']\n",
        "            )\n",
        "            print(f\"‚úì\")\n",
        "            temporal_features_created.append('ShotAccuracy_Diff_L5')\n",
        "\n",
        "        print()\n",
        "\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # STEP 5: Summary\n",
        "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    print(\"=\"*90)\n",
        "    print(\"[SUMMARY] Temporal Features Creation Complete\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    print(f\"\\n  ‚úÖ Total features created: {len(temporal_features_created)}\")\n",
        "    print(f\"  ‚úÖ Window size: L5 (last 5 matches)\")\n",
        "    print(f\"  ‚úÖ Leakage prevention: shift(1) applied\")\n",
        "    print(f\"  ‚úÖ Min periods: 3 matches\\n\")\n",
        "\n",
        "    if temporal_features_created:\n",
        "        print(\"  üìã Created features:\")\n",
        "        for i, feat in enumerate(temporal_features_created, 1):\n",
        "            coverage = matches_all[feat].notna().sum() / len(matches_all) * 100\n",
        "            print(f\"     {i:2d}. {feat:35s} ({coverage:5.1f}% coverage)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CRITICAL: Verify temporal_features_created exists\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "if 'temporal_features_created' not in globals():\n",
        "    print(\"[WARN] temporal_features_created was not initialized!\")\n",
        "    temporal_features_created = []\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 1.6: TEMPORAL FEATURES VALIDATION (Quality Control)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 1.6: VALIDATING TEMPORAL FEATURES (Quality Control)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "def validate_lag_features(df, lag_feature_names, original_features, output_dir):\n",
        "    \"\"\"\n",
        "    ‚úÖ V5.1 UYUMLU: Validate DIFF-ONLY temporal features\n",
        "\n",
        "    Quality Control Tests:\n",
        "    1. ‚úÖ Leakage Check: Correlation with original features (should be <0.95)\n",
        "    2. ‚úÖ Coverage Analysis: % of non-null values (should be >60%)\n",
        "    3. ‚ö†Ô∏è  Window Consistency: SKIPPED (only L5 exists in V5.1)\n",
        "    4. ‚úÖ Visual Inspection: Distribution plots\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"  Validating {len(lag_feature_names)} temporal features...\\n\")\n",
        "\n",
        "    if not lag_feature_names:\n",
        "        print(\"  ‚ö†Ô∏è  No lag features to validate\")\n",
        "        return None\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # TEST 1: LEAKAGE CHECK (FIXED FOR V5.1)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[TEST 1] üîí LEAKAGE CHECK (shift(1) validation)\")\n",
        "    print(\"  ‚Üí Verifying temporal isolation...\\n\")\n",
        "\n",
        "    leakage_detected = False\n",
        "    leakage_details = []\n",
        "\n",
        "    # ‚úÖ FIX: V5.1'de sadece Diff features var\n",
        "    # √ñrnek: Target_Diff_L5 ‚Üí HomeTarget ve AwayTarget ile kar≈üƒ±la≈ütƒ±r\n",
        "\n",
        "    for lag_feat in lag_feature_names:\n",
        "        # Extract base feature name\n",
        "        # 'Target_Diff_L5' ‚Üí 'Target'\n",
        "        base_name = lag_feat.replace('_Diff_L5', '').replace('_Diff_L3', '')\n",
        "\n",
        "        # Look for Home/Away versions\n",
        "        home_col = f'Home{base_name}'\n",
        "        away_col = f'Away{base_name}'\n",
        "\n",
        "        # Test against HomeX\n",
        "        if home_col in df.columns and lag_feat in df.columns:\n",
        "            valid_pairs = df[[home_col, lag_feat]].dropna()\n",
        "\n",
        "            if len(valid_pairs) > 10:\n",
        "                correlation = valid_pairs.corr().iloc[0, 1]\n",
        "\n",
        "                # ‚úÖ For Diff features, correlation should be LOWER\n",
        "                # Because Diff = Home - Away (not just Home)\n",
        "                if abs(correlation) > 0.95:\n",
        "                    leakage_detected = True\n",
        "                    leakage_details.append({\n",
        "                        'lag_feature': lag_feat,\n",
        "                        'original': home_col,\n",
        "                        'correlation': correlation,\n",
        "                        'type': 'Home'\n",
        "                    })\n",
        "                    print(f\"     ‚ö†Ô∏è  {lag_feat} ‚Üî {home_col}: r={correlation:.4f}\")\n",
        "\n",
        "        # Test against AwayX\n",
        "        if away_col in df.columns and lag_feat in df.columns:\n",
        "            valid_pairs = df[[away_col, lag_feat]].dropna()\n",
        "\n",
        "            if len(valid_pairs) > 10:\n",
        "                correlation = valid_pairs.corr().iloc[0, 1]\n",
        "\n",
        "                if abs(correlation) > 0.95:\n",
        "                    leakage_detected = True\n",
        "                    leakage_details.append({\n",
        "                        'lag_feature': lag_feat,\n",
        "                        'original': away_col,\n",
        "                        'correlation': correlation,\n",
        "                        'type': 'Away'\n",
        "                    })\n",
        "                    print(f\"     ‚ö†Ô∏è  {lag_feat} ‚Üî {away_col}: r={correlation:.4f}\")\n",
        "\n",
        "    if leakage_detected:\n",
        "        print(f\"\\n  üö® CRITICAL: {len(leakage_details)} potential leakage cases!\")\n",
        "        print(f\"     Note: For Diff features, some correlation is expected.\")\n",
        "        print(f\"     Only |r| > 0.95 indicates leakage.\\n\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ No leakage detected (all |correlations| < 0.95)\")\n",
        "        print(f\"     ‚Üí shift(1) is working correctly\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # TEST 2: COVERAGE ANALYSIS (WITH SAFETY CHECK)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[TEST 2] üìä COVERAGE ANALYSIS\")\n",
        "\n",
        "    coverage_data = []\n",
        "    for feat in lag_feature_names:\n",
        "        if feat in df.columns:\n",
        "            non_null = df[feat].notna().sum()\n",
        "            total = len(df)\n",
        "            coverage_pct = (non_null / total) * 100\n",
        "\n",
        "            coverage_data.append({\n",
        "                'feature': feat,\n",
        "                'coverage_pct': coverage_pct\n",
        "            })\n",
        "\n",
        "    if not coverage_data:\n",
        "        print(f\"  ‚ö†Ô∏è  No coverage data collected - features might be missing\")\n",
        "        min_coverage = 0\n",
        "        mean_coverage = 0\n",
        "        coverage_df = pd.DataFrame()\n",
        "    else:\n",
        "        coverage_df = pd.DataFrame(coverage_data).sort_values('coverage_pct')\n",
        "\n",
        "        min_coverage = coverage_df['coverage_pct'].min()\n",
        "        mean_coverage = coverage_df['coverage_pct'].mean()\n",
        "\n",
        "        print(f\"  Coverage Statistics:\")\n",
        "        print(f\"    ‚Ä¢ Min:  {min_coverage:.2f}%\")\n",
        "        print(f\"    ‚Ä¢ Mean: {mean_coverage:.2f}%\")\n",
        "\n",
        "        if min_coverage < 60:\n",
        "            print(f\"    ‚ö†Ô∏è  Some features have <60% coverage\")\n",
        "        else:\n",
        "            print(f\"    ‚úÖ All features have ‚â•60% coverage\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # TEST 3: WINDOW CONSISTENCY (SKIPPED FOR V5.1)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"\\n[TEST 3] üîó WINDOW SIZE CONSISTENCY\")\n",
        "    print(f\"  ‚ö†Ô∏è  SKIPPED: V5.1 only uses L5 features (no L3 for comparison)\")\n",
        "    print(f\"     Rationale: L5 selected as optimal window size\\n\")\n",
        "\n",
        "    avg_consistency = 0  # Not applicable for V5.1\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # TEST 4: VISUALIZATION (WITH SAFETY CHECKS)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    if output_dir and not coverage_df.empty:\n",
        "        print(f\"[TEST 4] üìà Creating validation plots...\")\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 6))\n",
        "        gs = fig.add_gridspec(1, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "        # PLOT 1: Coverage Heatmap\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "\n",
        "        # ‚úÖ Safety: Check if we have data\n",
        "        if len(coverage_df) > 0:\n",
        "            worst_20 = coverage_df.head(min(20, len(coverage_df)))\n",
        "            coverage_matrix = worst_20['coverage_pct'].values.reshape(-1, 1)\n",
        "\n",
        "            im = ax1.imshow(coverage_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
        "            ax1.set_yticks(range(len(worst_20)))\n",
        "            ax1.set_yticklabels([f[:35] for f in worst_20['feature']], fontsize=7)\n",
        "            ax1.set_xticks([])\n",
        "            ax1.set_title('Coverage (All Features)\\nNon-Null %', fontweight='bold', fontsize=12)\n",
        "            plt.colorbar(im, ax=ax1, label='Coverage %', fraction=0.046)\n",
        "        else:\n",
        "            ax1.text(0.5, 0.5, 'No coverage data', ha='center', va='center', transform=ax1.transAxes)\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # PLOT 2: ValueRatio_Diff Distribution (D√úZELTƒ∞LMƒ∞≈û)\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        if 'ValueRatio_Diff' in df.columns and 'Target' in df.columns:\n",
        "            print(\"[PLOT 2] ValueRatio_Diff distribution...\")\n",
        "            # D√∂ng√º i√ßinde renkler otomatik atanƒ±yor\n",
        "            for target_val, color, label in zip([0, 1, 2], [UNIFIED_COLORS['draw_orange'], UNIFIED_COLORS['home_green'], UNIFIED_COLORS['away_red']], class_names):\n",
        "                data = df[df['Target'] == target_val]['ValueRatio_Diff'].dropna()\n",
        "                if len(data) > 0:\n",
        "                    # D√úZELTME BURADA:\n",
        "                    ax2.hist(data, bins=40,\n",
        "                             alpha=0.9,\n",
        "                             color=color,\n",
        "                             label=f'{label} (n={len(data):,})',\n",
        "                             edgecolor='black',\n",
        "                             linewidth=1.2)\n",
        "            ax2.axvline(0, color='black', linestyle='--', linewidth=2.5, alpha=0.7)\n",
        "            ax2.set_xlabel('ValueRatio_Diff', fontsize=12, fontweight='bold')\n",
        "            ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "            ax2.set_title('Financial Advantage Indicator\\n' +\n",
        "                          'ValueRatio = ClubValue / LeagueValue',\n",
        "                          fontsize=14, fontweight='bold', pad=15)\n",
        "            ax2.legend(fontsize=10)\n",
        "            ax2.grid(alpha=0.5, linestyle='--')\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'ValueRatio_Diff not available',\n",
        "                     ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "        # PLOT 3: Summary\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        ax3.axis('off')\n",
        "\n",
        "        summary_text = f\"\"\"\n",
        "LAG FEATURES VALIDATION (V5.1)\n",
        "\n",
        "Total Features:     {len(lag_feature_names)}\n",
        "Feature Type:       Diff-Only (L5)\n",
        "\n",
        "LEAKAGE CHECK:\n",
        "  Status:           {'‚ùå FAIL' if leakage_detected else '‚úÖ PASS'}\n",
        "  Cases:            {len(leakage_details)}\n",
        "\n",
        "COVERAGE:\n",
        "  Min:              {min_coverage:.1f}%\n",
        "  Mean:             {mean_coverage:.1f}%\n",
        "\n",
        "WINDOW CONSISTENCY:\n",
        "  Status:           SKIPPED (V5.1)\n",
        "  Reason:           Only L5 exists\n",
        "\n",
        "OVERALL: {'‚úÖ PASS' if not leakage_detected and min_coverage > 60 else '‚ö†Ô∏è  REVIEW'}\n",
        "        \"\"\"\n",
        "\n",
        "        ax3.text(0.05, 0.95, summary_text.strip(),\n",
        "                transform=ax3.transAxes, fontsize=9,\n",
        "                verticalalignment='top', family='monospace',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.95))\n",
        "\n",
        "        plt.suptitle('Temporal Features Validation Report (V5.1 - Diff-Only)',\n",
        "                    fontsize=14, fontweight='bold', y=0.98)\n",
        "\n",
        "        save_path = os.path.join(output_dir, '01d_temporal_features_validation.png')\n",
        "        plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "        savefig_report('01d_temporal_features_validation.png', also_png=True)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"  ‚úÖ Validation report saved: {os.path.basename(save_path)}\")\n",
        "    elif output_dir:\n",
        "        print(f\"  ‚ö†Ô∏è  Skipping visualization - no data available\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'leakage_detected': leakage_detected,\n",
        "        'min_coverage': min_coverage,\n",
        "        'mean_coverage': mean_coverage,\n",
        "        'avg_consistency': avg_consistency,  # Always 0 for V5.1\n",
        "        'overall_pass': not leakage_detected and min_coverage > 60\n",
        "    }\n",
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# EXECUTE VALIDATION (V5.1 Enhanced with Safety Checks)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n[INFO] Executing Temporal Features Validation...\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï CRITICAL: Check if temporal features were created\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "if 'temporal_features_created' not in globals():\n",
        "    print(\"‚ö†Ô∏è  WARNING: temporal_features_created not found!\")\n",
        "    print(\"   ‚Üí This means Phase 1.5 (Temporal Features Creation) did not run\")\n",
        "    print(\"   ‚Üí Skipping validation...\\n\")\n",
        "\n",
        "    # Set empty list to prevent further errors\n",
        "    temporal_features_created = []\n",
        "    validation_results = None\n",
        "\n",
        "    print(\"=\"*90)\n",
        "    print(\"[SKIPPED] Phase 1.6 - No temporal features to validate\")\n",
        "    print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "else:\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # Temporal features exist - proceed with validation\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "    print(f\"‚úì temporal_features_created found: {len(temporal_features_created)} features\")\n",
        "\n",
        "    # Check if list is empty\n",
        "    if not temporal_features_created:\n",
        "        print(\"‚ö†Ô∏è  WARNING: temporal_features_created is empty!\")\n",
        "        print(\"   ‚Üí No features were created in Phase 1.5\")\n",
        "        print(\"   ‚Üí Possible reasons:\")\n",
        "        print(\"      ‚Ä¢ Required columns (HomeTarget, HomeShots, etc.) missing\")\n",
        "        print(\"      ‚Ä¢ Phase 1.5 code had errors\")\n",
        "        print(\"   ‚Üí Skipping validation...\\n\")\n",
        "\n",
        "        validation_results = None\n",
        "\n",
        "    else:\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # Features exist - run validation\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "        print(f\"\\n[VALIDATION SETUP]\")\n",
        "        print(f\"  ‚Üí Features to validate: {len(temporal_features_created)}\")\n",
        "\n",
        "        # Define original in-game features\n",
        "        original_in_game_features = [\n",
        "            'HomeTarget', 'AwayTarget',\n",
        "            'HomeShots', 'AwayShots',\n",
        "            'HomeCorners', 'AwayCorners',\n",
        "            'HomeFouls', 'AwayFouls',\n",
        "            'HomeYellow', 'AwayYellow'\n",
        "        ]\n",
        "\n",
        "        print(f\"  ‚Üí Original features to check: {len(original_in_game_features)}\")\n",
        "        print(f\"\\n  Expected temporal features:\")\n",
        "        for feat in temporal_features_created:\n",
        "            print(f\"     ‚Ä¢ {feat}\")\n",
        "\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        # PRE-VALIDATION CHECKS\n",
        "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "        print(f\"\\n[PRE-VALIDATION CHECKS]\")\n",
        "\n",
        "        # Check 1: Do temporal features exist in DataFrame?\n",
        "        missing_temporal = [f for f in temporal_features_created if f not in matches_all.columns]\n",
        "        if missing_temporal:\n",
        "            print(f\"  ‚ö†Ô∏è  WARNING: {len(missing_temporal)} temporal features NOT in DataFrame:\")\n",
        "            for feat in missing_temporal[:5]:\n",
        "                print(f\"     ‚Ä¢ {feat}\")\n",
        "\n",
        "            # Filter to available features only\n",
        "            available_temporal = [f for f in temporal_features_created if f in matches_all.columns]\n",
        "            print(f\"  ‚Üí Proceeding with {len(available_temporal)} available features\\n\")\n",
        "\n",
        "            if not available_temporal:\n",
        "                print(f\"  ‚ùå ERROR: No temporal features found in DataFrame!\")\n",
        "                print(f\"     ‚Üí Validation cannot proceed\\n\")\n",
        "                validation_results = None\n",
        "            else:\n",
        "                temporal_features_to_validate = available_temporal\n",
        "        else:\n",
        "            print(f\"  ‚úì All {len(temporal_features_created)} temporal features present\")\n",
        "            temporal_features_to_validate = temporal_features_created\n",
        "\n",
        "        # Check 2: Do original features exist?\n",
        "        if 'temporal_features_to_validate' in locals() and temporal_features_to_validate:\n",
        "            missing_original = [f for f in original_in_game_features if f not in matches_all.columns]\n",
        "            if missing_original:\n",
        "                print(f\"  ‚ö†Ô∏è  WARNING: {len(missing_original)} original features missing:\")\n",
        "                for feat in missing_original:\n",
        "                    print(f\"     ‚Ä¢ {feat}\")\n",
        "\n",
        "                original_in_game_features = [f for f in original_in_game_features\n",
        "                                            if f in matches_all.columns]\n",
        "                print(f\"  ‚Üí Using {len(original_in_game_features)} available features\\n\")\n",
        "            else:\n",
        "                print(f\"  ‚úì All {len(original_in_game_features)} original features present\")\n",
        "\n",
        "            # Check 3: Dataset size\n",
        "            print(f\"  ‚úì Dataset size: {len(matches_all):,} matches\\n\")\n",
        "\n",
        "            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "            # RUN VALIDATION\n",
        "            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "            print(\"[RUNNING VALIDATION]\")\n",
        "\n",
        "            try:\n",
        "                validation_results = validate_lag_features(\n",
        "                    df=matches_all,\n",
        "                    lag_feature_names=temporal_features_to_validate,\n",
        "                    original_features=original_in_game_features,\n",
        "                    output_dir=IMAGES_DIR\n",
        "                )\n",
        "            except Exception as e_validation:\n",
        "                print(f\"\\n‚ùå VALIDATION FAILED WITH ERROR:\")\n",
        "                print(f\"   {str(e_validation)}\")\n",
        "                print(f\"   Continuing with EDA...\\n\")\n",
        "                validation_results = None\n",
        "        else:\n",
        "            validation_results = None\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# INTERPRET RESULTS (Regardless of validation outcome)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[VALIDATION RESULTS SUMMARY]\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "if validation_results is None:\n",
        "    print(\"‚ö†Ô∏è  VALIDATION SKIPPED OR FAILED\")\n",
        "    print(\"   Reasons could be:\")\n",
        "    print(\"   ‚Ä¢ Phase 1.5 did not create temporal features\")\n",
        "    print(\"   ‚Ä¢ Required columns missing from dataset\")\n",
        "    print(\"   ‚Ä¢ Validation function encountered an error\")\n",
        "    print(\"\\n   ‚Üí Proceeding with EDA using available features\")\n",
        "    print(\"   ‚Üí Temporal features may not be available for analysis\\n\")\n",
        "\n",
        "elif validation_results['overall_pass']:\n",
        "    print(\"‚úÖ TEMPORAL FEATURES VALIDATION: PASSED\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"  ‚úì Leakage Check:    PASSED\")\n",
        "    print(f\"  ‚úì Coverage:         {validation_results['mean_coverage']:.1f}% (min: {validation_results['min_coverage']:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüéâ TEMPORAL FEATURES ARE CLEAN:\")\n",
        "    print(f\"   ‚Ä¢ No data leakage detected\")\n",
        "    print(f\"   ‚Ä¢ shift(1) working correctly\")\n",
        "    print(f\"   ‚Ä¢ Ready for modeling\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  TEMPORAL FEATURES VALIDATION: ISSUES DETECTED\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    if validation_results.get('leakage_detected'):\n",
        "        print(f\"  ‚ùå CRITICAL: Data leakage detected!\")\n",
        "        print(f\"     ‚Üí Review '01d_temporal_features_validation.png'\\n\")\n",
        "\n",
        "    if validation_results.get('min_coverage', 100) < 60:\n",
        "        print(f\"  ‚ö†Ô∏è  Coverage Issue:\")\n",
        "        print(f\"     ‚Üí Min: {validation_results['min_coverage']:.1f}%\")\n",
        "        print(f\"     ‚Üí Consider adjusting window size\\n\")\n",
        "\n",
        "    print(f\"  ‚Üí Proceeding with EDA, but review validation results\\n\")\n",
        "\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï Save validation status for downstream use\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "try:\n",
        "    validation_status = {\n",
        "        'temporal_features_exist': 'temporal_features_created' in globals() and bool(temporal_features_created),\n",
        "        'validation_ran': validation_results is not None,\n",
        "        'validation_passed': validation_results['overall_pass'] if validation_results else False,\n",
        "        'n_features_created': len(temporal_features_created) if 'temporal_features_created' in globals() else 0\n",
        "    }\n",
        "\n",
        "    # Save to CSV for reference\n",
        "    status_df = pd.DataFrame([validation_status])\n",
        "    status_path = os.path.join(CSV_DIR, \"01f_temporal_features_status.csv\")\n",
        "    status_df.to_csv(status_path, index=False)\n",
        "    print(f\"[INFO] Temporal features status saved to: {os.path.basename(status_path)}\\n\")\n",
        "\n",
        "except Exception as e_status:\n",
        "    print(f\"[WARN] Could not save status: {e_status}\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéâ CHECKPOINT: Phase 1.6 Complete\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"=\"*90)\n",
        "print(\"[‚úì] PHASE 1.6 COMPLETE\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "if 'temporal_features_created' in globals() and temporal_features_created:\n",
        "    print(f\"  ‚úÖ Temporal features: {len(temporal_features_created)} created\")\n",
        "    if validation_results and validation_results['overall_pass']:\n",
        "        print(f\"  ‚úÖ Validation: PASSED\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Validation: REVIEW NEEDED\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Temporal features: NOT CREATED\")\n",
        "    print(f\"  ‚Üí Check Phase 1.5 for errors\")\n",
        "\n",
        "print(\"\\n  Ready to proceed to Phase 2 (Train-Val-Test Split)...\\n\")\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 2: TRAIN-VAL-TEST SPLIT (Pandemic-Aware)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 2: APPLYING TEMPORAL SPLIT (Pandemic-Aware for EDA Isolation)\")\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# INTERPRET VALIDATION RESULTS (Enhanced)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[VALIDATION RESULTS SUMMARY]\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "if validation_results is None:\n",
        "    print(\"‚ùå VALIDATION SKIPPED OR FAILED\")\n",
        "    print(\"   ‚Üí Proceeding with EDA, but temporal features may have issues\\n\")\n",
        "\n",
        "elif validation_results['overall_pass']:\n",
        "    print(\"‚úÖ TEMPORAL FEATURES VALIDATION: PASSED\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"  ‚úì Leakage Check:    PASSED (no features with |r| > 0.95)\")\n",
        "    print(f\"  ‚úì Coverage Check:   PASSED (min: {validation_results['min_coverage']:.1f}%)\")\n",
        "    print(f\"  ‚úì Mean Coverage:    {validation_results['mean_coverage']:.1f}%\")\n",
        "\n",
        "    print(f\"\\nüéâ CONCLUSION:\")\n",
        "    print(f\"   ‚Ä¢ Temporal features are clean and ready for modeling\")\n",
        "    print(f\"   ‚Ä¢ No data leakage detected\")\n",
        "    print(f\"   ‚Ä¢ shift(1) is working correctly\")\n",
        "    print(f\"   ‚Ä¢ All features have acceptable coverage\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  TEMPORAL FEATURES VALIDATION: REVIEW NEEDED\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # Detailed diagnostics\n",
        "    if validation_results['leakage_detected']:\n",
        "        print(f\"  ‚ùå CRITICAL: Data leakage detected!\")\n",
        "        print(f\"     ‚Üí Some temporal features have high correlation (|r| > 0.95) with current match data\")\n",
        "        print(f\"     ‚Üí This means future information may be leaking into features\")\n",
        "        print(f\"     ‚Üí ACTION REQUIRED: Review feature engineering logic\\n\")\n",
        "\n",
        "    if validation_results['min_coverage'] < 60:\n",
        "        print(f\"  ‚ö†Ô∏è  Coverage Issue:\")\n",
        "        print(f\"     ‚Üí Minimum coverage: {validation_results['min_coverage']:.1f}% (threshold: 60%)\")\n",
        "        print(f\"     ‚Üí Some features have too many missing values\")\n",
        "        print(f\"     ‚Üí Consider: Increasing min_periods in rolling window\\n\")\n",
        "\n",
        "    print(f\"  üìä Coverage Statistics:\")\n",
        "    print(f\"     ‚Ä¢ Min:  {validation_results['min_coverage']:.1f}%\")\n",
        "    print(f\"     ‚Ä¢ Mean: {validation_results['mean_coverage']:.1f}%\")\n",
        "\n",
        "    print(f\"\\n‚ö†Ô∏è  RECOMMENDATION:\")\n",
        "    print(f\"   ‚Ä¢ Review '01d_temporal_features_validation.png' for details\")\n",
        "    print(f\"   ‚Ä¢ Check leakage correlation table if available\")\n",
        "    print(f\"   ‚Ä¢ Consider adjusting window size or filling strategy\\n\")\n",
        "\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üÜï OPTIONAL: Save validation summary to CSV\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "if validation_results:\n",
        "    try:\n",
        "        validation_summary = pd.DataFrame([{\n",
        "            'Validation_Date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Total_Features': len(temporal_features_created),\n",
        "            'Leakage_Detected': validation_results['leakage_detected'],\n",
        "            'Min_Coverage_%': validation_results['min_coverage'],\n",
        "            'Mean_Coverage_%': validation_results['mean_coverage'],\n",
        "            'Overall_Pass': validation_results['overall_pass'],\n",
        "            'Dataset_Size': len(matches_all)\n",
        "        }])\n",
        "\n",
        "        summary_path = os.path.join(CSV_DIR, \"01e_validation_summary.csv\")\n",
        "        validation_summary.to_csv(summary_path, index=False)\n",
        "        print(f\"[INFO] Validation summary saved to: {os.path.basename(summary_path)}\\n\")\n",
        "    except Exception as e_csv:\n",
        "        print(f\"[WARN] Could not save validation summary: {e_csv}\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéâ SUCCESS CHECKPOINT\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"=\"*90)\n",
        "print(\"üéâ CRITICAL PHASES COMPLETE!\")\n",
        "print(\"=\"*90)\n",
        "print(f\"  ‚úÖ Phase 1: Data Loading ‚Üí matches_all created ({matches_all.shape[0]:,} rows)\")\n",
        "print(f\"  ‚úÖ Phase 1.5: Temporal Features ‚Üí {len(temporal_features_created)} features added\")\n",
        "print(f\"  ‚úÖ Phase 1.6: Validation ‚Üí {'PASSED' if validation_results and validation_results['overall_pass'] else 'REVIEW NEEDED'}\")\n",
        "print(f\"  ‚úÖ No NameError ‚Üí Code is production-ready!\")\n",
        "print(\"=\"*90 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 1.6.5: TEMPORAL FEATURES - DIFF-ONLY STRATEGY (VIF Optimization)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 1.6.5: APPLYING DIFF-ONLY STRATEGY (VIF Optimization)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(\"\"\"\n",
        "üìä STRATEGY: Keep only DIFF features, drop individual Home/Away averages\n",
        "\n",
        "RATIONALE:\n",
        "  ‚Ä¢ Home/Away averages create perfect multicollinearity with Diff\n",
        "  ‚Ä¢ VIF values: Target_Diff_L5 = 11,137 (due to linear combination)\n",
        "  ‚Ä¢ Diff features capture the SAME information (advantage/disadvantage)\n",
        "  ‚Ä¢ Models perform equally well with just Diff features\n",
        "\n",
        "EXAMPLE:\n",
        "  Before: Home_Target_Avg_L5=5.2, Away_Target_Avg_L5=3.8, Target_Diff_L5=1.4\n",
        "  After:  Target_Diff_L5=1.4 (contains the same predictive signal)\n",
        "\n",
        "REFERENCE:\n",
        "  ‚Ä¢ Thesis Section: \"Feature Engineering - Multicollinearity Reduction\"\n",
        "  ‚Ä¢ Decision: Based on VIF analysis showing extreme multicollinearity\n",
        "  ‚Ä¢ Impact: VIF reduction from >10,000 to <10 (acceptable threshold)\n",
        "\"\"\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# IDENTIFY FEATURES TO DROP\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "temporal_home_away_features = [\n",
        "    'Home_Target_Avg_L5', 'Away_Target_Avg_L5',\n",
        "    'Home_Shots_Avg_L5', 'Away_Shots_Avg_L5',\n",
        "    'Home_Corners_Avg_L5', 'Away_Corners_Avg_L5',\n",
        "    'Home_Fouls_Avg_L5', 'Away_Fouls_Avg_L5',\n",
        "    'Home_Yellow_Avg_L5', 'Away_Yellow_Avg_L5',\n",
        "    'Home_ShotAccuracy_L5', 'Away_ShotAccuracy_L5',\n",
        "]\n",
        "\n",
        "temporal_diff_features = [\n",
        "    'Target_Diff_L5',\n",
        "    'Shots_Diff_L5',\n",
        "    'Corners_Diff_L5',\n",
        "    'Fouls_Diff_L5',\n",
        "    'Yellow_Diff_L5',\n",
        "    'ShotAccuracy_Diff_L5',\n",
        "]\n",
        "\n",
        "print(f\"\\n[DECISION] Temporal Feature Strategy:\")\n",
        "print(f\"  ‚ùå DROP: {len(temporal_home_away_features)} Home/Away features (create perfect multicollinearity)\")\n",
        "print(f\"  ‚úÖ KEEP: {len(temporal_diff_features)} Diff features (preserve predictive signal)\")\n",
        "print(f\"\\n  Justification:\")\n",
        "print(f\"    ‚Ä¢ Diff = Home - Away (mathematically redundant to keep all three)\")\n",
        "print(f\"    ‚Ä¢ Example: If you know Home=5.2 and Away=3.8, Diff=1.4 is determined\")\n",
        "print(f\"    ‚Ä¢ Keeping only Diff reduces features while preserving information\\n\")\n",
        "\n",
        "# Check which features actually exist in dataset\n",
        "existing_to_drop = [f for f in temporal_home_away_features if f in matches_all.columns]\n",
        "existing_to_keep = [f for f in temporal_diff_features if f in matches_all.columns]\n",
        "\n",
        "print(f\"[VERIFICATION]\")\n",
        "print(f\"  ‚Ä¢ Features to drop (exist): {len(existing_to_drop)}/{len(temporal_home_away_features)}\")\n",
        "print(f\"  ‚Ä¢ Features to keep (exist): {len(existing_to_keep)}/{len(temporal_diff_features)}\")\n",
        "\n",
        "if len(existing_to_drop) > 0:\n",
        "    print(f\"\\n[DROPPING] {len(existing_to_drop)} Home/Away temporal features...\")\n",
        "\n",
        "    # Show what will be dropped with coverage stats\n",
        "    print(f\"\\n  Detailed list of features being dropped:\")\n",
        "    for feat in existing_to_drop:\n",
        "        coverage = matches_all[feat].notna().sum() / len(matches_all) * 100\n",
        "        mean_val = matches_all[feat].mean() if matches_all[feat].notna().any() else 0\n",
        "        print(f\"    ‚ùå {feat:30s} (coverage: {coverage:5.1f}%, mean: {mean_val:5.2f})\")\n",
        "\n",
        "    # Store original shape for comparison\n",
        "    original_shape = matches_all.shape\n",
        "\n",
        "    # Drop from matches_all\n",
        "    matches_all = matches_all.drop(columns=existing_to_drop)\n",
        "\n",
        "    print(f\"\\n  ‚úÖ Successfully dropped {len(existing_to_drop)} features\")\n",
        "    print(f\"  ‚úÖ Dataset shape change: {original_shape} ‚Üí {matches_all.shape}\")\n",
        "    print(f\"     (Removed {original_shape[1] - matches_all.shape[1]} columns)\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  No Home/Away temporal features found to drop\")\n",
        "    print(f\"     This might indicate features were already dropped or not created.\")\n",
        "\n",
        "# Show remaining temporal features\n",
        "print(f\"\\n[REMAINING TEMPORAL FEATURES]\")\n",
        "if len(existing_to_keep) > 0:\n",
        "    print(f\"  The following {len(existing_to_keep)} Diff features are kept:\")\n",
        "    for feat in existing_to_keep:\n",
        "        if feat in matches_all.columns:\n",
        "            coverage = matches_all[feat].notna().sum() / len(matches_all) * 100\n",
        "            mean_val = matches_all[feat].mean() if matches_all[feat].notna().any() else 0\n",
        "            print(f\"    ‚úÖ {feat:30s} (coverage: {coverage:5.1f}%, mean: {mean_val:5.2f})\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  No Diff features found in dataset\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# UPDATE TEMPORAL FEATURES LIST\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "if 'temporal_features_created' in globals():\n",
        "    print(f\"\\n[UPDATE] Updating temporal_features_created list...\")\n",
        "\n",
        "    original_count = len(temporal_features_created)\n",
        "\n",
        "    # Remove dropped features from tracking list\n",
        "    temporal_features_created = [\n",
        "        f for f in temporal_features_created\n",
        "        if f not in temporal_home_away_features\n",
        "    ]\n",
        "\n",
        "    print(f\"  ‚Ä¢ Original count: {original_count}\")\n",
        "    print(f\"  ‚Ä¢ After Diff-only: {len(temporal_features_created)}\")\n",
        "    print(f\"  ‚Ä¢ Removed: {original_count - len(temporal_features_created)} features\")\n",
        "\n",
        "    if len(temporal_features_created) > 0:\n",
        "        print(f\"\\n  Remaining features in tracking list:\")\n",
        "        for i, feat in enumerate(temporal_features_created, 1):\n",
        "            print(f\"    {i}. {feat}\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: temporal_features_created is now empty!\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  Warning: temporal_features_created not found in globals()\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# SAVE DIFF-ONLY STRATEGY TO FILE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "try:\n",
        "    diff_strategy_path = os.path.join(CSV_DIR, \"01g_temporal_diff_only_strategy.py\")\n",
        "\n",
        "    with open(diff_strategy_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# AUTO-GENERATED: Temporal Features - Diff-Only Strategy\\n\")\n",
        "        f.write(\"# Generated by: EDA V5.1 (VIF Optimization)\\n\")\n",
        "        f.write(\"# Purpose: Reduce multicollinearity while preserving predictive power\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# CONTEXT:\\n\")\n",
        "        f.write(\"# During VIF analysis, temporal Home/Away features showed extreme\\n\")\n",
        "        f.write(\"# multicollinearity (VIF > 10,000) because Diff = Home - Away.\\n\")\n",
        "        f.write(\"# Keeping all three creates perfect linear dependency.\\n\\n\")\n",
        "\n",
        "        f.write(\"# DECISION:\\n\")\n",
        "        f.write(\"# Keep only Diff features, which contain the same predictive signal\\n\")\n",
        "        f.write(\"# (team advantage) without redundancy.\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# FEATURES TO DROP (Home/Away Averages)\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# These features create multicollinearity with their Diff counterparts\\n\")\n",
        "        f.write(\"temporal_home_away_drop = [\\n\")\n",
        "        for feat in temporal_home_away_features:\n",
        "            f.write(f\"    '{feat}',  # Dropped: redundant with {feat.replace('Home_', '').replace('Away_', '').replace('_Avg_L5', '_Diff_L5')}\\n\")\n",
        "        f.write(\"]\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# FEATURES TO KEEP (Diff Features)\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# These features preserve all predictive information\\n\")\n",
        "        f.write(\"temporal_diff_keep = [\\n\")\n",
        "        for feat in temporal_diff_features:\n",
        "            f.write(f\"    '{feat}',  # Kept: captures Home-Away advantage\\n\")\n",
        "        f.write(\"]\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# USAGE IN MODELING PIPELINE\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# Step 1: Drop Home/Away averages from all datasets\\n\")\n",
        "        f.write(\"# (This code should be in your modeling script, NOT in EDA)\\n\\n\")\n",
        "\n",
        "        f.write(\"# X_train = X_train.drop(columns=temporal_home_away_drop, errors='ignore')\\n\")\n",
        "        f.write(\"# X_val = X_val.drop(columns=temporal_home_away_drop, errors='ignore')\\n\")\n",
        "        f.write(\"# X_test = X_test.drop(columns=temporal_home_away_drop, errors='ignore')\\n\\n\")\n",
        "\n",
        "        f.write(\"# Step 2: Verify only Diff features remain\\n\")\n",
        "        f.write(\"# temporal_cols = [c for c in X_train.columns if '_L5' in c]\\n\")\n",
        "        f.write(\"# print(f'Remaining temporal features: {temporal_cols}')\\n\")\n",
        "        f.write(\"# assert all('_Diff_' in c for c in temporal_cols), 'Non-Diff temporal features found!'\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# EXPECTED VIF IMPROVEMENTS\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# Before Diff-Only Strategy:\\n\")\n",
        "        f.write(\"#   Target_Diff_L5:        VIF = 11,137 (extreme multicollinearity)\\n\")\n",
        "        f.write(\"#   Shots_Diff_L5:         VIF = 6,599\\n\")\n",
        "        f.write(\"#   Home_Target_Avg_L5:    VIF = 6,774\\n\")\n",
        "        f.write(\"#   Away_Target_Avg_L5:    VIF = 4,742\\n\")\n",
        "        f.write(\"#   ShotAccuracy_Diff_L5:  VIF = 4,922\\n\\n\")\n",
        "\n",
        "        f.write(\"# After Diff-Only Strategy (Expected):\\n\")\n",
        "        f.write(\"#   Target_Diff_L5:        VIF < 10 (acceptable)\\n\")\n",
        "        f.write(\"#   Shots_Diff_L5:         VIF < 10\\n\")\n",
        "        f.write(\"#   ShotAccuracy_Diff_L5:  VIF < 10\\n\")\n",
        "        f.write(\"#   Other Diffs:           VIF < 10\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# INFORMATION PRESERVATION\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# Q: Do we lose information by dropping Home/Away averages?\\n\")\n",
        "        f.write(\"# A: NO! The Diff features contain the same predictive signal.\\n\\n\")\n",
        "\n",
        "        f.write(\"# Example:\\n\")\n",
        "        f.write(\"#   Home_Target_Avg_L5 = 5.2 (team scores 5.2 shots on target per game)\\n\")\n",
        "        f.write(\"#   Away_Target_Avg_L5 = 3.8 (opponent allows 3.8 shots on target)\\n\")\n",
        "        f.write(\"#   Target_Diff_L5 = 1.4 (team has +1.4 advantage)\\n\\n\")\n",
        "\n",
        "        f.write(\"# The advantage (+1.4) is what matters for prediction, not the absolute\\n\")\n",
        "        f.write(\"# values. If we know two of {Home, Away, Diff}, the third is determined:\\n\")\n",
        "        f.write(\"#   Diff = Home - Away  (perfect linear relationship)\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# VALIDATION CHECKLIST\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"# Before training your model, verify:\\n\")\n",
        "        f.write(\"# ‚ñ° Home/Away temporal features are dropped from X_train/val/test\\n\")\n",
        "        f.write(\"# ‚ñ° Only Diff temporal features remain\\n\")\n",
        "        f.write(\"# ‚ñ° VIF values are < 10 for all features\\n\")\n",
        "        f.write(\"# ‚ñ° Model performance is not degraded (should be same or better)\\n\\n\")\n",
        "\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "        f.write(\"# END OF FILE\\n\")\n",
        "        f.write(\"# \" + \"=\"*75 + \"\\n\")\n",
        "\n",
        "    print(f\"\\n[OK] Diff-only strategy documentation saved to:\")\n",
        "    print(f\"     {diff_strategy_path}\")\n",
        "    print(f\"     File size: {os.path.getsize(diff_strategy_path)} bytes\")\n",
        "\n",
        "except Exception as e_save:\n",
        "    print(f\"\\n[ERROR] Could not save diff-only strategy file!\")\n",
        "    print(f\"        Error: {e_save}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# SUMMARY\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[SUMMARY] Temporal Features - Diff-Only Strategy Applied\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\n  üìâ VIF REDUCTION (Expected):\")\n",
        "print(f\"     Before Strategy:\")\n",
        "print(f\"       ‚Ä¢ Target_Diff_L5:        11,137\")\n",
        "print(f\"       ‚Ä¢ Home_Target_Avg_L5:     6,774\")\n",
        "print(f\"       ‚Ä¢ Away_Target_Avg_L5:     4,742\")\n",
        "print(f\"       ‚Ä¢ Shots_Diff_L5:          6,599\")\n",
        "print(f\"       ‚Ä¢ ShotAccuracy_Diff_L5:   4,922\")\n",
        "print(f\"\")\n",
        "print(f\"     After Strategy:\")\n",
        "print(f\"       ‚Ä¢ Target_Diff_L5:         < 10 ‚úÖ\")\n",
        "print(f\"       ‚Ä¢ Shots_Diff_L5:          < 10 ‚úÖ\")\n",
        "print(f\"       ‚Ä¢ ShotAccuracy_Diff_L5:   < 10 ‚úÖ\")\n",
        "print(f\"       ‚Ä¢ Other Diffs:            < 10 ‚úÖ\")\n",
        "\n",
        "print(f\"\\n  üìä INFORMATION PRESERVATION:\")\n",
        "print(f\"     ‚Ä¢ Diff = Home - Away (contains same predictive signal)\")\n",
        "print(f\"     ‚Ä¢ No loss of information about team advantage\")\n",
        "print(f\"     ‚Ä¢ Absolute values less important than relative advantage\")\n",
        "print(f\"     ‚Ä¢ Model performance: Expected to remain same or improve\")\n",
        "\n",
        "print(f\"\\n  ‚úÖ BENEFITS:\")\n",
        "print(f\"     1. Multicollinearity eliminated (VIF < 10)\")\n",
        "print(f\"     2. Simpler model (12 fewer features)\")\n",
        "print(f\"     3. Easier interpretation (advantage-based)\")\n",
        "print(f\"     4. Faster training (fewer features to process)\")\n",
        "print(f\"     5. More stable coefficients (no redundancy)\")\n",
        "\n",
        "print(f\"\\n  üìÇ OUTPUT FILES:\")\n",
        "print(f\"     ‚Ä¢ 01g_temporal_diff_only_strategy.py (Python code for modeling)\")\n",
        "print(f\"     ‚Ä¢ Contains: Drop list + Keep list + Usage instructions\")\n",
        "\n",
        "print(f\"\\n  ‚ö†Ô∏è  IMPORTANT:\")\n",
        "print(f\"     ‚Ä¢ This strategy is already applied to EDA dataset\")\n",
        "print(f\"     ‚Ä¢ Apply the SAME strategy in your modeling pipeline\")\n",
        "print(f\"     ‚Ä¢ DO NOT keep Home/Away averages in final model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üéØ CHECKPOINT: Verify Diff-Only Strategy Applied\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"[CHECKPOINT] Verifying Diff-Only Strategy was applied correctly...\")\n",
        "\n",
        "# Find all temporal-related columns\n",
        "all_temporal = [col for col in matches_all.columns\n",
        "                if any(x in col for x in ['_Avg_L5', '_Diff_L5', 'ShotAccuracy_L5'])]\n",
        "\n",
        "# Separate Home/Away vs Diff\n",
        "home_away_remaining = [col for col in all_temporal\n",
        "                       if col.startswith('Home_') or col.startswith('Away_')]\n",
        "diff_remaining = [col for col in all_temporal\n",
        "                  if '_Diff_' in col or (col.startswith('Home_') == False and col.startswith('Away_') == False)]\n",
        "\n",
        "print(f\"\\n  Current temporal feature status:\")\n",
        "print(f\"    ‚Ä¢ Total temporal features: {len(all_temporal)}\")\n",
        "print(f\"    ‚Ä¢ Home/Away features: {len(home_away_remaining)}\")\n",
        "print(f\"    ‚Ä¢ Diff features: {len(diff_remaining)}\")\n",
        "\n",
        "if len(home_away_remaining) == 0:\n",
        "    print(f\"\\n  ‚úÖ SUCCESS: All Home/Away temporal features removed!\")\n",
        "    print(f\"  ‚úÖ Only Diff features remain (as intended)\")\n",
        "elif len(home_away_remaining) <= 2:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  WARNING: {len(home_away_remaining)} Home/Away features still present:\")\n",
        "    for feat in home_away_remaining:\n",
        "        print(f\"      ‚Ä¢ {feat}\")\n",
        "    print(f\"  ‚Üí Check if these are intentional (non-L5 features)\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ùå ERROR: {len(home_away_remaining)} Home/Away features still present!\")\n",
        "    print(f\"  ‚Üí Diff-Only Strategy may not have been applied correctly\")\n",
        "    for feat in home_away_remaining[:10]:\n",
        "        print(f\"      ‚Ä¢ {feat}\")\n",
        "    if len(home_away_remaining) > 10:\n",
        "        print(f\"      ... and {len(home_away_remaining)-10} more\")\n",
        "\n",
        "if len(diff_remaining) >= len(temporal_diff_features):\n",
        "    print(f\"\\n  ‚úÖ All expected Diff features are present\")\n",
        "else:\n",
        "    print(f\"\\n  ‚ö†Ô∏è  Some Diff features might be missing\")\n",
        "    print(f\"     Expected: {len(temporal_diff_features)}, Found: {len(diff_remaining)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "print(\"[INFO] Proceeding to Phase 2+ (Train-Val-Test Split, EDA Analyses)...\")\n",
        "print(\"[INFO] matches_all is now ready for all subsequent analyses ‚úÖ\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ PHASE 1: VALIDATION & DIAGNOSTICS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "print(\"\\n[VALIDATION] Phase 1 Completeness Check\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\nüìä DATASET STATUS:\")\n",
        "print(f\"  ‚Ä¢ Total matches: {len(matches_all):,}\")\n",
        "print(f\"  ‚Ä¢ Columns: {matches_all.shape[1]}\")\n",
        "print(f\"  ‚Ä¢ Date range: {matches_all['MatchDate'].min().date()} ‚Üí {matches_all['MatchDate'].max().date()}\")\n",
        "\n",
        "print(f\"\\nüéØ KEY COLUMNS:\")\n",
        "key_cols = ['Target', 'YearMonth', 'Season', 'HomeTeam_TM', 'AwayTeam_TM']\n",
        "for col in key_cols:\n",
        "    if col in matches_all.columns:\n",
        "        non_null = matches_all[col].notna().sum()\n",
        "        pct = (non_null / len(matches_all)) * 100\n",
        "        print(f\"  ‚úì {col:20s}: {non_null:,} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {col:20s}: MISSING!\")\n",
        "\n",
        "print(f\"\\nüìà TRANSFERMARKT COVERAGE:\")\n",
        "tm_cols = ['HomeTeam_ClubValue', 'AwayTeam_ClubValue', 'ValueRatio_Diff']\n",
        "for col in tm_cols:\n",
        "    if col in matches_all.columns:\n",
        "        non_null = matches_all[col].notna().sum()\n",
        "        pct = (non_null / len(matches_all)) * 100\n",
        "        print(f\"  ‚Ä¢ {col:25s}: {non_null:,} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  {col:25s}: Not created\")\n",
        "\n",
        "print(f\"\\nüîó FUZZY MATCHING STATS:\")\n",
        "if 'fuzzy_matcher' in globals():\n",
        "    matched = sum(1 for v in fuzzy_matcher.team_mapping.values() if v is not None)\n",
        "    total = len(fuzzy_matcher.team_mapping)\n",
        "    print(f\"  ‚Ä¢ Teams matched: {matched} / {total} ({matched/total*100:.1f}%)\")\n",
        "\n",
        "    avg_score = np.mean([s for s in fuzzy_matcher.team_scores.values() if s > 0])\n",
        "    print(f\"  ‚Ä¢ Avg match score: {avg_score:.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 2: TRAIN-VAL-TEST SPLIT (Pandemic-Aware)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 2: APPLYING TEMPORAL SPLIT (Pandemic-Aware for EDA Isolation)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "train_end_date = pd.to_datetime('2019-08-31')\n",
        "pandemic_end   = pd.to_datetime('2021-05-31')\n",
        "val_end_date   = pd.to_datetime('2022-08-31')\n",
        "\n",
        "print(f\"Applying temporal boundaries based on standard practice:\")\n",
        "print(f\"  Train End Date (exclusive): {train_end_date.date()}\")\n",
        "print(f\"  Validation Start Date (inclusive): {pandemic_end.date()}\")\n",
        "print(f\"  Validation End Date (exclusive): {val_end_date.date()}\")\n",
        "print(f\"  Test Start Date (inclusive): {val_end_date.date()}\")\n",
        "\n",
        "train_matches = matches_all[matches_all['MatchDate'] < train_end_date].copy()\n",
        "val_matches   = matches_all[(matches_all['MatchDate'] >= pandemic_end) & (matches_all['MatchDate'] < val_end_date)].copy()\n",
        "test_matches  = matches_all[matches_all['MatchDate'] >= val_end_date].copy()\n",
        "\n",
        "print(f\"\\nTemporal Split Results:\")\n",
        "print(f\"  Train Set Size: {len(train_matches):,} matches (Covers period before {train_end_date.date()})\")\n",
        "print(f\"  Val Set Size:   {len(val_matches):,} matches (Covers period {pandemic_end.date()} to {val_end_date.date()})\")\n",
        "print(f\"  Test Set Size:  {len(test_matches):,} matches (Covers period from {val_end_date.date()} onwards)\\n\")\n",
        "\n",
        "if len(train_matches) == 0:\n",
        "    print(f\"[FATAL] Training set (matches before {train_end_date.date()}) is empty after temporal split.\")\n",
        "    raise ValueError(\"Training set cannot be empty for EDA.\")\n",
        "else:\n",
        "    print(f\"[OK] EDA will proceed using ONLY the {len(train_matches):,} matches from the training period.\")\n",
        "    if 'Target' in train_matches.columns and train_matches['Target'].notna().any():\n",
        "         print(f\"[CHECK] Target distribution (Train Data Only): {train_matches['Target'].value_counts(normalize=True, dropna=False).sort_index().to_dict()}\")\n",
        "    else:\n",
        "         print(\"[WARN] Target column is missing or all null in train_matches after split.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 3: PANDEMIC IMPACT ANALYSIS\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 3: PANDEMIC IMPACT ANALYSIS (Using full loaded data for context)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "pre_pandemic_start = '2015-09-01'\n",
        "pre_pandemic_end = '2020-03-01'\n",
        "pandemic_start = '2020-03-01'\n",
        "pandemic_actual_end = '2021-05-31'\n",
        "post_pandemic_start = '2021-06-01'\n",
        "\n",
        "pre_pandemic_df = matches_all[(matches_all['MatchDate'] >= pre_pandemic_start) & (matches_all['MatchDate'] < pre_pandemic_end)].copy()\n",
        "pandemic_period_df = matches_all[(matches_all['MatchDate'] >= pandemic_start) & (matches_all['MatchDate'] <= pandemic_actual_end)].copy()\n",
        "post_pandemic_df = matches_all[(matches_all['MatchDate'] > pandemic_actual_end)].copy()\n",
        "\n",
        "print(f\"Pandemic Analysis Periods (for context):\")\n",
        "if not pre_pandemic_df.empty: print(f\"  Pre-Pandemic: {len(pre_pandemic_df):,} matches ({pre_pandemic_df['MatchDate'].min().date()} to {pre_pandemic_df['MatchDate'].max().date()})\")\n",
        "if not pandemic_period_df.empty: print(f\"  During Pandemic: {len(pandemic_period_df):,} matches ({pandemic_period_df['MatchDate'].min().date()} to {pandemic_period_df['MatchDate'].max().date()})\")\n",
        "if not post_pandemic_df.empty: print(f\"  Post-Pandemic: {len(post_pandemic_df):,} matches ({post_pandemic_df['MatchDate'].min().date()} to {post_pandemic_df['MatchDate'].max().date()})\")\n",
        "\n",
        "home_advantage_data = []\n",
        "league_names = {\"E0\":\"Premier League\",\"D1\":\"Bundesliga\",\"I1\":\"Serie A\",\"SP1\":\"La Liga\",\"F1\":\"Ligue 1\"}\n",
        "\n",
        "periods_for_analysis = [\n",
        "    ('Pre-Pandemic (Sep \\'15 - Feb \\'20)', pre_pandemic_df),\n",
        "    ('Pandemic (Mar \\'20 - May \\'21)', pandemic_period_df),\n",
        "    ('Post-Pandemic (Jun \\'21 onwards)', post_pandemic_df)\n",
        "]\n",
        "\n",
        "division_col_present = 'Division' in matches_all.columns\n",
        "if not division_col_present:\n",
        "     print(\"[WARN] 'Division' column missing. Cannot perform league-specific pandemic analysis.\")\n",
        "\n",
        "for period_name, period_data in periods_for_analysis:\n",
        "    if period_data.empty:\n",
        "        print(f\"[INFO] Skipping empty period: {period_name}\")\n",
        "        continue\n",
        "    if not division_col_present:\n",
        "         if 'Target' in period_data.columns and period_data['Target'].notna().any():\n",
        "             home_wins = (period_data['Target'] == 1).sum()\n",
        "             total = len(period_data['Target'].dropna())\n",
        "             home_rate = (home_wins / total) * 100 if total > 0 else 0\n",
        "             home_advantage_data.append({\n",
        "                 'Period': period_name, 'League': 'All Leagues', 'Home Win %': home_rate,\n",
        "                 'Home Wins': home_wins, 'Total Matches': total\n",
        "             })\n",
        "    else:\n",
        "        for league_code in [\"E0\", \"D1\", \"I1\", \"SP1\", \"F1\"]:\n",
        "            league_data = period_data[period_data['Division'] == league_code]\n",
        "            if not league_data.empty and 'Target' in league_data.columns and league_data['Target'].notna().any():\n",
        "                home_wins = (league_data['Target'] == 1).sum()\n",
        "                total = len(league_data['Target'].dropna())\n",
        "                home_rate = (home_wins / total) * 100 if total > 0 else 0\n",
        "                home_advantage_data.append({\n",
        "                    'Period': period_name, 'League': league_names.get(league_code, league_code),\n",
        "                    'Home Win %': home_rate, 'Home Wins': home_wins, 'Total Matches': total\n",
        "                })\n",
        "\n",
        "if not home_advantage_data:\n",
        "    print(\"[WARN] Could not compute any home advantage data. Skipping pandemic plot and CSV.\")\n",
        "else:\n",
        "    ha_df = pd.DataFrame(home_advantage_data)\n",
        "    try:\n",
        "        pivot_table = pd.pivot_table(ha_df, index='League', columns='Period', values='Home Win %')\n",
        "        ordered_cols = [p[0] for p in periods_for_analysis if p[0] in pivot_table.columns]\n",
        "        pivot_table = pivot_table[ordered_cols]\n",
        "\n",
        "        pre_col = periods_for_analysis[0][0]\n",
        "        pand_col = periods_for_analysis[1][0]\n",
        "        if pand_col in pivot_table.columns and pre_col in pivot_table.columns:\n",
        "             pivot_table['Pandemic Impact (pp)'] = pivot_table[pand_col] - pivot_table[pre_col]\n",
        "        else:\n",
        "             pivot_table['Pandemic Impact (pp)'] = np.nan\n",
        "\n",
        "        csv_path = os.path.join(CSV_DIR, \"pandemic_home_advantage_analysis.csv\")\n",
        "        pivot_table.round(2).to_csv(csv_path)\n",
        "        print(f\"[OK] Pandemic analysis pivot table saved to: {csv_path}\")\n",
        "        save_df_as_image(pivot_table.round(2), \"03b_pandemic_pivot_table.png\",\n",
        "                         title=\"Home Win % by League and Period\")\n",
        "    except Exception as e_pivot:\n",
        "        print(f\"[WARN] Could not create or save pandemic pivot table: {e_pivot}\")\n",
        "\n",
        "    try:\n",
        "        avg_values_dict = {}\n",
        "        for period_name, _ in periods_for_analysis:\n",
        "            avg_values_dict[period_name] = ha_df[ha_df['Period'] == period_name]['Home Win %'].mean()\n",
        "\n",
        "        avg_pre = avg_values_dict.get(periods_for_analysis[0][0], np.nan)\n",
        "        avg_pandemic = avg_values_dict.get(periods_for_analysis[1][0], np.nan)\n",
        "        avg_post = avg_values_dict.get(periods_for_analysis[2][0], np.nan)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        if 'pivot_table' in locals():\n",
        "            pivot_for_plot = pivot_table[ordered_cols]\n",
        "            plot_colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
        "            pivot_for_plot.plot(kind='bar', ax=axes[0], color=plot_colors[:len(ordered_cols)], legend=False)\n",
        "            axes[0].set_title('Home Win Rate by League & Period', fontsize=12, fontweight='bold')\n",
        "            axes[0].set_ylabel('Home Win %')\n",
        "            axes[0].tick_params(axis='x', rotation=30)  # ‚úì D√úZELTME 1\n",
        "            plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=30, ha='right')  # ‚úì D√úZELTME 2\n",
        "            axes[0].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "            min_rate = ha_df['Home Win %'].min()\n",
        "            max_rate = ha_df['Home Win %'].max()\n",
        "            axes[0].set_ylim([max(0, np.floor(min_rate / 5) * 5), min(100, np.ceil(max_rate / 5) * 5)])\n",
        "        else:\n",
        "             axes[0].text(0.5, 0.5, 'League-specific data not available\\nor pivot table failed', ha='center', va='center', transform=axes[0].transAxes)\n",
        "             axes[0].set_title('Home Win Rate by League & Period', fontsize=12, fontweight='bold')\n",
        "\n",
        "        short_periods = ['Pre-Pandemic', 'Pandemic', 'Post-Pandemic']\n",
        "        avg_values = [avg_pre, avg_pandemic, avg_post]\n",
        "        valid_indices = [i for i, v in enumerate(avg_values) if pd.notna(v)]\n",
        "        if len(valid_indices) > 1:\n",
        "            plot_x = [short_periods[i] for i in valid_indices]\n",
        "            plot_y = [avg_values[i] for i in valid_indices]\n",
        "            axes[1].plot(plot_x, plot_y, marker='o', markersize=10, linewidth=2, color='#34495e', linestyle='-')\n",
        "            for i, idx in enumerate(valid_indices):\n",
        "                axes[1].text(plot_x[i], plot_y[i] + 0.3, f'{plot_y[i]:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "            axes[1].set_title('Average Home Win Rate Trend (All Leagues)', fontsize=12, fontweight='bold')\n",
        "            axes[1].set_ylabel('Average Home Win %')\n",
        "            min_avg = min(plot_y)\n",
        "            max_avg = max(plot_y)\n",
        "            axes[1].set_ylim([max(0, np.floor(min_avg / 2) * 2 - 1), min(100, np.ceil(max_avg / 2) * 2 + 1)])\n",
        "            axes[1].grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        else:\n",
        "             axes[1].text(0.5, 0.5, 'Not enough valid data points for trend', ha='center', va='center', transform=axes[1].transAxes)\n",
        "             axes[1].set_title('Average Home Win Rate Trend', fontsize=12, fontweight='bold')\n",
        "\n",
        "        if 'pivot_table' in locals():\n",
        "            handles = [plt.Rectangle((0,0),1,1, color=plot_colors[i]) for i in range(len(ordered_cols))]\n",
        "            fig.legend(handles, ordered_cols, loc='lower center', bbox_to_anchor=(0.5, 0.01), ncol=3, fontsize=10, title=\"Period\")\n",
        "\n",
        "        plt.subplots_adjust(bottom=0.15)\n",
        "        savefig_report(\"03a_pandemic_home_advantage.png\", also_png=True)\n",
        "        print(\"[OK] Pandemic analysis plot saved.\")\n",
        "    except Exception as e_plot:\n",
        "        print(f\"[WARN] Could not create pandemic analysis plot: {e_plot}\")\n",
        "\n",
        "print(\"[OK] Phase 3: Pandemic Impact Analysis completed.\\n\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 4: EDA ON TRAIN SET ONLY\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4: EDA ON TRAIN SET ONLY\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"[INFO] Starting detailed EDA using ONLY the 'train_matches' DataFrame ({len(train_matches):,} rows).\")\n",
        "\n",
        "eda_data = train_matches.copy()\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 4.1: MARKET ODDS ANALYSIS\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.1: Analyzing Market Odds (Overround / Vig)\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "odds_cols = ['OddHome', 'OddDraw', 'OddAway']\n",
        "if all(c in eda_data.columns for c in odds_cols):\n",
        "    try:\n",
        "        eda_data['Prob_H'] = np.where(eda_data['OddHome'] > 0, 1 / eda_data['OddHome'], np.nan)\n",
        "        eda_data['Prob_D'] = np.where(eda_data['OddDraw'] > 0, 1 / eda_data['OddDraw'], np.nan)\n",
        "        eda_data['Prob_A'] = np.where(eda_data['OddAway'] > 0, 1 / eda_data['OddAway'], np.nan)\n",
        "\n",
        "        eda_data['Overround'] = eda_data['Prob_H'] + eda_data['Prob_D'] + eda_data['Prob_A']\n",
        "        avg_overround = eda_data['Overround'].mean()\n",
        "        print(f\"[OK] Calculated implied probabilities and overround (Avg: {avg_overround:.3f})\")\n",
        "\n",
        "        fig_ov, ax_ov = plt.subplots(figsize=(10, 5))\n",
        "        sns.histplot(eda_data['Overround'].dropna(), ax=ax_ov, kde=True, bins=50, color='purple')\n",
        "        ax_ov.set_title(f'Market Odds \"Overround\" Distribution \\nAvg = {avg_overround:.3f}', fontweight='bold')\n",
        "        ax_ov.set_xlabel('Overround (1/H + 1/D + 1/A)')\n",
        "        ax_ov.set_ylabel('Frequency')\n",
        "        ax_ov.axvline(1.0, color='red', linestyle='--', label='\"Fair\" Market (1.0)')\n",
        "        ax_ov.axvline(avg_overround, color='black', linestyle=':', label=f'Average ({avg_overround:.3f})')\n",
        "        ax_ov.legend()\n",
        "        ax_ov.grid(alpha=0.3)\n",
        "        savefig_report(\"04a_overround_distribution.png\", also_png=True)\n",
        "        print(\"[OK] Overround distribution plot saved.\")\n",
        "\n",
        "        if avg_overround > 1.01:\n",
        "            print(f\"[INFO] Overround > 1.01. Normalizing probabilities by removing overround...\")\n",
        "            eda_data['Prob_H_Norm'] = eda_data['Prob_H'] / eda_data['Overround']\n",
        "            eda_data['Prob_D_Norm'] = eda_data['Prob_D'] / eda_data['Overround']\n",
        "            eda_data['Prob_A_Norm'] = eda_data['Prob_A'] / eda_data['Overround']\n",
        "        else:\n",
        "            print(\"[INFO] Overround is ~1.0. Data seems pre-normalized. Copying probabilities.\")\n",
        "            eda_data['Prob_H_Norm'] = eda_data['Prob_H']\n",
        "            eda_data['Prob_D_Norm'] = eda_data['Prob_D']\n",
        "            eda_data['Prob_A_Norm'] = eda_data['Prob_A']\n",
        "        print(\"[OK] Normalized probability features (Prob_H/D/A_Norm) created.\")\n",
        "\n",
        "    except Exception as e_odds:\n",
        "        print(f\"[WARN] Market odds analysis failed: {e_odds}\")\n",
        "        try:\n",
        "            plt.close(fig_ov)\n",
        "        except:\n",
        "            pass\n",
        "else:\n",
        "    print(\"[WARN] Odds columns (OddHome, OddDraw, OddAway) not found. Skipping Market Odds Analysis.\")\n",
        "\n",
        "print(\"-\" * 90)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 4.2: IDENTIFY AND HANDLE HIGH CORRELATION PAIRS (V5.0)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.2: IDENTIFYING HIGH CORRELATION PAIRS (Multicollinearity Check)\")\n",
        "print(\"=\"*90)\n",
        "print(\"‚ö†Ô∏è  CRITICAL: Detecting features with correlation |r| > 0.85\")\n",
        "print(\"   These pairs create multicollinearity and must be addressed before modeling\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "# √ñnce sayƒ±sal s√ºtunlarƒ± tanƒ±mla\n",
        "print(\"\\n[INFO] PRE-FLIGHT VALIDATION CHECKS (on Training Data)\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "if 'Target' not in eda_data.columns or eda_data['Target'].isnull().all():\n",
        "    print(f\"[ERROR] Target column not found or is all null in the training set!\")\n",
        "    raise ValueError(\"Target column is REQUIRED for EDA analysis on training data.\")\n",
        "else:\n",
        "    target_non_null_count = eda_data['Target'].notna().sum()\n",
        "    if target_non_null_count == 0:\n",
        "         print(f\"[ERROR] Target column exists but contains only null values in the training set!\")\n",
        "         raise ValueError(\"Target column has no valid values in training data.\")\n",
        "    print(f\"[OK] Target column found and valid in training data ({target_non_null_count} non-null values).\")\n",
        "\n",
        "if len(eda_data) < 100:\n",
        "    print(f\"[WARN] Training set for EDA is very small ({len(eda_data)} rows). Results might be unstable.\")\n",
        "else:\n",
        "    print(f\"[OK] Training set size for EDA: {len(eda_data):,} rows\")\n",
        "\n",
        "# üî• KRƒ∞Tƒ∞K DEƒûƒ∞≈ûƒ∞KLƒ∞K: DATA LEAKAGE √ñZELLƒ∞KLERƒ∞Nƒ∞ √áIKAR\n",
        "numeric_cols_all_train = eda_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# LEAKAGE √ñZELLƒ∞KLERƒ∞ Lƒ∞STESƒ∞ (GENƒ∞≈ûLETƒ∞LDƒ∞)\n",
        "exclude_for_analysis = [\n",
        "    'Target', 'match_id', 'Year', 'Month', 'DoW',\n",
        "    'HomeRed', 'AwayRed',\n",
        "\n",
        "    # ===== DATA LEAKAGE (Ma√ß i√ßi veriler) =====\n",
        "    'FTHome', 'FTAway',           # Ma√ß sonu golleri\n",
        "    'HTHome', 'HTAway',           # Devre arasƒ± golleri\n",
        "    'HomeTarget', 'AwayTarget',   # Kaleyi bulan ≈üutlar (LEAKAGE!)\n",
        "    'HomeShots', 'AwayShots',     # Toplam ≈üutlar (LEAKAGE!)\n",
        "    'HomeCorners', 'AwayCorners', # Kornerler (LEAKAGE!)\n",
        "    'HomeFouls', 'AwayFouls',     # Fauller (LEAKAGE!)\n",
        "    'HomeYellow', 'AwayYellow',   # Sarƒ± kartlar (LEAKAGE!)\n",
        "\n",
        "    # ===== T√úRETƒ∞LMƒ∞≈û LEAKAGE √ñZELLƒ∞KLERƒ∞ =====\n",
        "    'ShotsDifference',            # HomeShots - AwayShots (LEAKAGE!)\n",
        "    'ShotsTotal',                 # HomeShots + AwayShots (LEAKAGE!)\n",
        "    'ShotAccuracyHome',           # HomeTarget / HomeShots (LEAKAGE!)\n",
        "    'ShotAccuracyAway',           # AwayTarget / AwayShots (LEAKAGE!)\n",
        "    'ShotAccuracyDiff',           # (LEAKAGE!)\n",
        "    'CornersDifference',          # HomeCorners - AwayCorners (LEAKAGE!)\n",
        "    'CornersTotal'                # HomeCorners + AwayCorners (LEAKAGE!)\n",
        "]\n",
        "\n",
        "numeric_cols = [c for c in numeric_cols_all_train if c not in exclude_for_analysis]\n",
        "\n",
        "if len(numeric_cols) < 5:\n",
        "    print(f\"[WARN] Only {len(numeric_cols)} numeric features identified for analysis (excluding IDs/Target/Leakage).\")\n",
        "    numeric_cols = [c for c in numeric_cols_all_train if c not in ['Target', 'match_id']]\n",
        "    print(f\"[INFO] Retrying with {len(numeric_cols)} numeric features (only excluding Target/match_id).\")\n",
        "    if len(numeric_cols) < 5:\n",
        "         print(\"[ERROR] Still too few numeric features. Check data processing.\")\n",
        "else:\n",
        "    print(f\"[OK] Identified {len(numeric_cols)} CLEAN numeric features for detailed analysis (LEAKAGE REMOVED).\")\n",
        "    print(f\"     ‚úì Excluded {len(exclude_for_analysis)} leakage/non-predictive features\")\n",
        "    print(f\"     ‚úì Included {len([f for f in temporal_features_created if f in numeric_cols])} temporal features\")\n",
        "\n",
        "print(f\"[INFO] Sample CLEAN features for analysis: {numeric_cols[:15]}...\")\n",
        "print(\"-\" * 90 + \"\\n\")\n",
        "\n",
        "# √ñzellik kaynak e≈üle≈ütirmesi\n",
        "print(\"\\n[INFO] Mapping features to their original data sources...\")\n",
        "\n",
        "source_matches_stats = [col for col in eda_data.columns if col in [\n",
        "    'OddHome', 'OddDraw', 'OddAway', 'MaxHome', 'MaxDraw', 'MaxAway',\n",
        "    'Over25', 'Under25', 'MaxOver25', 'MaxUnder25',\n",
        "    'HandiSize', 'HandiHome', 'HandiAway',\n",
        "    'C_LTH', 'C_LTA', 'C_VHD', 'C_VAD', 'C_HTB', 'C_PHB',\n",
        "]]\n",
        "\n",
        "source_matches_form = [col for col in eda_data.columns if col in [\n",
        "    'Form3Home', 'Form5Home', 'Form3Away', 'Form5Away',\n",
        "    'Form3Difference', 'Form5Difference'\n",
        "]]\n",
        "\n",
        "source_elo = [col for col in eda_data.columns if col in [\n",
        "    'HomeElo', 'AwayElo',\n",
        "    'ELO_Diff', 'EloTotal', 'EloAdvantage'\n",
        "]]\n",
        "\n",
        "source_transfermarkt = [col for col in eda_data.columns if col.startswith(('HomeTeam_', 'AwayTeam_',\n",
        "                               'ClubValue_', 'MaxPlayerValue_', 'ManagerTrophies_', 'ManagerTenureDays_',\n",
        "                               'NetTransferSpending_', 'n_players_injured_', 'ValueRatio_'))\n",
        "                        or col in ['ClubValue_Diff', 'MaxPlayerValue_Diff', 'ManagerTrophies_Diff',\n",
        "                                   'ManagerTenureDays_Diff', 'NetTransferSpending_Diff',\n",
        "                                   'n_players_injured_Diff', 'ValueRatio_Diff']]\n",
        "\n",
        "source_temporal = [col for col in eda_data.columns if any(col.startswith(f'{feat}_') or col.endswith(('_L3', '_L5', '_L10'))\n",
        "                                                          for feat in ['Home_Target', 'Away_Target', 'Target_Diff',\n",
        "                                                                      'Home_Shots', 'Away_Shots', 'Shots_Diff',\n",
        "                                                                      'Home_Corners', 'Away_Corners', 'Corners_Diff',\n",
        "                                                                      'Home_Fouls', 'Away_Fouls', 'Fouls_Diff',\n",
        "                                                                      'Home_Yellow', 'Away_Yellow', 'Yellow_Diff',\n",
        "                                                                      'ShotAccuracy'])]\n",
        "\n",
        "source_derived_other = [col for col in eda_data.columns if col in [\n",
        "    'Year', 'Month', 'DoW', 'SeasonPhase', 'IsWeekend'\n",
        "]]\n",
        "\n",
        "feature_source_map = {}\n",
        "all_mapped_features = set()\n",
        "\n",
        "def map_feature(col, source_name, feature_dict, mapped_set):\n",
        "    feature_dict[col] = source_name\n",
        "    mapped_set.add(col)\n",
        "\n",
        "feature_categories = {\n",
        "    'Matches_Stats': source_matches_stats,\n",
        "    'Matches_Form': source_matches_form,\n",
        "    'ELO_Ratings': source_elo,\n",
        "    'Transfermarkt_Data': source_transfermarkt,\n",
        "    'Temporal_Features': source_temporal,\n",
        "    'Derived_Time': source_derived_other\n",
        "}\n",
        "\n",
        "for source_name, col_list in feature_categories.items():\n",
        "    for col in col_list:\n",
        "        if col in eda_data.columns and col not in all_mapped_features:\n",
        "             map_feature(col, source_name, feature_source_map, all_mapped_features)\n",
        "\n",
        "unmapped_numeric = [col for col in numeric_cols if col not in all_mapped_features]\n",
        "if unmapped_numeric:\n",
        "    print(f\"[WARN] {len(unmapped_numeric)} numeric features could not be mapped to a source and are assigned to 'Unknown':\")\n",
        "    print(f\"       {unmapped_numeric[:10]}...\")\n",
        "    for col in unmapped_numeric:\n",
        "         feature_source_map[col] = 'Unknown_Numeric'\n",
        "         all_mapped_features.add(col)\n",
        "else:\n",
        "    print(\"[OK] All numeric features successfully mapped to a data source category.\")\n",
        "\n",
        "feature_sources = pd.Series(feature_source_map, name=\"DataSource\")\n",
        "\n",
        "print(\"\\nFeature count by Data Source (for numeric features used in EDA):\")\n",
        "source_counts = feature_sources[numeric_cols].value_counts()\n",
        "print(source_counts)\n",
        "source_counts.to_csv(os.path.join(CSV_DIR, \"feature_counts_by_source.csv\"))\n",
        "\n",
        "try:\n",
        "    fig_source, ax_source = plt.subplots(figsize=(8, 5))\n",
        "    source_counts.plot(kind='bar', ax=ax_source, color=plt.cm.Set2(np.arange(len(source_counts))), edgecolor='black')\n",
        "    ax_source.set_title('Number of Numeric Features per Data Source Category', fontweight='bold')\n",
        "    ax_source.set_ylabel('Number of Features')\n",
        "    ax_source.tick_params(axis='x', rotation=0)\n",
        "    ax_source.grid(axis='y', linestyle='--', alpha=0.9)\n",
        "    plt.tight_layout()\n",
        "    savefig_report(\"00_feature_source_distribution.png\", also_png=True)\n",
        "    print(\"[OK] Feature source distribution plotted and saved.\")\n",
        "except Exception as e_source_plot:\n",
        "     print(f\"[WARN] Could not plot feature source distribution: {e_source_plot}\")\n",
        "     plt.close(fig_source)\n",
        "\n",
        "\n",
        "# üî• ≈ûƒ∞MDƒ∞ Y√úKSEK KORELASYON ANALƒ∞Zƒ∞NE GE√áELƒ∞M\n",
        "print(\"\\n[INFO] Computing correlation matrix for multicollinearity detection...\")\n",
        "\n",
        "try:\n",
        "    correlation_matrix = eda_data[numeric_cols].corr()\n",
        "    correlation_matrix = correlation_matrix.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
        "\n",
        "    if correlation_matrix.empty:\n",
        "        print(\"[WARN] Correlation matrix is empty after dropping NaNs.\")\n",
        "    else:\n",
        "        # Y√ºksek korelasyonlu √ßiftleri bul\n",
        "        print(\"\\n[INFO] Identifying highly correlated feature pairs (|r| > 0.85)...\")\n",
        "        corr_threshold = 0.85\n",
        "        high_corr_pairs = []\n",
        "\n",
        "        for i in range(len(correlation_matrix.columns)):\n",
        "            for j in range(i+1, len(correlation_matrix.columns)):\n",
        "                corr_val = correlation_matrix.iloc[i, j]\n",
        "                if abs(corr_val) > corr_threshold:\n",
        "                    feat1 = correlation_matrix.columns[i]\n",
        "                    feat2 = correlation_matrix.columns[j]\n",
        "                    high_corr_pairs.append({\n",
        "                        'Feature_1': feat1,\n",
        "                        'Feature_2': feat2,\n",
        "                        'Correlation': corr_val,\n",
        "                        'Abs_Correlation': abs(corr_val)\n",
        "                    })\n",
        "\n",
        "        if high_corr_pairs:\n",
        "            high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Abs_Correlation', ascending=False)\n",
        "            print(f\"\\n‚ö†Ô∏è  ALERT: Found {len(high_corr_df)} highly correlated pairs (|r| > {corr_threshold}):\")\n",
        "            print(\"=\" * 90)\n",
        "            print(high_corr_df.head(20).to_string(index=False))\n",
        "            print(\"=\" * 90)\n",
        "\n",
        "            # CSV'ye kaydet\n",
        "            high_corr_csv_path = os.path.join(CSV_DIR, \"09_high_correlation_pairs.csv\")\n",
        "            high_corr_df.to_csv(high_corr_csv_path, index=False)\n",
        "            print(f\"\\n[OK] High correlation pairs saved to: {high_corr_csv_path}\")\n",
        "\n",
        "            # üéØ √ñNERƒ∞LER TABLOSU OLU≈ûTUR\n",
        "            print(\"\\n\" + \"=\"*90)\n",
        "            print(\"üìä AUTOMATED FEATURE SELECTION RECOMMENDATIONS:\")\n",
        "            print(\"=\"*90)\n",
        "\n",
        "            # üÜï V5.2: PROTECTED FEATURES (DO NOT DROP!)\n",
        "            protected_features = [\n",
        "                'ELO_Diff',       # Top 10 MI, critical for non-odds models\n",
        "                'OddHome',        # #1 MI feature\n",
        "                'OddDraw',        # High MI feature\n",
        "                'OddAway',        # High MI feature\n",
        "                'Prob_H_Norm',    # #2 MI feature\n",
        "                'Prob_D_Norm',    # High MI feature\n",
        "                'Prob_A_Norm'     # #3 MI feature\n",
        "            ]\n",
        "\n",
        "            print(\"\\nüõ°Ô∏è  PROTECTED FEATURES (Will not be dropped):\")\n",
        "            for feat in protected_features:\n",
        "                print(f\"   ‚Ä¢ {feat}\")\n",
        "            print()\n",
        "\n",
        "            # Prob_H/Prob_H_Norm gibi √ßiftlerde hangisini tutacaƒüƒ±z?\n",
        "            recommendations = []\n",
        "            features_to_drop = set()\n",
        "\n",
        "            for _, row in high_corr_df.iterrows():\n",
        "                feat1, feat2 = row['Feature_1'], row['Feature_2']\n",
        "                corr = row['Correlation']\n",
        "\n",
        "                # Recommendation mantƒ±ƒüƒ±\n",
        "                recommendation = None\n",
        "                feature_to_drop = None\n",
        "\n",
        "                # üÜï √ñNCE: Protected features kontrol√º\n",
        "                if feat1 in protected_features and feat2 in protected_features:\n",
        "                    recommendation = f\"‚ö†Ô∏è  BOTH PROTECTED: Keep both (handle in modeling)\"\n",
        "                    feature_to_drop = None\n",
        "                elif feat1 in protected_features:\n",
        "                    recommendation = f\"üõ°Ô∏è  KEEP {feat1} (protected), DROP {feat2}\"\n",
        "                    feature_to_drop = feat2\n",
        "                elif feat2 in protected_features:\n",
        "                    recommendation = f\"üõ°Ô∏è  KEEP {feat2} (protected), DROP {feat1}\"\n",
        "                    feature_to_drop = feat1\n",
        "\n",
        "                # Kural 1: Normalized versiyonlarƒ± tercih et (NON-PROTECTED)\n",
        "                elif feat1.endswith('_Norm') and not feat2.endswith('_Norm'):\n",
        "                    recommendation = f\"KEEP {feat1} (normalized), DROP {feat2}\"\n",
        "                    feature_to_drop = feat2\n",
        "                elif feat2.endswith('_Norm') and not feat1.endswith('_Norm'):\n",
        "                    recommendation = f\"KEEP {feat2} (normalized), DROP {feat1}\"\n",
        "                    feature_to_drop = feat1\n",
        "\n",
        "                # üÜï Kural 2: Max* ALWAYS drop (Odd* already protected above)\n",
        "                elif feat1.startswith('Max'):\n",
        "                    recommendation = f\"DROP {feat1} (Max odds redundant)\"\n",
        "                    feature_to_drop = feat1\n",
        "                elif feat2.startswith('Max'):\n",
        "                    recommendation = f\"DROP {feat2} (Max odds redundant)\"\n",
        "                    feature_to_drop = feat2\n",
        "\n",
        "                # Kural 3: Non-normalized Prob* drop (normalized already protected)\n",
        "                elif feat1 in ['Prob_H', 'Prob_D', 'Prob_A']:\n",
        "                    recommendation = f\"DROP {feat1} (use normalized version)\"\n",
        "                    feature_to_drop = feat1\n",
        "                elif feat2 in ['Prob_H', 'Prob_D', 'Prob_A']:\n",
        "                    recommendation = f\"DROP {feat2} (use normalized version)\"\n",
        "                    feature_to_drop = feat2\n",
        "\n",
        "                # Kural 4: League value √ßiftleri (birini tut)\n",
        "                elif 'LeagueValue' in feat1 and 'LeagueValue' in feat2:\n",
        "                    recommendation = f\"KEEP {feat1}, DROP {feat2} (redundant league value)\"\n",
        "                    feature_to_drop = feat2\n",
        "\n",
        "                # Kural 5: Prob_H ile HandiSize gibi √ßapraz korelasyonlar\n",
        "                elif 'HandiSize' in feat1 and 'Prob' in feat2:\n",
        "                    recommendation = f\"REVIEW NEEDED: {feat1} ‚Üî {feat2} (cross-domain correlation)\"\n",
        "                    feature_to_drop = None\n",
        "                elif 'HandiSize' in feat2 and 'Prob' in feat1:\n",
        "                    recommendation = f\"REVIEW NEEDED: {feat1} ‚Üî {feat2} (cross-domain correlation)\"\n",
        "                    feature_to_drop = None\n",
        "\n",
        "                # Varsayƒ±lan: MI skoru ile karar ver\n",
        "                else:\n",
        "                    recommendation = f\"CHECK MI SCORES: Compare {feat1} vs {feat2}\"\n",
        "                    feature_to_drop = None\n",
        "\n",
        "                recommendations.append({\n",
        "                    'Pair': f\"{feat1} ‚Üî {feat2}\",\n",
        "                    'Correlation': f\"{corr:.4f}\",\n",
        "                    'Recommendation': recommendation,\n",
        "                    'Feature_to_Drop': feature_to_drop if feature_to_drop else 'TBD'\n",
        "                })\n",
        "\n",
        "                if feature_to_drop:\n",
        "                    features_to_drop.add(feature_to_drop)\n",
        "\n",
        "            recommendations_df = pd.DataFrame(recommendations)\n",
        "            print(recommendations_df.head(15).to_string(index=False))\n",
        "\n",
        "            # √áƒ±karƒ±lacak √∂zelliklerin final listesi\n",
        "            features_to_drop_list = sorted(list(features_to_drop))\n",
        "\n",
        "            # üÜï V5.2: PROTECTED FEATURES FINAL CHECK\n",
        "            features_to_drop_list = [f for f in features_to_drop_list if f not in protected_features]\n",
        "\n",
        "            print(f\"\\nüéØ FINAL DROP LIST (After protected features filter):\")\n",
        "            print(f\"   Original candidates: {len(features_to_drop)} ‚Üí After filter: {len(features_to_drop_list)}\")\n",
        "            print(\"=\" * 90)\n",
        "            for i, feat in enumerate(features_to_drop_list, 1):\n",
        "                print(f\"  {i:2d}. {feat}\")\n",
        "            print(\"=\" * 90)\n",
        "\n",
        "            # üÜï REMOVED FROM DROP LIST (Protected)\n",
        "            removed_from_drop = sorted([f for f in features_to_drop if f in protected_features])\n",
        "            if removed_from_drop:\n",
        "                print(f\"\\nüõ°Ô∏è  PROTECTED (Not in drop list despite high correlation):\")\n",
        "                print(\"   These features kept due to high MI scores:\")\n",
        "                for feat in removed_from_drop:\n",
        "                    print(f\"   ‚Ä¢ {feat}\")\n",
        "                print()\n",
        "\n",
        "            # Python kodu olarak kaydet\n",
        "            drop_code_path = os.path.join(CSV_DIR, \"09b_features_to_drop_code.py\")\n",
        "            with open(drop_code_path, 'w') as f:\n",
        "                f.write(\"# AUTO-GENERATED: Features to drop due to high correlation (|r| > 0.85)\\n\")\n",
        "                f.write(\"# Generated by EDA V5.2 (with Protected Features)\\n\")\n",
        "                f.write(\"# Protected features (NOT dropped): ELO_Diff, OddHome, OddDraw, OddAway, Prob_*_Norm\\n\\n\")\n",
        "                f.write(\"features_to_drop_multicollinearity = [\\n\")\n",
        "                for feat in features_to_drop_list:\n",
        "                    f.write(f\"    '{feat}',\\n\")\n",
        "                f.write(\"]\\n\\n\")\n",
        "                f.write(\"# Usage in your modeling pipeline:\\n\")\n",
        "                f.write(\"# X_train = X_train.drop(columns=features_to_drop_multicollinearity)\\n\")\n",
        "                f.write(\"# X_val = X_val.drop(columns=features_to_drop_multicollinearity)\\n\")\n",
        "                f.write(\"# X_test = X_test.drop(columns=features_to_drop_multicollinearity)\\n\")\n",
        "\n",
        "            print(f\"[OK] Python code for dropping features saved to: {drop_code_path}\")\n",
        "            print(f\"[RECOMMENDATION] Copy this list to your modeling script and apply BEFORE training!\")\n",
        "\n",
        "            # Recommendations CSV\n",
        "            recommendations_csv_path = os.path.join(CSV_DIR, \"09c_multicollinearity_recommendations.csv\")\n",
        "            recommendations_df.to_csv(recommendations_csv_path, index=False)\n",
        "            print(f\"[OK] Detailed recommendations saved to: {recommendations_csv_path}\")\n",
        "\n",
        "            # G√∂rselle≈ütirme\n",
        "            try:\n",
        "                fig_hc, ax_hc = plt.subplots(figsize=(12, 8))\n",
        "                top_n_pairs = min(15, len(high_corr_df))\n",
        "                plot_data_hc = high_corr_df.head(top_n_pairs).copy()\n",
        "                plot_data_hc['Pair'] = plot_data_hc['Feature_1'] + ' ‚Üî ' + plot_data_hc['Feature_2']\n",
        "\n",
        "                colors_hc = ['#e74c3c' if x < 0 else '#2ecc71' for x in plot_data_hc['Correlation'].values]\n",
        "\n",
        "                ax_hc.barh(range(top_n_pairs), plot_data_hc['Correlation'].values, color=colors_hc, height=0.7, edgecolor='black')\n",
        "                ax_hc.set_yticks(range(top_n_pairs))\n",
        "                ax_hc.set_yticklabels(plot_data_hc['Pair'].values, fontsize=9)\n",
        "                ax_hc.invert_yaxis()\n",
        "                ax_hc.set_xlabel('Pearson Correlation', fontweight='bold', fontsize=12)\n",
        "                ax_hc.set_title(f'Top {top_n_pairs} Highly Correlated Feature Pairs (|r| > {corr_threshold})',\n",
        "                               fontweight='bold', fontsize=12, pad=15)\n",
        "                ax_hc.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "                ax_hc.grid(axis='x', alpha=0.3)\n",
        "\n",
        "                for i, v in enumerate(plot_data_hc['Correlation'].values):\n",
        "                    ax_hc.text(v + (0.01 if v >= 0 else -0.01), i, f'{v:.3f}',\n",
        "                              va='center', ha='left' if v >= 0 else 'right',\n",
        "                              fontsize=8, fontweight='bold')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                savefig_report(\"09e_high_correlation_pairs_bar.png\", also_png=True)\n",
        "                print(\"[OK] High correlation pairs visualization saved.\")\n",
        "            except Exception as e_hc_plot:\n",
        "                print(f\"[WARN] High correlation pairs plot failed: {e_hc_plot}\")\n",
        "                try:\n",
        "                    plt.close(fig_hc)\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ EXCELLENT: No feature pairs found with |r| > {corr_threshold}.\")\n",
        "            print(\"   Data shows good feature independence - no multicollinearity concerns!\")\n",
        "\n",
        "except Exception as e_corr_main:\n",
        "    print(f\"[ERROR] Correlation analysis failed: {e_corr_main}\")\n",
        "\n",
        "print(\"-\" * 90 + \"\\n\")\n",
        "\n",
        "\n",
        "# ===== TARGET DISTRIBUTION =====\n",
        "print(\"[INFO] Analyzing Target distribution (TRAIN SET ONLY)...\")\n",
        "if \"Target\" in eda_data.columns:\n",
        "    vc = eda_data[\"Target\"].value_counts(normalize=True).sort_index() * 100\n",
        "    vc_counts = eda_data[\"Target\"].value_counts().sort_index()\n",
        "    print(f\"[OK] Target distribution : {vc.to_dict()}\")\n",
        "    print(f\"      Counts: Home={vc_counts.get(1,0)}, Draw={vc_counts.get(0,0)}, Away={vc_counts.get(2,0)}\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "    target_labels = [\"Draw (0)\", \"Home (1)\", \"Away (2)\"]\n",
        "    target_values = [vc.get(0,0), vc.get(1,0), vc.get(2,0)]\n",
        "    target_colors=['#f39c12', '#2ecc71', '#e74c3c']\n",
        "    bars0 = axes[0].bar(target_labels, target_values, color=target_colors, alpha=0.8, edgecolor='black')\n",
        "    axes[0].set_ylabel(\"Percentage (%)\")\n",
        "    axes[0].set_title(f\"Overall Outcome Distribution (Train Set, N={len(eda_data):,})\")\n",
        "    axes[0].set_ylim([0, max(target_values) * 1.15])\n",
        "    axes[0].grid(axis='y', linestyle='--', alpha=0.9)\n",
        "    for i, v in enumerate(target_values):\n",
        "        axes[0].text(i, v + 0.5, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "    league_colors = {'E0': '#1f77b4', 'D1': '#ff7f0e', 'I1': '#2ca02c', 'SP1': '#d62728', 'F1': '#9467bd'}\n",
        "    home_win_rates = {}\n",
        "    if 'Division' in eda_data.columns:\n",
        "        for div in eda_data['Division'].unique():\n",
        "             if div in league_names:\n",
        "                 sub = eda_data[eda_data[\"Division\"]==div]\n",
        "                 if not sub.empty:\n",
        "                     vc_div = sub[\"Target\"].value_counts(normalize=True).sort_index() * 100\n",
        "                     home_win_rates[league_names[div]] = vc_div.get(1,0)\n",
        "\n",
        "        if home_win_rates:\n",
        "            sorted_leagues = sorted(home_win_rates.keys(), key=lambda k: home_win_rates[k], reverse=True)\n",
        "            sorted_rates = [home_win_rates[k] for k in sorted_leagues]\n",
        "            sorted_colors = [league_colors.get(code) for code, name in league_names.items() if name in sorted_leagues]\n",
        "\n",
        "            bars1 = axes[1].bar(sorted_leagues, sorted_rates, color=sorted_colors, alpha=0.8, edgecolor='black')\n",
        "            axes[1].set_ylabel(\"Home Win %\")\n",
        "            axes[1].set_title(\"Home Win Rate by League \")\n",
        "            min_hw = min(sorted_rates) if sorted_rates else 40\n",
        "            max_hw = max(sorted_rates) if sorted_rates else 55\n",
        "            axes[1].set_ylim([max(0, np.floor(min_hw / 2)*2 - 1), min(100, np.ceil(max_hw / 2)*2 + 1)])\n",
        "            axes[1].grid(axis='y', linestyle='--', alpha=0.9)\n",
        "            axes[1].tick_params(axis='x')\n",
        "            plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=30, ha='right')\n",
        "            for i, v in enumerate(sorted_rates):\n",
        "                 axes[1].text(i, v + 0.2, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=9)\n",
        "        else:\n",
        "             axes[1].text(0.5, 0.5, 'League data not available\\nor Target missing', ha='center', va='center', transform=axes[1].transAxes)\n",
        "             axes[1].set_title(\"Home Win Rate by League \")\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, \"'Division' column not found\", ha='center', va='center', transform=axes[1].transAxes)\n",
        "        axes[1].set_title(\"Home Win Rate by League\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    savefig_report(\"04_target_distribution.png\", also_png=True)\n",
        "    print(\"[OK] Target distribution analysis saved.\")\n",
        "else:\n",
        "    print(\"[WARN] Target column not found in eda_data, skipping target distribution analysis.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 4.2.5: DIFF-ONLY STRATEGY (MULTICOLLINEARITY FIX)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.2.5: APPLYING DIFF-ONLY STRATEGY FOR TRANSFERMARKT FEATURES\")\n",
        "print(\"=\"*90)\n",
        "print(\"üìä STRATEGY: Keep only DIFF features, drop Home/Away absolutes\")\n",
        "print(\"   Rationale: Eliminate perfect multicollinearity (VIF=313 ‚Üí VIF<10)\")\n",
        "print(\"-\"*90)\n",
        "\n",
        "# Define Transfermarkt base features\n",
        "transfermarkt_base_features = [\n",
        "    'ClubValue',\n",
        "    'MaxPlayerValue',\n",
        "    'ManagerTrophies',\n",
        "    'ManagerTenureDays',\n",
        "    'NetTransferSpending',\n",
        "    'ValueRatio',\n",
        "    'n_players_injured',\n",
        "    'n_suspended_players',\n",
        "    'LeagueValue'\n",
        "]\n",
        "\n",
        "# Automatically detect Home/Away absolutes to drop\n",
        "features_to_drop_transfermarkt = []\n",
        "features_to_keep_diff = []\n",
        "\n",
        "print(\"\\n[DETECTING] Home/Away/Diff triplets...\")\n",
        "for base_feature in transfermarkt_base_features:\n",
        "    home_col = f'HomeTeam_{base_feature}'\n",
        "    away_col = f'AwayTeam_{base_feature}'\n",
        "    diff_col = f'{base_feature}_Diff'\n",
        "\n",
        "    # Check if all three exist in the dataset\n",
        "    home_exists = home_col in eda_data.columns\n",
        "    away_exists = away_col in eda_data.columns\n",
        "    diff_exists = diff_col in eda_data.columns\n",
        "\n",
        "    if home_exists and away_exists and diff_exists:\n",
        "        # Keep Diff, drop Home and Away\n",
        "        features_to_drop_transfermarkt.append(home_col)\n",
        "        features_to_drop_transfermarkt.append(away_col)\n",
        "        features_to_keep_diff.append(diff_col)\n",
        "        print(f\"  ‚úì Found triplet: {base_feature}\")\n",
        "        print(f\"    ‚Üí DROP: {home_col}, {away_col}\")\n",
        "        print(f\"    ‚Üí KEEP: {diff_col}\")\n",
        "    elif home_exists and away_exists:\n",
        "        # Diff doesn't exist, but Home/Away do\n",
        "        print(f\"  ‚ö†Ô∏è  Warning: {base_feature} has Home/Away but NO Diff!\")\n",
        "        print(f\"    ‚Üí Keeping Home/Away (no Diff alternative)\")\n",
        "    elif diff_exists:\n",
        "        # Only Diff exists (good!)\n",
        "        features_to_keep_diff.append(diff_col)\n",
        "        print(f\"  ‚úì Found solo Diff: {diff_col}\")\n",
        "\n",
        "# Also add odds/probability redundancies\n",
        "print(\"\\n[ADDING] Odds and probability redundancies...\")\n",
        "features_to_drop_odds = [\n",
        "    'MaxHome', 'MaxDraw', 'MaxAway',  # Already covered by OddHome/Draw/Away\n",
        "    'MaxOver25', 'MaxUnder25',         # Already covered by Over25/Under25\n",
        "    'Prob_H', 'Prob_D', 'Prob_A',      # Already covered by Prob_*_Norm\n",
        "]\n",
        "features_to_drop_transfermarkt.extend(features_to_drop_odds)\n",
        "\n",
        "# Special case: HandiSize\n",
        "print(\"\\n[SPECIAL CASE] HandiSize...\")\n",
        "if 'HandiSize' in eda_data.columns:\n",
        "    print(\"  ‚Üí DROP HandiSize (r=-0.976 with Prob_H_Norm)\")\n",
        "    features_to_drop_transfermarkt.append('HandiSize')\n",
        "\n",
        "# Combine with original correlation-based drops\n",
        "print(\"\\n[COMBINING] Correlation drops + Diff-only drops...\")\n",
        "features_to_drop_list_revised = sorted(list(set(\n",
        "    features_to_drop_list +              # Original correlation drops\n",
        "    features_to_drop_transfermarkt       # Diff-only strategy drops\n",
        ")))\n",
        "\n",
        "# Remove any Diff features from drop list (protect them!)\n",
        "features_to_drop_list_revised = [\n",
        "    f for f in features_to_drop_list_revised\n",
        "    if f not in features_to_keep_diff\n",
        "]\n",
        "\n",
        "print(f\"\\n[SUMMARY] Drop List Statistics:\")\n",
        "print(f\"  ‚Ä¢ Original correlation drops: {len(features_to_drop_list)}\")\n",
        "print(f\"  ‚Ä¢ Transfermarkt absolute drops: {len(features_to_drop_transfermarkt)}\")\n",
        "print(f\"  ‚Ä¢ Total unique drops: {len(features_to_drop_list_revised)}\")\n",
        "print(f\"  ‚Ä¢ Protected Diff features: {len(features_to_keep_diff)}\")\n",
        "\n",
        "# Show what changed\n",
        "removed_from_drops = set(features_to_drop_list) - set(features_to_drop_list_revised)\n",
        "added_to_drops = set(features_to_drop_list_revised) - set(features_to_drop_list)\n",
        "\n",
        "if removed_from_drops:\n",
        "    print(f\"\\n[PROTECTED] Features removed from drop list (now kept):\")\n",
        "    for feat in sorted(removed_from_drops):\n",
        "        print(f\"  ‚úì {feat}\")\n",
        "\n",
        "if added_to_drops:\n",
        "    print(f\"\\n[ADDED] New features added to drop list:\")\n",
        "    for feat in sorted(added_to_drops):\n",
        "        print(f\"  + {feat}\")\n",
        "\n",
        "# =============================================================================\n",
        "# SAVE REVISED DROP LIST\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[SAVING] Revised drop list to file...\")\n",
        "\n",
        "drop_code_path_revised = os.path.join(CSV_DIR, \"09b_features_to_drop_code_REVISED.py\")\n",
        "\n",
        "with open(drop_code_path_revised, 'w') as f:\n",
        "    f.write(\"# \" + \"=\"*77 + \"\\n\")\n",
        "    f.write(\"# AUTO-GENERATED: Features to drop (REVISED with Diff-Only Strategy)\\n\")\n",
        "    f.write(\"# Generated by EDA V5.1 - Phase 4.2.5\\n\")\n",
        "    f.write(\"# Strategy: Keep ONLY Diff features, drop Home/Away absolutes\\n\")\n",
        "    f.write(\"# Rationale: Eliminate perfect multicollinearity (VIF=313 ‚Üí VIF<10)\\n\")\n",
        "    f.write(\"# \" + \"=\"*77 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"# Protected features (KEPT):\\n\")\n",
        "    f.write(\"# - ELO_Diff, OddHome, OddDraw, OddAway, Prob_*_Norm\\n\")\n",
        "    f.write(\"# - All *_Diff features (ClubValue_Diff, MaxPlayerValue_Diff, etc.)\\n\\n\")\n",
        "\n",
        "    f.write(\"features_to_drop_multicollinearity_REVISED = [\\n\")\n",
        "    for feat in features_to_drop_list_revised:\n",
        "        f.write(f\"    '{feat}',\\n\")\n",
        "    f.write(\"]\\n\\n\")\n",
        "\n",
        "    f.write(\"# Diff features to KEEP:\\n\")\n",
        "    f.write(\"features_to_keep_diff = [\\n\")\n",
        "    for feat in features_to_keep_diff:\n",
        "        f.write(f\"    '{feat}',\\n\")\n",
        "    f.write(\"]\\n\\n\")\n",
        "\n",
        "    f.write(\"# Usage in modeling:\\n\")\n",
        "    f.write(\"# X_train = X_train.drop(columns=features_to_drop_multicollinearity_REVISED, errors='ignore')\\n\")\n",
        "    f.write(\"# X_val = X_val.drop(columns=features_to_drop_multicollinearity_REVISED, errors='ignore')\\n\")\n",
        "    f.write(\"# X_test = X_test.drop(columns=features_to_drop_multicollinearity_REVISED, errors='ignore')\\n\")\n",
        "\n",
        "print(f\"[OK] Revised drop list saved: {os.path.basename(drop_code_path_revised)}\")\n",
        "print(f\"     File size: {os.path.getsize(drop_code_path_revised)} bytes\")\n",
        "\n",
        "# Also save original for comparison\n",
        "print(f\"[OK] Original drop list retained: {os.path.basename(drop_code_path)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"‚úÖ PHASE 4.2.5 COMPLETE - Diff-Only Strategy Applied\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 4.3: CLASS IMBALANCE ANALYSIS (DESCRIPTIVE ONLY - NO SMOTE!)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.3: CLASS IMBALANCE ANALYSIS (Descriptive Only)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "def analyze_class_imbalance_descriptive(df, target_col='Target', output_dir=None):\n",
        "    \"\"\"\n",
        "    üìä DESCRIPTIVE ANALYSIS ONLY - NO DATA MANIPULATION!\n",
        "\n",
        "    Purpose:\n",
        "        - Document class distribution in REAL training data\n",
        "        - Assess imbalance severity\n",
        "        - Provide recommendations for MODELING phase\n",
        "\n",
        "    ‚ö†Ô∏è  CRITICAL PHILOSOPHY:\n",
        "        EDA should analyze REAL data, not synthetic data!\n",
        "        SMOTE creates FAKE samples and should NEVER be used in EDA.\n",
        "\n",
        "    Why SMOTE is WRONG in EDA:\n",
        "        1. EDA Goal: Understand the TRUE data distribution\n",
        "        2. SMOTE: Creates SYNTHETIC samples (not real matches!)\n",
        "        3. Risk: EDA insights become misleading (patterns in fake data)\n",
        "        4. Correct Place: SMOTE belongs in MODELING pipeline, not EDA\n",
        "\n",
        "    Correct Workflow:\n",
        "        ‚úÖ EDA: Analyze real data ‚Üí Document imbalance ‚Üí Recommend strategies\n",
        "        ‚úÖ Modeling: Apply SMOTE (if needed) ‚Üí Train on synthetic + real data\n",
        "        ‚ùå WRONG: EDA on SMOTE data (analyzing fake patterns!)\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with Target column (REAL training data only)\n",
        "        target_col: Name of target column (default: 'Target')\n",
        "        output_dir: Directory to save plots (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict: Imbalance metrics and recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüìä CLASS IMBALANCE ANALYSIS - TRAIN SET (Real Data Only)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"‚ö†Ô∏è  IMPORTANT NOTE:\")\n",
        "    print(\"   This analysis uses REAL training data only.\")\n",
        "    print(\"   NO synthetic samples (SMOTE) are created or analyzed.\")\n",
        "    print(\"   SMOTE (if needed) will be applied later in modeling phase.\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 1: PREPARE DATA (Remove NaN only, NO SMOTE!)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[STEP 1] Preparing data for analysis...\")\n",
        "\n",
        "    # Remove NaN from Target (data quality check)\n",
        "    original_len = len(df)\n",
        "    df_clean = df[df[target_col].notna()].copy()\n",
        "    n_nan_removed = original_len - len(df_clean)\n",
        "\n",
        "    if n_nan_removed > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  Removed {n_nan_removed:,} samples with NaN in Target column\")\n",
        "        print(f\"      Original: {original_len:,} ‚Üí Clean: {len(df_clean):,}\")\n",
        "    else:\n",
        "        print(f\"  ‚úì No NaN values in Target column\")\n",
        "\n",
        "    y = df_clean[target_col].copy()\n",
        "\n",
        "    print(f\"\\n  Dataset for analysis:\")\n",
        "    print(f\"    ‚Ä¢ Total samples: {len(y):,}\")\n",
        "    print(f\"    ‚Ä¢ Data type: REAL matches (no synthetic data)\")\n",
        "    print(f\"    ‚Ä¢ Time period: Training set only (pre-2019)\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 2: CLASS DISTRIBUTION ANALYSIS\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"\\n[STEP 2] Analyzing class distribution...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    class_names = ['Draw', 'Home Win', 'Away Win']\n",
        "    y_counts = y.value_counts().sort_index()\n",
        "    y_dist = y.value_counts(normalize=True).sort_index()\n",
        "\n",
        "    print(f\"\\n  Class Distribution (Real Training Data):\")\n",
        "    for class_idx in range(3):\n",
        "        count = y_counts.get(class_idx, 0)\n",
        "        pct = y_dist.get(class_idx, 0) * 100\n",
        "        bar = '‚ñà' * int(pct / 2)  # Simple bar chart\n",
        "        print(f\"    Class {class_idx} ({class_names[class_idx]:9s}): {count:5,} ({pct:5.2f}%) {bar}\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 3: IMBALANCE SEVERITY ASSESSMENT\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"\\n[STEP 3] Assessing imbalance severity...\")\n",
        "\n",
        "    max_count = y_counts.max()\n",
        "    min_count = y_counts.min()\n",
        "    imbalance_ratio = max_count / min_count\n",
        "\n",
        "    majority_class = y_counts.idxmax()\n",
        "    minority_class = y_counts.idxmin()\n",
        "\n",
        "    print(f\"\\n  Imbalance Metrics:\")\n",
        "    print(f\"    ‚Ä¢ Majority Class: {majority_class} ({class_names[majority_class]}) = {max_count:,} samples\")\n",
        "    print(f\"    ‚Ä¢ Minority Class: {minority_class} ({class_names[minority_class]}) = {min_count:,} samples\")\n",
        "    print(f\"    ‚Ä¢ Imbalance Ratio: {imbalance_ratio:.2f}x\")\n",
        "\n",
        "    # Severity classification\n",
        "    if imbalance_ratio < 1.5:\n",
        "        severity = \"MILD\"\n",
        "        severity_emoji = \"‚úÖ\"\n",
        "        severity_desc = \"Acceptable - No intervention strictly required\"\n",
        "    elif imbalance_ratio < 3.0:\n",
        "        severity = \"MODERATE\"\n",
        "        severity_emoji = \"‚ö†Ô∏è\"\n",
        "        severity_desc = \"Consider class balancing techniques\"\n",
        "    else:\n",
        "        severity = \"SEVERE\"\n",
        "        severity_emoji = \"üö®\"\n",
        "        severity_desc = \"Strong intervention recommended\"\n",
        "\n",
        "    print(f\"\\n  {severity_emoji} Severity Assessment: {severity}\")\n",
        "    print(f\"     ‚Üí {severity_desc}\")\n",
        "\n",
        "    # Football context\n",
        "    print(f\"\\n  ‚öΩ Football Context:\")\n",
        "    print(f\"     ‚Ä¢ Home win advantage is NATURAL in football\")\n",
        "    print(f\"     ‚Ä¢ Ratio of {imbalance_ratio:.2f}x is typical for home/draw/away\")\n",
        "    print(f\"     ‚Ä¢ This reflects REAL-WORLD match outcomes\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 4: RECOMMENDATIONS FOR MODELING PHASE\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"[STEP 4] RECOMMENDATIONS FOR MODELING PHASE\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"üéØ STRATEGY 1: Class Weights (‚≠ê RECOMMENDED for Football Prediction)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"\\n  Why Recommended:\")\n",
        "    print(\"    ‚Ä¢ No synthetic data created (preserves true distribution)\")\n",
        "    print(\"    ‚Ä¢ Works with all algorithms (tree-based, linear models)\")\n",
        "    print(\"    ‚Ä¢ Interpretable (real matches, real patterns)\")\n",
        "    print(\"    ‚Ä¢ Faster training (no data augmentation)\")\n",
        "\n",
        "    print(\"\\n  How to Implement:\")\n",
        "    print(\"    # For scikit-learn models:\")\n",
        "    print(\"    from sklearn.linear_model import LogisticRegression\")\n",
        "    print(\"    model = LogisticRegression(class_weight='balanced')\")\n",
        "    print(\"\")\n",
        "    print(\"    # For XGBoost (recommended for football):\")\n",
        "    print(\"    import xgboost as xgb\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    total_samples = len(y)\n",
        "    n_classes = len(y_counts)\n",
        "    class_weights = {}\n",
        "    for class_idx in range(n_classes):\n",
        "        weight = total_samples / (n_classes * y_counts.get(class_idx, 1))\n",
        "        class_weights[class_idx] = weight\n",
        "\n",
        "    print(f\"    class_weights = {{\")\n",
        "    for class_idx, weight in class_weights.items():\n",
        "        print(f\"        {class_idx}: {weight:.3f},  # {class_names[class_idx]}\")\n",
        "    print(f\"    }}\")\n",
        "    print(f\"    model = xgb.XGBClassifier(scale_pos_weight=class_weights)\")\n",
        "\n",
        "    print(\"\\n  ‚úÖ Advantages:\")\n",
        "    print(\"    ‚Ä¢ Simple to implement\")\n",
        "    print(\"    ‚Ä¢ No additional computational cost\")\n",
        "    print(\"    ‚Ä¢ Preserves natural data distribution\")\n",
        "    print(\"    ‚Ä¢ Works well with tree-based models\")\n",
        "\n",
        "    if imbalance_ratio >= 1.5:\n",
        "        print(\"\\n\\nüéØ STRATEGY 2: SMOTE (‚ö†Ô∏è Alternative - Use with Caution)\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        print(\"\\n  ‚ö†Ô∏è  CRITICAL WARNINGS:\")\n",
        "        print(\"    ‚Ä¢ SMOTE creates SYNTHETIC samples (not real matches!)\")\n",
        "        print(\"    ‚Ä¢ Apply ONLY in modeling pipeline, NEVER in EDA\")\n",
        "        print(\"    ‚Ä¢ Only for TRAINING set (never validation/test)\")\n",
        "        print(\"    ‚Ä¢ May create unrealistic match scenarios\")\n",
        "\n",
        "        print(\"\\n  When to Consider SMOTE:\")\n",
        "        print(\"    ‚Ä¢ Class weights alone are insufficient\")\n",
        "        print(\"    ‚Ä¢ Very severe imbalance (>5x)\")\n",
        "        print(\"    ‚Ä¢ Using algorithms sensitive to class balance\")\n",
        "\n",
        "        print(\"\\n  Correct Implementation:\")\n",
        "        print(\"    from imblearn.over_sampling import SMOTE\")\n",
        "        print(\"    from sklearn.model_selection import train_test_split\")\n",
        "        print(\"\")\n",
        "        print(\"    # Step 1: Split data FIRST (before any SMOTE!)\")\n",
        "        print(\"    X_train, X_val, y_train, y_val = train_test_split(X, y, ...\")\n",
        "        print(\"    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, ...)\")\n",
        "        print(\"\")\n",
        "        print(\"    # Step 2: Apply SMOTE ONLY to training set\")\n",
        "        print(\"    smote = SMOTE(sampling_strategy='not majority', random_state=42)\")\n",
        "        print(\"    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\")\n",
        "        print(\"\")\n",
        "        print(\"    # Step 3: Train on balanced data\")\n",
        "        print(\"    model.fit(X_train_balanced, y_train_balanced)\")\n",
        "        print(\"\")\n",
        "        print(\"    # Step 4: Evaluate on REAL validation/test data (no SMOTE!)\")\n",
        "        print(\"    y_val_pred = model.predict(X_val)  # Original val set\")\n",
        "        print(\"    y_test_pred = model.predict(X_test)  # Original test set\")\n",
        "\n",
        "        print(\"\\n  ‚ùå WRONG Implementation (DO NOT DO THIS!):\")\n",
        "        print(\"    # ‚ùå Applying SMOTE before split (DATA LEAKAGE!)\")\n",
        "        print(\"    X_balanced, y_balanced = smote.fit_resample(X, y)\")\n",
        "        print(\"    X_train, X_test, ... = train_test_split(X_balanced, y_balanced)\")\n",
        "        print(\"\")\n",
        "        print(\"    # ‚ùå Applying SMOTE to validation/test (CONTAMINATION!)\")\n",
        "        print(\"    X_val_balanced, y_val_balanced = smote.fit_resample(X_val, y_val)\")\n",
        "\n",
        "        print(\"\\n  Expected SMOTE Results (Simulation):\")\n",
        "        print(f\"    Before SMOTE: {dict(y_counts)}\")\n",
        "        print(f\"    After SMOTE:  Draw={max_count}, Home Win={max_count}, Away Win={max_count}\")\n",
        "        print(f\"    ‚Üí Perfect balance (1:1:1 ratio)\")\n",
        "\n",
        "    print(\"\\n\\nüéØ STRATEGY 3: Stratified Sampling (‚úÖ ALWAYS Recommended)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\n  Purpose:\")\n",
        "    print(\"    ‚Ä¢ Ensure proportional class distribution in train/val/test\")\n",
        "    print(\"    ‚Ä¢ Prevent accidentally creating imbalanced splits\")\n",
        "\n",
        "    print(\"\\n  Implementation:\")\n",
        "    print(\"    from sklearn.model_selection import train_test_split\")\n",
        "    print(\"\")\n",
        "    print(\"    # Use stratify parameter\")\n",
        "    print(\"    X_train, X_temp, y_train, y_temp = train_test_split(\")\n",
        "    print(\"        X, y, test_size=0.3, stratify=y, random_state=42\")\n",
        "    print(\"    )\")\n",
        "    print(\"\")\n",
        "    print(\"    X_val, X_test, y_val, y_test = train_test_split(\")\n",
        "    print(\"        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\")\n",
        "    print(\"    )\")\n",
        "\n",
        "    print(\"\\n  ‚úÖ Benefits:\")\n",
        "    print(\"    ‚Ä¢ Maintains class proportions across all sets\")\n",
        "    print(\"    ‚Ä¢ More reliable model evaluation\")\n",
        "    print(\"    ‚Ä¢ Essential for time-series cross-validation\")\n",
        "\n",
        "    print(\"\\n\\nüéØ STRATEGY 4: Evaluation Metrics (‚úÖ CRITICAL)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\n  Why Important:\")\n",
        "    print(\"    ‚Ä¢ Accuracy is MISLEADING with imbalanced data\")\n",
        "    print(f\"    ‚Ä¢ Example: Predicting all 'Home Win' gives {y_dist.max()*100:.1f}% accuracy!\")\n",
        "\n",
        "    print(\"\\n  Recommended Metrics for Football Prediction:\")\n",
        "    print(\"    1. F1-Score (macro/weighted)  ‚Üê Primary metric\")\n",
        "    print(\"    2. Precision & Recall (per class)\")\n",
        "    print(\"    3. Cohen's Kappa (agreement measure)\")\n",
        "    print(\"    4. Confusion Matrix (detailed view)\")\n",
        "    print(\"    5. ROC-AUC (one-vs-rest)\")\n",
        "\n",
        "    print(\"\\n  Implementation:\")\n",
        "    print(\"    from sklearn.metrics import classification_report, cohen_kappa_score\")\n",
        "    print(\"\")\n",
        "    print(\"    # Comprehensive evaluation\")\n",
        "    print(\"    print(classification_report(y_true, y_pred, target_names=class_names))\")\n",
        "    print(\"    print(f'Kappa Score: {cohen_kappa_score(y_true, y_pred):.3f}')\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 5: VISUALIZATION (Single Plot - Distribution Only)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    if output_dir:\n",
        "        print(\"[STEP 5] Creating visualization...\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        colors = ['#f39c12', '#2ecc71', '#e74c3c']\n",
        "\n",
        "        # PLOT 1: Bar Chart with Annotations\n",
        "        ax = axes[0]\n",
        "        bars = ax.bar(class_names, y_counts.values, color=colors,\n",
        "                     alpha=0.85, edgecolor='black', linewidth=2)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, bar in enumerate(bars):\n",
        "            height = bar.get_height()\n",
        "            pct = y_dist.iloc[i] * 100\n",
        "\n",
        "            # Count label (on top)\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + max(y_counts)*0.02,\n",
        "                   f'{int(height):,}',\n",
        "                   ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # Percentage label (inside bar)\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height/2,\n",
        "                   f'{pct:.1f}%',\n",
        "                   ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                   color='white' if pct > 15 else 'black')\n",
        "\n",
        "        ax.set_ylabel('Number of Matches', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Match Outcome', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'Class Distribution \\n' +\n",
        "                    f'Imbalance Ratio: {imbalance_ratio:.2f}x ({severity})',\n",
        "                    fontsize=13, fontweight='bold', pad=15)\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "        ax.set_ylim([0, max(y_counts.values) * 1.15])\n",
        "\n",
        "        # PLOT 2: Pie Chart with Better Labels\n",
        "        ax = axes[1]\n",
        "\n",
        "        # Explode minority classes slightly\n",
        "        explode_values = []\n",
        "        for idx in range(len(y_counts)):\n",
        "            if idx == minority_class:\n",
        "                explode_values.append(0.1)\n",
        "            elif idx == majority_class:\n",
        "                explode_values.append(0)\n",
        "            else:\n",
        "                explode_values.append(0.05)\n",
        "\n",
        "        wedges, texts, autotexts = ax.pie(\n",
        "            y_counts.values,\n",
        "            labels=class_names,\n",
        "            colors=colors,\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=90,\n",
        "            explode=explode_values,\n",
        "            textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
        "            shadow=True\n",
        "        )\n",
        "\n",
        "        # Enhance autotext\n",
        "        for autotext in autotexts:\n",
        "            autotext.set_color('white')\n",
        "            autotext.set_fontsize(12)\n",
        "\n",
        "        ax.set_title(f'Class Proportions \\n' +\n",
        "                    f'Majority: {class_names[majority_class]} ({y_dist.max()*100:.1f}%)',\n",
        "                    fontsize=13, fontweight='bold', pad=15)\n",
        "\n",
        "        # Add legend with counts\n",
        "        legend_labels = [f'{name}: {count:,}' for name, count in zip(class_names, y_counts.values)]\n",
        "        ax.legend(legend_labels, loc='upper left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10)\n",
        "\n",
        "        # Main title\n",
        "        plt.suptitle('‚öΩ Class Imbalance Analysis - REAL Training Data Only (NO SMOTE)\\n' +\n",
        "                    'Purpose: Document natural distribution for modeling decisions',\n",
        "                    fontsize=14, fontweight='bold', y=1.00)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "        # Save plot\n",
        "        save_path = os.path.join(output_dir, '04c_class_imbalance_analysis.png')\n",
        "        plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "        savefig_report('04c_class_imbalance_analysis.png', also_png=True)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"  ‚úÖ Visualization saved: {os.path.basename(save_path)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 6: RETURN METRICS\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    return {\n",
        "        'class_counts': y_counts.to_dict(),\n",
        "        'class_distribution': y_dist.to_dict(),\n",
        "        'imbalance_ratio': imbalance_ratio,\n",
        "        'majority_class': majority_class,\n",
        "        'minority_class': minority_class,\n",
        "        'severity': severity,\n",
        "        'class_weights': class_weights,\n",
        "        'recommendations': {\n",
        "            'primary': 'Use class_weight=\"balanced\" (tree-based models)',\n",
        "            'alternative': 'SMOTE (only if class weights insufficient)',\n",
        "            'critical': 'NEVER apply SMOTE in EDA or on validation/test sets',\n",
        "            'evaluation': 'Use F1-score, Kappa, not accuracy'\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# EXECUTE ANALYSIS (DESCRIPTIVE ONLY - NO SMOTE!)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "if 'Target' in eda_data.columns:\n",
        "    print(\"[INFO] Executing class imbalance analysis...\")\n",
        "\n",
        "    imbalance_metrics = analyze_class_imbalance_descriptive(\n",
        "        df=eda_data,\n",
        "        target_col='Target',\n",
        "        output_dir=IMAGES_DIR\n",
        "    )\n",
        "\n",
        "    if imbalance_metrics:\n",
        "        print(\"\\n[OK] Class imbalance analysis complete (descriptive only).\")\n",
        "        print(f\"     ‚Üí Recommendations saved in analysis output\")\n",
        "        print(f\"     ‚Üí Apply strategies in MODELING phase, not in EDA\")\n",
        "    else:\n",
        "        print(\"\\n[WARN] Class imbalance analysis returned no metrics.\")\n",
        "else:\n",
        "    print(\"[WARN] Skipping class imbalance analysis (Target column missing).\")\n",
        "    imbalance_metrics = None\n",
        "\n",
        "\n",
        "# ===== CATEGORICAL FEATURES DISTRIBUTION =====\n",
        "print(\"\\n[INFO] Analyzing Categorical Feature Distributions (Frequency Plots)...\")\n",
        "\n",
        "try:\n",
        "    temp_cat_features_all = eda_data.select_dtypes(include=['category', 'object']).columns.tolist()\n",
        "    temp_exclude_chi = ['HomeTeam', 'AwayTeam', 'MatchDate', 'YearMonth', 'Season',\n",
        "                        'FTResult', 'Target', 'HTResult', 'Referee',\n",
        "                        'HomeTeam_ManagerName', 'AwayTeam_ManagerName']\n",
        "    cat_features_to_plot = [f for f in temp_cat_features_all if f not in temp_exclude_chi and f in eda_data.columns and eda_data[f].nunique() < 50]\n",
        "\n",
        "    if not cat_features_to_plot:\n",
        "        print(\"[WARN] No suitable low-cardinality categorical features found for plotting.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Plotting frequency for: {cat_features_to_plot}\")\n",
        "        num_cat_plots = len(cat_features_to_plot)\n",
        "        cols_cat = 3\n",
        "        rows_cat = (num_cat_plots + cols_cat - 1) // cols_cat\n",
        "\n",
        "        fig_cat, axes_cat = plt.subplots(rows_cat, cols_cat, figsize=(6 * cols_cat, 5 * rows_cat), squeeze=False)\n",
        "        axes_cat = axes_cat.flatten()\n",
        "        plot_count_cat = 0\n",
        "\n",
        "        for idx, feature in enumerate(cat_features_to_plot):\n",
        "            ax = axes_cat[idx]\n",
        "            try:\n",
        "                plot_data = eda_data[feature].astype(str).fillna('Missing')\n",
        "                sns.countplot(y=plot_data, ax=ax, palette=\"Set2\",\n",
        "                              order = plot_data.value_counts().index[:15])\n",
        "                ax.set_title(f\"Frequency of '{feature}' (Top 15, Train Set)\", fontsize=10, fontweight='bold')\n",
        "                ax.set_xlabel(\"Count\")\n",
        "                ax.set_ylabel(feature)\n",
        "                ax.tick_params(axis='y', labelsize=8)\n",
        "                ax.grid(axis='x', linestyle='--', alpha=0.9)\n",
        "                plot_count_cat += 1\n",
        "            except Exception as e_cat_plot:\n",
        "                ax.text(0.5, 0.5, f\"Plot failed for {feature}:\\n{str(e_cat_plot)[:100]}\", ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "\n",
        "        for i in range(plot_count_cat, len(axes_cat)):\n",
        "            axes_cat[i].axis('off')\n",
        "\n",
        "        if plot_count_cat > 0:\n",
        "            plt.tight_layout(pad=2.0)\n",
        "            savefig_report(\"04b_categorical_distributions.png\", also_png=True)\n",
        "            print(f\"[OK] Plotted {plot_count_cat} categorical feature distributions.\")\n",
        "        else:\n",
        "            plt.close(fig_cat)\n",
        "\n",
        "except Exception as e_cat_main:\n",
        "    print(f\"[ERROR] Categorical distribution plotting failed: {e_cat_main}\")\n",
        "    try:\n",
        "        plt.close(fig_cat)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# ===== NUMERICAL FEATURES DISTRIBUTION (Home vs Away) =====\n",
        "print(\"\\n[INFO] Analyzing Numerical Feature Distributions (Home vs Away)...\")\n",
        "\n",
        "home_away_pairs = [\n",
        "    ('HomeElo', 'AwayElo', 'ELO Rating'),\n",
        "    ('HomeTeam_ClubValue', 'AwayTeam_ClubValue', 'Club Value'),\n",
        "    ('HomeTeam_MaxPlayerValue', 'AwayTeam_MaxPlayerValue', 'Max Player Value'),\n",
        "    ('HomeTeam_ManagerTrophies', 'AwayTeam_ManagerTrophies', 'Manager Trophies'),\n",
        "    ('HomeTeam_NetTransferSpending', 'AwayTeam_NetTransferSpending', 'Net Transfer Spend'),\n",
        "    ('HomeTeam_n_players_injured', 'AwayTeam_n_players_injured', 'Injured Players'),\n",
        "    # üÜï Temporal Features\n",
        "    ('Home_Target_Avg_L5', 'Away_Target_Avg_L5', 'Shots on Target (L5)'),\n",
        "    ('Home_Shots_Avg_L5', 'Away_Shots_Avg_L5', 'Total Shots (L5)'),\n",
        "]\n",
        "\n",
        "num_pairs = len(home_away_pairs)\n",
        "cols_dist = 3\n",
        "rows_dist = (num_pairs + cols_dist - 1) // cols_dist\n",
        "fig_dist, axes_dist = plt.subplots(rows_dist, cols_dist, figsize=(5 * cols_dist, 4 * rows_dist))\n",
        "axes_dist = axes_dist.flatten()\n",
        "plot_count = 0\n",
        "\n",
        "for idx, (home_feat, away_feat, label) in enumerate(home_away_pairs):\n",
        "    ax = axes_dist[idx]\n",
        "    if home_feat in eda_data.columns and away_feat in eda_data.columns:\n",
        "        home_data = eda_data[home_feat].dropna()\n",
        "        away_data = eda_data[away_feat].dropna()\n",
        "\n",
        "        if len(home_data) > 0 or len(away_data) > 0:\n",
        "            sns.histplot(home_data, ax=ax, color=UNIFIED_COLORS['main_blue'], label=f'Home ({home_data.mean():.1f} avg)', kde=True, bins=25, alpha=0.9)\n",
        "            sns.histplot(away_data, ax=ax, color='#e74c3c', label=f'Away ({away_data.mean():.1f} avg)', kde=True, bins=25, alpha=0.9)\n",
        "            ax.set_title(f'{label}\\nDistribution ', fontsize=10, fontweight='bold')\n",
        "            ax.set_xlabel(label)\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.legend(fontsize=8)\n",
        "            ax.grid(alpha=0.3)\n",
        "            plot_count += 1\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{label}\\nDistribution ', fontsize=10, fontweight='bold')\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, f'Features Missing:\\n{home_feat}\\nor\\n{away_feat}', ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "        ax.set_title(f'{label}\\nDistribution ', fontsize=10, fontweight='bold')\n",
        "    ax.tick_params(axis='x', labelsize=8)\n",
        "    ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "for i in range(idx + 1, len(axes_dist)):\n",
        "    axes_dist[i].axis('off')\n",
        "\n",
        "if plot_count > 0:\n",
        "    plt.tight_layout()\n",
        "    savefig_report(\"05_numerical_distributions_home_away.png\", also_png=True)\n",
        "    print(f\"[OK] Plotted {plot_count} Home vs Away distributions.\")\n",
        "else:\n",
        "    print(\"[WARN] No valid Home/Away pairs found to plot distributions.\")\n",
        "    plt.close(fig_dist)\n",
        "\n",
        "\n",
        "# ===== DIFFERENCE FEATURES DISTRIBUTION =====\n",
        "print(\"\\n[INFO] Analyzing Difference Feature Distributions...\")\n",
        "\n",
        "diff_features = [\n",
        "    'ELO_Diff', 'ClubValue_Diff', 'MaxPlayerValue_Diff', 'ManagerTrophies_Diff',\n",
        "    'ManagerTenureDays_Diff', 'NetTransferSpending_Diff', 'n_players_injured_Diff',\n",
        "    'ValueRatio_Diff',\n",
        "    # üÜï Temporal Diffs\n",
        "    'Target_Diff_L5', 'Shots_Diff_L5', 'Corners_Diff_L5', 'ShotAccuracy_Diff_L5'\n",
        "]\n",
        "\n",
        "available_diff_features = [f for f in diff_features if f in eda_data.columns]\n",
        "\n",
        "if not available_diff_features:\n",
        "    print(\"[WARN] No difference features (_Diff) found in eda_data. Skipping this analysis.\")\n",
        "else:\n",
        "    num_diff = len(available_diff_features)\n",
        "    cols_diff = 3\n",
        "    rows_diff = (num_diff + cols_diff - 1) // cols_diff\n",
        "    fig_diff, axes_diff = plt.subplots(rows_diff, cols_diff, figsize=(5 * cols_diff, 4 * rows_diff))\n",
        "    axes_diff = axes_diff.flatten()\n",
        "    plot_count_diff = 0\n",
        "\n",
        "    for idx, feature in enumerate(available_diff_features):\n",
        "        ax = axes_diff[idx]\n",
        "        data = eda_data[feature].dropna()\n",
        "\n",
        "        if len(data) > 0:\n",
        "            sns.histplot(data, ax=ax, color='#9b59b6', kde=True, bins=30, alpha=0.7)\n",
        "            mean_val = data.mean()\n",
        "            median_val = data.median()\n",
        "            ax.axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
        "            ax.axvline(median_val, color='blue', linestyle=':', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
        "\n",
        "            ax.set_title(f'{feature}\\nDistribution ', fontsize=10, fontweight='bold')\n",
        "            ax.set_xlabel(feature)\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.legend(fontsize=8)\n",
        "            ax.grid(alpha=0.3)\n",
        "            plot_count_diff += 1\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{feature}\\nDistribution ', fontsize=10, fontweight='bold')\n",
        "        ax.tick_params(axis='x', labelsize=8)\n",
        "        ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "    for i in range(idx + 1, len(axes_diff)):\n",
        "        axes_diff[i].axis('off')\n",
        "\n",
        "    if plot_count_diff > 0:\n",
        "        plt.tight_layout()\n",
        "        savefig_report(\"06_difference_features_distribution.png\", also_png=True)\n",
        "        print(f\"[OK] Plotted {plot_count_diff} Difference feature distributions.\")\n",
        "    else:\n",
        "        plt.close(fig_diff)\n",
        "\n",
        "\n",
        "# ===== DRAW ANALYSIS (VIOLINPLOTS) =====\n",
        "print(\"\\n[INFO] Analyzing Difference Features vs. Target (Draw Analysis)...\")\n",
        "\n",
        "if 'available_diff_features' in locals() and available_diff_features:\n",
        "    features_for_boxplot = available_diff_features[:9]\n",
        "    num_box = len(features_for_boxplot)\n",
        "    cols_box = 3\n",
        "    rows_box = (num_box + cols_box - 1) // cols_box\n",
        "    fig_box_draw, axes_box_draw = plt.subplots(rows_box, cols_box,\n",
        "                                                figsize=(6 * cols_box, 5 * rows_box),\n",
        "                                                squeeze=False)\n",
        "    axes_box_draw = axes_box_draw.flatten()\n",
        "    plot_count_box_draw = 0\n",
        "\n",
        "    # ‚úÖ V5.2 FIX: ROBUST PALETTE SETUP WITH STRING KEYS\n",
        "    print(\"  [INFO] Preparing palette and target encoding...\")\n",
        "\n",
        "    # üÜï CRITICAL FIX: Clean NaN values BEFORE casting to int\n",
        "    original_len = len(eda_data)\n",
        "    n_nan_target = eda_data['Target'].isna().sum()\n",
        "\n",
        "    if n_nan_target > 0:\n",
        "        print(f\"      ‚ö†Ô∏è  Found {n_nan_target} NaN values in Target\")\n",
        "        print(f\"         ‚Üí Removing for violin plot analysis...\")\n",
        "        eda_data = eda_data[eda_data['Target'].notna()].copy()\n",
        "        print(f\"         ‚úì Cleaned: {original_len:,} ‚Üí {len(eda_data):,} samples\")\n",
        "\n",
        "    # Now safe to cast to int\n",
        "    eda_data['Target'] = eda_data['Target'].astype(int)\n",
        "    print(f\"      ‚úì Target converted to integer type\")\n",
        "\n",
        "    # Get unique target values from data (handles missing classes)\n",
        "    unique_targets = sorted(eda_data['Target'].dropna().unique())\n",
        "\n",
        "    # Define base palette for reference (integer keys)\n",
        "    palette_colors_base = {0: '#f39c12', 1: '#2ecc71', 2: '#e74c3c'}\n",
        "\n",
        "    # ‚úÖ FIX: Create palette with STRING keys for seaborn compatibility\n",
        "    palette_draw = {str(k): palette_colors_base[k]\n",
        "                    for k in unique_targets\n",
        "                    if k in palette_colors_base}\n",
        "    # Result: palette_draw = {'0': '#f39c12', '1': '#2ecc71', '2': '#e74c3c'}\n",
        "\n",
        "    # ‚úÖ Also create color list for consistent legend ordering\n",
        "    colors_list = [palette_colors_base[k] for k in unique_targets]\n",
        "\n",
        "    print(f\"      ‚úì Target classes present: {unique_targets}\")\n",
        "    print(f\"      ‚úì Palette configured for: {list(palette_draw.keys())} (string keys)\")\n",
        "\n",
        "    for idx, feature in enumerate(features_for_boxplot):\n",
        "        ax = axes_box_draw[idx]\n",
        "\n",
        "        try:\n",
        "            # ‚úÖ Additional safety: check if feature has variance\n",
        "            if eda_data[feature].std() < 1e-6:\n",
        "                print(f\"  [WARN] {feature}: No variance, skipping violin plot\")\n",
        "                ax.text(0.5, 0.5, f'{feature}\\n(No variance)',\n",
        "                       ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
        "                ax.set_title(f'{feature}\\n(Skipped)', fontsize=10, fontweight='bold')\n",
        "                continue\n",
        "\n",
        "            # Create violin plot with filtered data\n",
        "            plot_data = eda_data[[feature, 'Target']].dropna()\n",
        "\n",
        "            if len(plot_data) < 10:\n",
        "                print(f\"  [WARN] {feature}: Insufficient data ({len(plot_data)} samples)\")\n",
        "                ax.text(0.5, 0.5, f'{feature}\\n(Insufficient data)',\n",
        "                       ha='center', va='center', transform=ax.transAxes, fontsize=9)\n",
        "                ax.set_title(f'{feature}\\n(Skipped)', fontsize=10, fontweight='bold')\n",
        "                continue\n",
        "\n",
        "            # ‚úÖ FIX: Convert Target to string for seaborn compatibility\n",
        "            plot_data['Target_str'] = plot_data['Target'].astype(str)\n",
        "\n",
        "            # ‚úÖ NOW THIS WORKS: String Target column matches string palette keys\n",
        "            sns.violinplot(\n",
        "                data=plot_data,\n",
        "                x='Target_str',        # ‚úÖ String column: '0', '1', '2'\n",
        "                y=feature,\n",
        "                ax=ax,\n",
        "                palette=palette_draw,  # ‚úÖ String keys: {'0': ..., '1': ..., '2': ...}\n",
        "                inner='quartile',\n",
        "                cut=0\n",
        "            )\n",
        "\n",
        "            ax.set_title(f'{feature}\\nby Match Outcome ',\n",
        "                        fontsize=10, fontweight='bold')\n",
        "            ax.set_xlabel('Outcome', fontweight='bold')\n",
        "\n",
        "            # ‚úÖ Custom x-tick labels with class names\n",
        "            class_names = ['Draw', 'Home Win', 'Away Win']\n",
        "            tick_labels = [f\"{class_names[t]} ({t})\" for t in unique_targets]\n",
        "            ax.set_xticklabels(tick_labels, fontsize=9)\n",
        "\n",
        "            ax.set_ylabel(feature, fontweight='bold')\n",
        "            ax.axhline(0, color='black', linestyle='--', linewidth=1.5, alpha=0.7,\n",
        "                      label='Zero Line')\n",
        "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "            plot_count_box_draw += 1\n",
        "\n",
        "            # ‚úÖ Hypothesis check for ELO_Diff\n",
        "            if feature == 'ELO_Diff' and 0 in unique_targets:\n",
        "                median_draw = plot_data[plot_data['Target']==0][feature].median()\n",
        "                print(f\"  [HYPOTHESIS CHECK] Median ELO_Diff for Draws: {median_draw:.2f} (Expected ~0)\")\n",
        "\n",
        "                # ‚úÖ Add annotation to plot\n",
        "                if pd.notna(median_draw):\n",
        "                    ax.text(0.02, 0.98, f'Draw Median: {median_draw:.2f}',\n",
        "                           transform=ax.transAxes, fontsize=8,\n",
        "                           verticalalignment='top',\n",
        "                           bbox=dict(boxstyle='round,pad=0.5',\n",
        "                                   facecolor='yellow', alpha=0.7))\n",
        "\n",
        "        except Exception as e_violin:\n",
        "            print(f\"  [ERROR] Violin plot failed for {feature}: {e_violin}\")\n",
        "            ax.text(0.5, 0.5, f\"Plot failed:\\n{feature}\\n{str(e_violin)[:50]}\",\n",
        "                   ha='center', va='center', transform=ax.transAxes, fontsize=8,\n",
        "                   bbox=dict(boxstyle='round,pad=0.5', facecolor='mistyrose', alpha=0.9))\n",
        "            ax.set_title(f'{feature}\\n(Error)', fontsize=10, fontweight='bold')\n",
        "\n",
        "    for i in range(plot_count_box_draw, len(axes_box_draw)):\n",
        "        axes_box_draw[i].axis('off')\n",
        "\n",
        "# ===== NUMERICAL SCATTER PLOTS =====\n",
        "print(\"\\n[INFO] Analyzing Numerical Scatter Plots vs Target (TRAIN SET ONLY)...\")\n",
        "\n",
        "scatter_pairs = [\n",
        "    ('ELO_Diff', 'ClubValue_Diff', 'ELO vs Club Value'),\n",
        "    ('Form5Difference', 'ManagerTrophies_Diff', 'Form (5) vs Manager Trophies'),\n",
        "    ('ValueRatio_Diff', 'NetTransferSpending_Diff', 'Value Ratio vs Net Transfer Spend'),\n",
        "    # üÜï Temporal Features\n",
        "    ('Target_Diff_L5', 'Shots_Diff_L5', 'Target L5 vs Shots L5')\n",
        "]\n",
        "\n",
        "n_samples_scatter = min(5000, len(eda_data))\n",
        "eda_data_sample_scatter = eda_data.sample(n_samples_scatter, random_state=RNG_SEED).copy()\n",
        "\n",
        "target_map_scatter = {0: 'Draw', 1: 'Home', 2: 'Away'}\n",
        "colors_scatter = {0: '#f39c12', 1: '#2ecc71', 2: '#e74c3c'}\n",
        "\n",
        "valid_scatter_pairs = []\n",
        "for x_feat, y_feat, label in scatter_pairs:\n",
        "    if x_feat in eda_data_sample_scatter.columns and y_feat in eda_data_sample_scatter.columns:\n",
        "        valid_scatter_pairs.append((x_feat, y_feat, label))\n",
        "    else:\n",
        "        print(f\"[WARN] Scatter plot pair skipped (missing): {x_feat} or {y_feat}\")\n",
        "\n",
        "num_pairs_sc = len(valid_scatter_pairs)\n",
        "\n",
        "if num_pairs_sc > 0:\n",
        "    fig_sc, axes_sc = plt.subplots(1, num_pairs_sc, figsize=(7 * num_pairs_sc, 6))\n",
        "    if num_pairs_sc == 1:\n",
        "        axes_sc = [axes_sc]\n",
        "    plot_count_sc = 0\n",
        "\n",
        "    for idx, (x_feat, y_feat, label) in enumerate(valid_scatter_pairs):\n",
        "        ax = axes_sc[idx]\n",
        "        try:\n",
        "            sns.scatterplot(\n",
        "                data=eda_data_sample_scatter,\n",
        "                x=x_feat,\n",
        "                y=y_feat,\n",
        "                hue='Target',\n",
        "                palette=colors_scatter,\n",
        "                ax=ax,\n",
        "                alpha=0.9,\n",
        "                s=30,\n",
        "                legend='full'\n",
        "            )\n",
        "            ax.set_title(f'{label}\\n(Sampled, N={n_samples_scatter})', fontsize=10, fontweight='bold')\n",
        "            ax.set_xlabel(x_feat)\n",
        "            ax.set_ylabel(y_feat)\n",
        "            ax.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "            try:\n",
        "                handles, labels = ax.get_legend_handles_labels()\n",
        "                valid_indices = [i for i, lbl in enumerate(labels) if lbl.strip()]\n",
        "                if 'Target' in labels:\n",
        "                    valid_indices = [i for i in valid_indices if labels[i] != 'Target']\n",
        "\n",
        "                new_handles = [handles[i] for i in valid_indices]\n",
        "                new_labels = [target_map_scatter.get(int(float(labels[i])), labels[i]) for i in valid_indices]\n",
        "\n",
        "                if new_labels:\n",
        "                    ax.legend(handles=new_handles, labels=new_labels, title='Outcome')\n",
        "                else:\n",
        "                    ax.legend(title='Outcome')\n",
        "            except Exception as e_leg:\n",
        "                print(f\"[WARN] Legend formatting failed for {x_feat}: {e_leg}\")\n",
        "\n",
        "            plot_count_sc += 1\n",
        "        except Exception as e_sc_plot:\n",
        "            ax.text(0.5, 0.5, f\"Plot failed for:\\n{x_feat} vs {y_feat}\", ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "\n",
        "    if plot_count_sc > 0:\n",
        "        plt.tight_layout(pad=2.0)\n",
        "        savefig_report(\"06b_numerical_scatter_plots.png\", also_png=True)\n",
        "        print(f\"[OK] Plotted {plot_count_sc} numerical scatter plots.\")\n",
        "    else:\n",
        "        plt.close(fig_sc)\n",
        "else:\n",
        "    print(\"[WARN] No valid scatter plot pairs defined or features available.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TABLE 1 - DESCRIPTIVE STATISTICS\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Generating Descriptive Statistics Table (TRAIN SET ONLY)...\")\n",
        "\n",
        "key_features_list = [\n",
        "    'HomeElo', 'AwayElo', 'ELO_Diff',\n",
        "    'HomeTeam_ClubValue', 'AwayTeam_ClubValue', 'ClubValue_Diff',\n",
        "    'HomeTeam_ValueRatio', 'AwayTeam_ValueRatio', 'ValueRatio_Diff',\n",
        "    'HomeTeam_ManagerTrophies', 'AwayTeam_ManagerTrophies', 'ManagerTrophies_Diff',\n",
        "    'HomeTeam_NetTransferSpending', 'AwayTeam_NetTransferSpending', 'NetTransferSpending_Diff',\n",
        "    # üÜï Temporal Features\n",
        "    'Home_Target_Avg_L5', 'Away_Target_Avg_L5', 'Target_Diff_L5',\n",
        "    'Home_Shots_Avg_L5', 'Away_Shots_Avg_L5', 'Shots_Diff_L5',\n",
        "    'ShotAccuracy_Diff_L5'\n",
        "]\n",
        "\n",
        "key_features = [f for f in key_features_list if f in eda_data.columns]\n",
        "key_features = list(dict.fromkeys(key_features))\n",
        "\n",
        "if not key_features:\n",
        "    print(\"[WARN] No key features found for descriptive stats. Using all numeric columns.\")\n",
        "    key_features = numeric_cols\n",
        "    if len(key_features) > 20:\n",
        "         key_features = key_features[:20]\n",
        "\n",
        "if not key_features:\n",
        "     print(\"[ERROR] No numeric features available for descriptive statistics. Skipping.\")\n",
        "else:\n",
        "    print(f\"[INFO] Calculating stats for {len(key_features)} selected features...\")\n",
        "    stat_summary = eda_data[key_features].describe().T\n",
        "    try:\n",
        "        stat_summary['Skewness'] = eda_data[key_features].skew()\n",
        "        stat_summary['Kurtosis'] = eda_data[key_features].kurtosis()\n",
        "        stat_summary = stat_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'Skewness', 'Kurtosis']]\n",
        "        stat_summary.columns = ['Count', 'Mean', 'Std Dev', 'Min', 'Q1', 'Median', 'Q3', 'Max', 'Skewness', 'Kurtosis']\n",
        "    except Exception as e_skew:\n",
        "        print(f\"[WARN] Could not calculate Skewness/Kurtosis: {e_skew}\")\n",
        "        stat_summary = stat_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
        "        stat_summary.columns = ['Count', 'Mean', 'Std Dev', 'Min', 'Q1', 'Median', 'Q3', 'Max']\n",
        "\n",
        "    stat_summary_rounded = stat_summary.round(3)\n",
        "    stat_summary_rounded.to_csv(os.path.join(CSV_DIR, \"01_descriptive_statistics_train.csv\"))\n",
        "    save_df_as_image(stat_summary_rounded, \"01_descriptive_statistics_train.png\",\n",
        "                     title=\"Table 1: Descriptive Statistics of Key Numerical Variables \")\n",
        "    print(\"[OK] Descriptive statistics saved (CSV + PNG).\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CORRELATION ANALYSIS (CONTINUED - Feature-Target Correlation)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Performing Feature-Target Correlation Analysis...\")\n",
        "\n",
        "try:\n",
        "    if 'correlation_matrix' in locals() and not correlation_matrix.empty:\n",
        "        if 'Target' in eda_data.columns:\n",
        "             target_corr = eda_data[numeric_cols + ['Target']].corr()['Target'].drop('Target').sort_values(ascending=False)\n",
        "             print(f\"[OK] Feature-Target correlations calculated (Top 5):\")\n",
        "             print(target_corr.head())\n",
        "        else:\n",
        "             target_corr = pd.Series(dtype=float)\n",
        "             print(\"[WARN] Target column not available for Feature-Target correlation.\")\n",
        "\n",
        "        # G√∂rselle≈ütirme: Feature-Target Korelasyonlarƒ±\n",
        "        if not target_corr.empty:\n",
        "            print(\"[INFO] Creating feature-target correlations bar chart...\")\n",
        "            fig_tcorr, ax_tcorr = plt.subplots(figsize=(12, 10))\n",
        "            try:\n",
        "                target_corr_sorted = target_corr.sort_values()\n",
        "                n_show_corr = 10\n",
        "                top_display = pd.concat([target_corr_sorted.head(n_show_corr), target_corr_sorted.tail(n_show_corr)])\n",
        "                colors = [UNIFIED_COLORS['away_red'] if x < 0 else UNIFIED_COLORS['main_blue'] for x in top_display.values]\n",
        "\n",
        "                bars = ax_tcorr.barh(range(len(top_display)), top_display.values, color=colors, height=0.7)\n",
        "                ax_tcorr.set_yticks(range(len(top_display)))\n",
        "                ax_tcorr.set_yticklabels(top_display.index, fontsize=10)\n",
        "                ax_tcorr.set_xlabel('Pearson Correlation with Target', fontweight='bold', fontsize=12)\n",
        "                ax_tcorr.set_title(f'Feature-Target Correlations (Top & Bottom {n_show_corr}, Train Set)',\n",
        "                                   fontweight='bold', fontsize=14, pad=20)\n",
        "                ax_tcorr.grid(axis='x', alpha=0.3)\n",
        "                for i, v in enumerate(top_display.values):\n",
        "                     ax_tcorr.text(v + (0.005 if v >= 0 else -0.005), i, f'{v:.3f}',\n",
        "                                   va='center', ha='left' if v >= 0 else 'right',\n",
        "                                   fontsize=9, fontweight='bold')\n",
        "                ax_tcorr.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "                plt.tight_layout()\n",
        "                savefig_report(\"09b_feature_target_correlations.png\", also_png=True)\n",
        "                print(\"[OK] Feature-target correlations bar chart saved.\")\n",
        "            except Exception as e_tcorr:\n",
        "                print(f\"[WARN] Feature-Target correlation plot failed: {e_tcorr}\")\n",
        "                plt.close(fig_tcorr)\n",
        "\n",
        "        # Focused Heatmap (Stats vs. Market Odds) - TOP 10 & BLUE THEME\n",
        "        print(\"\\n[INFO] Creating focused heatmap (Stats vs. Market Odds)...\")\n",
        "        # TO BE: √ñzellik sayƒ±sƒ±nƒ± 3-4'ten 10'a √ßƒ±karma (En y√ºksek korelasyonlu olanlarƒ± se√ßme)\n",
        "        if 'Target' in eda_data.columns:\n",
        "            # Target ile korelasyonu en y√ºksek 10 numeric √∂zelliƒüi bul (Odds hari√ß)\n",
        "            target_corr_abs = eda_data[numeric_cols + ['Target']].corr()['Target'].abs().sort_values(ascending=False)\n",
        "            # Target'ƒ±n kendisini ve Odds √∂zelliklerini listeden √ßƒ±kar\n",
        "            exclude_odds = ['Target', 'OddHome', 'OddDraw', 'OddAway', 'Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm', 'Overround']\n",
        "            top_10_stats = [f for f in target_corr_abs.index if f not in exclude_odds][:10]\n",
        "        else:\n",
        "            # Yedek liste\n",
        "            top_10_stats = ['ELO_Diff', 'Form5Difference', 'ValueRatio_Diff', 'ClubValue_Diff',\n",
        "                            'Target_Diff_L5', 'Shots_Diff_L5', 'NetTransferSpending_Diff',\n",
        "                            'ManagerTrophies_Diff', 'Home_Target_Avg_L5', 'Away_Target_Avg_L5']\n",
        "        odds_features = ['Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm', 'Overround']\n",
        "        stats_features_avail = [f for f in top_10_stats if f in eda_data.columns]\n",
        "        odds_features_avail = [f for f in odds_features if f in eda_data.columns]\n",
        "        if stats_features_avail and odds_features_avail:\n",
        "            try:\n",
        "                focused_cols = stats_features_avail + odds_features_avail\n",
        "                focused_corr = eda_data[focused_cols].corr()\n",
        "                cross_corr_matrix = focused_corr.loc[stats_features_avail, odds_features_avail]\n",
        "                fig_focus, ax_focus = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "                # TO BE: Renk tonlarƒ± mavi (cmap='Blues')\n",
        "                sns.heatmap(cross_corr_matrix,\n",
        "                            annot=True, fmt='.2f', cmap='Blues', center=None,\n",
        "                            ax=ax_focus, annot_kws={'fontsize': 9},\n",
        "                            cbar_kws={'label': 'Pearson Correlation'},\n",
        "                            linewidths=0.5, linecolor='white')\n",
        "\n",
        "                ax_focus.set_title('Cross-Correlation: Top 10 Statistical Features vs. Market Odds ',\n",
        "                                   fontweight='bold', fontsize=12, pad=20)\n",
        "                ax_focus.set_xticklabels(ax_focus.get_xticklabels(), fontsize=10, rotation=45, ha='right')\n",
        "                ax_focus.set_yticklabels(ax_focus.get_yticklabels(), fontsize=10, rotation=0)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # TO BE: PDF √áƒ±ktƒ±sƒ± (savefig_report zaten PDF'e ekler ama ayrƒ±ca kaydedelim)\n",
        "                pdf_path = os.path.join(IMAGES_DIR, \"09d_correlation_heatmap_stats_vs_odds.pdf\")\n",
        "                plt.savefig(pdf_path, dpi=600, bbox_inches=\"tight\")\n",
        "                savefig_report(\"09d_correlation_heatmap_stats_vs_odds.png\", also_png=True)\n",
        "                print(\"[OK] Focused Stats vs. Odds heatmap saved (Blue Theme, Top 10).\")\n",
        "\n",
        "                if 'ELO_Diff' in stats_features_avail and 'Prob_H_Norm' in odds_features_avail:\n",
        "                    elo_prob_corr = cross_corr_matrix.loc['ELO_Diff', 'Prob_H_Norm']\n",
        "                    print(f\"[HYPOTHESIS CHECK] Correlation(ELO_Diff, Prob_H_Norm) = {elo_prob_corr:.3f}\")\n",
        "                    if elo_prob_corr > 0.7:\n",
        "                        print(\"[INSIGHT] High correlation suggests market odds heavily incorporate ELO information.\")\n",
        "                    else:\n",
        "                        print(\"[INSIGHT] Correlation is not dominant. 'Odds-free' models may find value.\")\n",
        "\n",
        "            except Exception as e_focus_hmap:\n",
        "                print(f\"[WARN] Failed to create Stats vs Odds heatmap: {e_focus_hmap}\")\n",
        "                try: plt.close(fig_focus)\n",
        "                except:pass\n",
        "        else:\n",
        "            print(\"[WARN] Could not find all required features for Stats vs. Odds heatmap. Skipping.\")\n",
        "\n",
        "        # Save correlation data to CSV\n",
        "        try:\n",
        "            correlation_matrix.to_csv(os.path.join(CSV_DIR, \"correlation_matrix_full_train.csv\"))\n",
        "            if not target_corr.empty:\n",
        "                 target_corr.to_csv(os.path.join(CSV_DIR, \"feature_target_correlation_train.csv\"))\n",
        "                 top_bottom_df = pd.DataFrame({\n",
        "                     'Feature': top_display.index,\n",
        "                     'Correlation': top_display.values,\n",
        "                     'Type': ['Negative' if x < 0 else 'Positive' for x in top_display.values]\n",
        "                 }).sort_values('Correlation')\n",
        "                 top_bottom_df.to_csv(os.path.join(CSV_DIR, f\"top_bottom_{n_show_corr}_correlations_train.csv\"), index=False)\n",
        "            print(\"[OK] Correlation data saved to CSV files.\")\n",
        "        except Exception as e_csv_corr:\n",
        "             print(f\"[WARN] Failed to save correlation data to CSV: {e_csv_corr}\")\n",
        "\n",
        "except Exception as e_corr_main:\n",
        "    print(f\"[ERROR] Correlation analysis continuation failed: {e_corr_main}\")\n",
        "# =============================================================================\n",
        "# üÜï PHASE 4.4: VIF (VARIANCE INFLATION FACTOR) ANALYSIS - OPTIMIZED\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.4: VIF (VARIANCE INFLATION FACTOR) ANALYSIS\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üî• YENƒ∞: SAFE VIF COMPUTATION FUNCTION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def compute_vif_safe(df, threshold_corr=0.90, threshold_vif=10):\n",
        "    \"\"\"\n",
        "    √ñnce high correlation temizle, sonra VIF hesapla\n",
        "\n",
        "    Bu yakla≈üƒ±m:\n",
        "    - 3-5x daha hƒ±zlƒ±\n",
        "    - Numerically daha stabil\n",
        "    - High VIF'leri daha iyi tespit eder\n",
        "    \"\"\"\n",
        "    if not VIF_AVAILABLE:\n",
        "        print(\"  ‚ö†Ô∏è  VIF not available. Skipping analysis.\")\n",
        "        return None, []\n",
        "\n",
        "    print(\"\\nüîç VIF ANALYSIS - OPTIMIZED (Safe Mode)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 0: INITIAL CLEANUP\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[STEP 0] Initial data preparation...\")\n",
        "\n",
        "    # Numeric only\n",
        "    X = df.select_dtypes(include=[np.number]).copy()\n",
        "    print(f\"  ‚Ä¢ Starting features: {X.shape[1]}\")\n",
        "\n",
        "    # Remove zero variance\n",
        "    variance_threshold = 1e-6\n",
        "    zero_var = X.columns[X.std() <= variance_threshold]\n",
        "    if len(zero_var) > 0:\n",
        "        print(f\"  ‚Ä¢ Removing {len(zero_var)} zero-variance features\")\n",
        "        X = X.drop(columns=zero_var)\n",
        "\n",
        "    # Fill missing\n",
        "    X = X.fillna(X.median())\n",
        "    print(f\"  ‚Ä¢ After cleanup: {X.shape[1]} features\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 1: AGGRESSIVE CORRELATION FILTERING\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"[STEP 1] Removing high correlations (|r| > {threshold_corr})...\")\n",
        "\n",
        "    corr_matrix = X.corr().abs()\n",
        "    upper_tri = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "    upper = corr_matrix.where(upper_tri)\n",
        "\n",
        "    # Drop one from each high-corr pair\n",
        "    to_drop = []\n",
        "    dropped_pairs = []\n",
        "\n",
        "    for column in upper.columns:\n",
        "        high_corr_features = upper.index[upper[column] > threshold_corr].tolist()\n",
        "        if high_corr_features:\n",
        "            # Keep first, drop rest\n",
        "            for feat in high_corr_features:\n",
        "                if feat not in to_drop and column not in to_drop:\n",
        "                    to_drop.append(feat)\n",
        "                    corr_val = corr_matrix.loc[column, feat]\n",
        "                    dropped_pairs.append((column, feat, corr_val))\n",
        "\n",
        "    if to_drop:\n",
        "        print(f\"  ‚Ä¢ Dropping {len(to_drop)} features due to high correlation\")\n",
        "        print(f\"  ‚Ä¢ Sample pairs removed:\")\n",
        "        for pair in dropped_pairs[:5]:\n",
        "            print(f\"      ‚Üí {pair[0]} ‚Üî {pair[1]} (r={pair[2]:.3f})\")\n",
        "        if len(dropped_pairs) > 5:\n",
        "            print(f\"      ... and {len(dropped_pairs)-5} more pairs\")\n",
        "\n",
        "        X_clean = X.drop(columns=to_drop, errors='ignore')\n",
        "    else:\n",
        "        print(f\"  ‚úì No features exceeded correlation threshold\")\n",
        "        X_clean = X.copy()\n",
        "\n",
        "    print(f\"  ‚Ä¢ Features after filtering: {X_clean.shape[1]}\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 2: COMPUTE VIF (Now much faster!)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[STEP 2] Computing VIF on cleaned data...\")\n",
        "    print(\"  ‚Üí Expected time: 5-15 seconds (vs 30-60 before)\\n\")\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    vif_data = []\n",
        "    features_list = X_clean.columns.tolist()\n",
        "\n",
        "    for i, col in enumerate(features_list):\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  Progress: {i+1}/{len(features_list)}\", end='\\r')\n",
        "\n",
        "        try:\n",
        "            vif = variance_inflation_factor(X_clean.values, i)\n",
        "\n",
        "            # üî• Stability check\n",
        "            if np.isnan(vif) or np.isinf(vif):\n",
        "                print(f\"\\n  ‚ö†Ô∏è  {col}: VIF=NaN (numerical instability)\")\n",
        "                vif_data.append({'Feature': col, 'VIF': np.nan, 'Status': 'Unstable'})\n",
        "            elif vif > 1000:\n",
        "                print(f\"\\n  ‚ö†Ô∏è  {col}: VIF={vif:.1f} (suspiciously high)\")\n",
        "                vif_data.append({'Feature': col, 'VIF': vif, 'Status': 'Very High'})\n",
        "            else:\n",
        "                vif_data.append({'Feature': col, 'VIF': vif, 'Status': 'OK'})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  ‚ùå {col}: ERROR - {str(e)[:50]}\")\n",
        "            vif_data.append({'Feature': col, 'VIF': np.nan, 'Status': 'Error'})\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n  ‚úÖ VIF computation done in {elapsed:.1f} seconds\\n\")\n",
        "\n",
        "    vif_df = pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 3: IDENTIFY DROP CANDIDATES\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(f\"[STEP 3] Identifying features to drop (VIF ‚â• {threshold_vif})...\")\n",
        "\n",
        "    high_vif = vif_df[vif_df['VIF'] >= threshold_vif]\n",
        "\n",
        "    if len(high_vif) > 0:\n",
        "        print(f\"  ‚Ä¢ Found {len(high_vif)} features with VIF ‚â• {threshold_vif}:\")\n",
        "        for idx, row in high_vif.head(10).iterrows():\n",
        "            print(f\"      ‚Üí {row['Feature']:40s} VIF={row['VIF']:.1f}\")\n",
        "        if len(high_vif) > 10:\n",
        "            print(f\"      ... and {len(high_vif)-10} more\")\n",
        "\n",
        "        features_to_drop_vif = high_vif['Feature'].tolist()\n",
        "    else:\n",
        "        print(f\"  ‚úì No features exceed VIF threshold\")\n",
        "        features_to_drop_vif = []\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 4: SUMMARY\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"[SUMMARY] VIF Analysis Results\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"  ‚Ä¢ Initial features:           {len(df.columns)}\")\n",
        "    print(f\"  ‚Ä¢ After correlation filter:   {X_clean.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Features with VIF ‚â• {threshold_vif}:     {len(features_to_drop_vif)}\")\n",
        "    print(f\"  ‚Ä¢ Total recommended drops:    {len(to_drop) + len(features_to_drop_vif)}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return vif_df, features_to_drop_vif, to_drop\n",
        "\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üî• EXECUTE OPTIMIZED VIF ANALYSIS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "if len(numeric_cols) > 5 and VIF_AVAILABLE:\n",
        "    try:\n",
        "        print(\"[INFO] Starting OPTIMIZED VIF analysis...\")\n",
        "\n",
        "        # üî• YENƒ∞: √ñnce correlation-based drops uygula (eƒüer varsa)\n",
        "        if 'features_to_drop_list' in locals() and features_to_drop_list:\n",
        "            print(f\"\\n[PRE-FILTER] Applying {len(features_to_drop_list)} correlation-based drops first...\")\n",
        "            features_for_vif = [f for f in numeric_cols if f not in features_to_drop_list]\n",
        "            print(f\"  ‚Ä¢ Reduced features: {len(numeric_cols)} ‚Üí {len(features_for_vif)}\")\n",
        "        else:\n",
        "            features_for_vif = numeric_cols\n",
        "\n",
        "        # Prepare data\n",
        "        eda_data_vif = eda_data[features_for_vif].copy()\n",
        "\n",
        "        # üî• YENƒ∞: Safe VIF computation\n",
        "        vif_results, vif_drop_list, corr_drop_list = compute_vif_safe(\n",
        "            df=eda_data_vif,\n",
        "            threshold_corr=0.90,  # Aggressive correlation threshold\n",
        "            threshold_vif=10       # Standard VIF threshold\n",
        "        )\n",
        "\n",
        "        if vif_results is not None:\n",
        "            # Save results\n",
        "            vif_results.to_csv(os.path.join(CSV_DIR, \"09g_vif_results.csv\"), index=False)\n",
        "            print(f\"[OK] VIF results saved to CSV.\")\n",
        "\n",
        "            # üî• YENƒ∞: Combined drop list\n",
        "            all_drops_multicollinearity = list(set(\n",
        "                (features_to_drop_list if 'features_to_drop_list' in locals() else []) +\n",
        "                corr_drop_list +\n",
        "                vif_drop_list\n",
        "            ))\n",
        "\n",
        "            print(f\"\\n[FINAL DROP LIST] Total features to remove: {len(all_drops_multicollinearity)}\")\n",
        "            print(f\"  ‚Ä¢ From correlation (Phase 4.2): {len(features_to_drop_list) if 'features_to_drop_list' in locals() else 0}\")\n",
        "            print(f\"  ‚Ä¢ From VIF correlation filter: {len(corr_drop_list)}\")\n",
        "            print(f\"  ‚Ä¢ From VIF threshold (‚â•10):    {len(vif_drop_list)}\")\n",
        "\n",
        "            # Save combined drop list\n",
        "            drop_code_path = os.path.join(CSV_DIR, \"09h_combined_multicollinearity_drops.py\")\n",
        "            with open(drop_code_path, 'w') as f:\n",
        "                f.write(\"# AUTO-GENERATED: Combined multicollinearity drop list\\n\")\n",
        "                f.write(\"# Sources: Phase 4.2 correlation + VIF analysis\\n\")\n",
        "                f.write(\"# Generated by EDA V5.2 (Optimized VIF)\\n\\n\")\n",
        "                f.write(\"features_to_drop_multicollinearity = [\\n\")\n",
        "                for feat in sorted(all_drops_multicollinearity):\n",
        "                    f.write(f\"    '{feat}',\\n\")\n",
        "                f.write(\"]\\n\\n\")\n",
        "                f.write(\"# Usage:\\n\")\n",
        "                f.write(\"# X_train = X_train.drop(columns=features_to_drop_multicollinearity)\\n\")\n",
        "\n",
        "            print(f\"[OK] Combined drop list saved: {os.path.basename(drop_code_path)}\")\n",
        "\n",
        "            # üî• YENƒ∞: Visualization - VIF Analysis Plot (BLUE THEME)\n",
        "            if IMAGES_DIR and not vif_results.empty:\n",
        "                print(\"[INFO] Creating VIF visualization...\")\n",
        "\n",
        "                try:\n",
        "                    # En y√ºksek VIF deƒüerine sahip ilk 30 √∂zelliƒüi alalƒ±m (okunabilirlik i√ßin)\n",
        "                    top_vif_plot = vif_results.head(30).sort_values('VIF', ascending=True)\n",
        "\n",
        "                    fig_vif, ax_vif = plt.subplots(figsize=(10, max(6, len(top_vif_plot)*0.3)))\n",
        "\n",
        "                    # Renkler: Mavi tonlarƒ± (D√º≈ü√ºk VIF a√ßƒ±k mavi, Y√ºksek VIF koyu mavi)\n",
        "                    # VIF deƒüerine g√∂re normalize edelim\n",
        "                    norm = plt.Normalize(top_vif_plot['VIF'].min(), top_vif_plot['VIF'].max())\n",
        "                    colors = plt.cm.Blues(norm(top_vif_plot['VIF'].values))\n",
        "\n",
        "                    bars = ax_vif.barh(top_vif_plot['Feature'], top_vif_plot['VIF'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "                    # E≈üik √ßizgisi (VIF = 10)\n",
        "                    ax_vif.axvline(10, color='red', linestyle='--', linewidth=1.5, label='Threshold (VIF=10)')\n",
        "\n",
        "                    ax_vif.set_xlabel('Variance Inflation Factor (VIF)', fontweight='bold')\n",
        "                    ax_vif.set_title('Multicollinearity Check: Top 30 Features by VIF', fontweight='bold')\n",
        "                    ax_vif.legend(loc='lower right')\n",
        "                    ax_vif.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "                    # Deƒüerleri √ßubuklarƒ±n ucuna yaz\n",
        "                    for i, v in enumerate(top_vif_plot['VIF']):\n",
        "                        ax_vif.text(v + 0.2, i, f'{v:.1f}', va='center', fontsize=8, fontweight='bold')\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    # 1. PDF Olarak Kaydet\n",
        "                    pdf_path_vif = os.path.join(IMAGES_DIR, \"09f_vif_analysis.pdf\")\n",
        "                    plt.savefig(pdf_path_vif, dpi=600, bbox_inches='tight')\n",
        "\n",
        "                    # 2. PNG Olarak Kaydet (Rapora eklemek i√ßin)\n",
        "                    save_path_vif = os.path.join(IMAGES_DIR, \"09f_vif_analysis.png\")\n",
        "                    plt.savefig(save_path_vif, dpi=600, bbox_inches='tight')\n",
        "                    savefig_report(\"09f_vif_analysis.png\", also_png=False)\n",
        "\n",
        "                    plt.close(fig_vif)\n",
        "                    print(f\"[OK] VIF Analysis plot saved (PDF & PNG).\")\n",
        "\n",
        "                except Exception as e_plot_vif:\n",
        "                    print(f\"[WARN] Could not create VIF plot: {e_plot_vif}\")\n",
        "                    plt.close(fig_vif)\n",
        "\n",
        "    except Exception as e_vif:\n",
        "        print(f\"[ERROR] VIF analysis failed: {e_vif}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        vif_results = None\n",
        "        vif_drop_list = []\n",
        "else:\n",
        "    print(\"[WARN] VIF analysis skipped (need >5 features and statsmodels).\")\n",
        "    vif_results = None\n",
        "    vif_drop_list = []\n",
        "\n",
        "# =============================================================================\n",
        "# FEATURE RELEVANCE (Mutual Information)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Computing Feature Relevance (Mutual Information) (TRAIN SET ONLY)...\")\n",
        "\n",
        "mi_scores = pd.Series(dtype=float)\n",
        "\n",
        "try:\n",
        "    if not numeric_cols:\n",
        "         print(\"[WARN] No numeric columns for MI calculation. Skipping.\")\n",
        "    elif 'Target' not in eda_data.columns:\n",
        "         print(\"[WARN] Target column missing for MI calculation. Skipping.\")\n",
        "    else:\n",
        "        X_mi = eda_data[numeric_cols].copy()\n",
        "        y_mi = eda_data['Target'].copy()\n",
        "\n",
        "        all_nan_cols = X_mi.columns[X_mi.isnull().all()].tolist()\n",
        "        if all_nan_cols:\n",
        "             print(f\"[WARN] MI Input: Following columns are all NaN and will be dropped: {all_nan_cols}\")\n",
        "             X_mi = X_mi.drop(columns=all_nan_cols)\n",
        "             numeric_cols_mi = X_mi.columns.tolist()\n",
        "        else:\n",
        "             numeric_cols_mi = numeric_cols\n",
        "\n",
        "        X_mi = X_mi.fillna(X_mi.mean())\n",
        "        y_mi = y_mi.fillna(y_mi.mode()[0])\n",
        "\n",
        "        if len(X_mi) != len(y_mi):\n",
        "             print(f\"[ERROR] MI: Shape mismatch between X ({len(X_mi)}) and y ({len(y_mi)}). Skipping.\")\n",
        "        elif len(X_mi) == 0:\n",
        "             print(\"[ERROR] MI: Input data is empty after processing. Skipping.\")\n",
        "        else:\n",
        "            print(f\"[INFO] Calculating MI for {len(numeric_cols_mi)} features...\")\n",
        "            scaler_mi = StandardScaler()\n",
        "            X_mi_scaled = scaler_mi.fit_transform(X_mi)\n",
        "\n",
        "            mi_values = mutual_info_classif(X_mi_scaled, y_mi, random_state=RNG_SEED)\n",
        "            mi_scores = pd.Series(mi_values, index=numeric_cols_mi).sort_values(ascending=False)\n",
        "\n",
        "            print(f\"\\n[OK] Mutual Information scores calculated!\")\n",
        "            print(\"=\" * 90)\n",
        "            print(\"üèÜ TOP 10 MOST PREDICTIVE FEATURES (by MI Score):\")\n",
        "            print(\"=\" * 90)\n",
        "            for i, (feat, score) in enumerate(mi_scores.head(10).items(), 1):\n",
        "                print(f\"  {i:2d}. {feat:40s} ‚Üí MI = {score:.4f}\")\n",
        "            print(\"=\" * 90 + \"\\n\")\n",
        "\n",
        "            # Visualization 1: MI Scores Bar Chart - BLUE THEME\n",
        "            try:\n",
        "                fig_mi1, ax_mi1 = plt.subplots(figsize=(10, 8))\n",
        "                n_show_mi = min(20, len(mi_scores))\n",
        "\n",
        "                # TO BE: Mavi tonlarƒ± (Blues colormap)\n",
        "                colors_grad = plt.cm.Blues(np.linspace(0.4, 1.0, n_show_mi))\n",
        "\n",
        "                ax_mi1.barh(range(n_show_mi), mi_scores.values[:n_show_mi], color=colors_grad)\n",
        "                ax_mi1.set_yticks(range(n_show_mi))\n",
        "                ax_mi1.set_yticklabels(mi_scores.index[:n_show_mi], fontsize=9)\n",
        "                ax_mi1.invert_yaxis()\n",
        "                ax_mi1.set_xlabel('Mutual Information Score with Target', fontweight='bold')\n",
        "                ax_mi1.set_title(f'Mutual Information (MI) Scores with Target (Top {n_show_mi}, Train Set)',\n",
        "                                 fontweight='bold', fontsize=12)\n",
        "                ax_mi1.grid(axis='x', alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # TO BE: PDF √áƒ±ktƒ±sƒ±\n",
        "                plt.savefig(os.path.join(IMAGES_DIR, \"10a_mutual_information_scores.pdf\"), dpi=600, bbox_inches=\"tight\")\n",
        "                savefig_report(\"10a_mutual_information_scores.png\", also_png=True)\n",
        "                print(\"[OK] MI scores bar chart saved (Blue Theme).\")\n",
        "            except Exception as e_mi1:\n",
        "                 print(f\"[WARN] MI bar chart (10a) failed: {e_mi1}\")\n",
        "                 plt.close(fig_mi1)\n",
        "\n",
        "            # Visualization 2: Figure 4 Format - BLUE THEME\n",
        "            try:\n",
        "                fig_mi2 = plt.figure(figsize=(12, 10))\n",
        "                ax_mi2 = fig_mi2.add_subplot(111)\n",
        "                top_20_mi = mi_scores.head(20)\n",
        "                # TO BE: Renk turuncudan (#F39C12) maviye (#1f77b4 veya steelblue) √ßevrildi\n",
        "                bars_mi2 = ax_mi2.barh(range(len(top_20_mi)), top_20_mi.values,\n",
        "                                       color=UNIFIED_COLORS['main_blue'], edgecolor='black', linewidth=1.2)\n",
        "\n",
        "                ax_mi2.set_yticks(range(len(top_20_mi)))\n",
        "                ax_mi2.set_yticklabels(top_20_mi.index, fontsize=10)\n",
        "                ax_mi2.invert_yaxis()\n",
        "                for i, (feature, value) in enumerate(top_20_mi.items()):\n",
        "                    ax_mi2.text(value + mi_scores.max()*0.01, i, f'{value:.4f}',\n",
        "                                va='center', fontsize=9, fontweight='bold')\n",
        "                ax_mi2.set_xlabel('Information Gain (Mutual Information)', fontweight='bold', fontsize=12)\n",
        "                ax_mi2.set_title('Feature Relevance: Mutual Information (Information Gain) - Top 20 ',\n",
        "                                 fontweight='bold', fontsize=12, pad=20)\n",
        "                ax_mi2.grid(axis='x', alpha=0.3)\n",
        "                ax_mi2.set_xlim([0, mi_scores.max() * 1.15])\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # TO BE: PDF √áƒ±ktƒ±sƒ±\n",
        "                plt.savefig(os.path.join(IMAGES_DIR, \"10b_feature_relevance_mutual_information.pdf\"), dpi=600, bbox_inches=\"tight\")\n",
        "                savefig_report(\"10b_feature_relevance_mutual_information.png\", also_png=True)\n",
        "                print(\"[OK] Feature Relevance (MI) visualization saved (Blue Theme).\")\n",
        "            except Exception as e_mi2:\n",
        "                print(f\"[WARN] MI Figure 4 plot (10b) failed: {e_mi2}\")\n",
        "                plt.close(fig_mi2)\n",
        "\n",
        "            # Save MI scores to CSV\n",
        "            try:\n",
        "                mi_detail_df = pd.DataFrame({\n",
        "                    'Feature': mi_scores.index,\n",
        "                    'Mutual_Information': mi_scores.values,\n",
        "                    'Rank': range(1, len(mi_scores) + 1)\n",
        "                })\n",
        "                mi_detail_df.to_csv(os.path.join(CSV_DIR, \"02_feature_relevance_mutual_information_train.csv\"), index=False)\n",
        "                print(\"[OK] Feature relevance (MI) details saved to CSV.\")\n",
        "            except Exception as e_csv_mi:\n",
        "                 print(f\"[WARN] Failed to save MI scores to CSV: {e_csv_mi}\")\n",
        "\n",
        "except Exception as e_mi_main:\n",
        "    print(f\"[ERROR] Mutual Information calculation failed: {e_mi_main}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï t-SNE VISUALIZATION (MULTI-PERPLEXITY COMPARISON)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Computing t-SNE Visualization (Multi-Perplexity) (TRAIN SET ONLY)...\")\n",
        "\n",
        "def plot_tsne_multiperplexity(df, target_col='Target', n_components=2,\n",
        "                               perplexities=[5, 15, 30, 50],\n",
        "                               random_state=42, output_dir=None):\n",
        "    \"\"\"\n",
        "    t-SNE visualization with multiple perplexity values for comparison\n",
        "\n",
        "    Perplexity Impact:\n",
        "    - Low (5-10): Focus on local structure\n",
        "    - Medium (20-30): Balanced view (recommended)\n",
        "    - High (50+): Focus on global structure\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüîÆ t-SNE MULTI-PERPLEXITY COMPARISON\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 1: PREPARE DATA\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[INFO] Preparing data...\")\n",
        "\n",
        "    y = df[target_col].copy()\n",
        "    X = df.drop(columns=[target_col, 'MatchDate', 'HomeTeam', 'AwayTeam'], errors='ignore')\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Remove zero-variance\n",
        "    zero_var = X.columns[X.std() == 0]\n",
        "    if len(zero_var) > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  Removing {len(zero_var)} zero-variance features\")\n",
        "        X = X.drop(columns=zero_var)\n",
        "\n",
        "    X = X.fillna(X.median())\n",
        "\n",
        "    # Subsample if large\n",
        "    MAX_SAMPLES = 3000\n",
        "    if len(X) > MAX_SAMPLES:\n",
        "        print(f\"  ‚ö†Ô∏è  Subsampling to {MAX_SAMPLES:,} samples\")\n",
        "        sample_idx = np.random.choice(len(X), MAX_SAMPLES, replace=False)\n",
        "        X_tsne = X.iloc[sample_idx].copy()\n",
        "        y_tsne = y.iloc[sample_idx].copy()\n",
        "    else:\n",
        "        X_tsne = X.copy()\n",
        "        y_tsne = y.copy()\n",
        "\n",
        "    print(f\"  Dataset: {X_tsne.shape[0]:,} samples √ó {X_tsne.shape[1]} features\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 2: COMPUTE t-SNE FOR EACH PERPLEXITY\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"[COMPUTING t-SNE] Testing multiple perplexities...\")\n",
        "    print(\"  ‚Üí This may take 2-5 minutes\\n\")\n",
        "\n",
        "    embeddings = {}\n",
        "    computation_times = {}\n",
        "\n",
        "    for perp in perplexities:\n",
        "        print(f\"  Computing perplexity={perp}...\", end=' ')\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if perp >= len(X_tsne):\n",
        "                print(f\"‚ö†Ô∏è  SKIPPED (perp must be < n_samples)\")\n",
        "                continue\n",
        "\n",
        "            tsne = TSNE(\n",
        "                n_components=n_components,\n",
        "                perplexity=perp,\n",
        "                random_state=random_state,\n",
        "                init='pca',\n",
        "                learning_rate='auto',\n",
        "                n_iter=1000,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            embedding = tsne.fit_transform(X_tsne)\n",
        "            embeddings[perp] = embedding\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            computation_times[perp] = elapsed\n",
        "\n",
        "            print(f\"‚úÖ ({elapsed:.1f}s)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ERROR: {str(e)[:50]}\")\n",
        "            continue\n",
        "\n",
        "    if not embeddings:\n",
        "        print(\"\\n  ‚ùå No valid embeddings computed!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n  ‚úÖ Successfully computed {len(embeddings)} embeddings\")\n",
        "    print(f\"  Total time: {sum(computation_times.values()):.1f}s\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 3: VISUALIZATION\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    if output_dir and len(embeddings) > 0:\n",
        "        print(\"[VISUALIZATION] Creating comparison plots...\")\n",
        "\n",
        "        n_plots = len(embeddings)\n",
        "        if n_plots <= 2:\n",
        "            nrows, ncols = 1, 2\n",
        "            figsize = (12, 6)\n",
        "        elif n_plots <= 4:\n",
        "            nrows, ncols = 2, 2\n",
        "            figsize = (14, 12)\n",
        "        else:\n",
        "            nrows, ncols = 2, 3\n",
        "            figsize = (18, 12)\n",
        "\n",
        "        fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        axes = axes.flatten() if n_plots > 1 else [axes]\n",
        "\n",
        "        class_names = ['Draw', 'Home Win', 'Away Win']\n",
        "        colors = ['#AED6F1', '#2E86C1', '#154360']\n",
        "\n",
        "        for idx, (perp, embedding) in enumerate(sorted(embeddings.items())):\n",
        "            ax = axes[idx]\n",
        "\n",
        "            for class_idx, (class_name, color) in enumerate(zip(class_names, colors)):\n",
        "                mask = (y_tsne == class_idx)\n",
        "\n",
        "                if mask.sum() > 0:\n",
        "                    ax.scatter(\n",
        "                        embedding[mask, 0],\n",
        "                        embedding[mask, 1],\n",
        "                        c=color,\n",
        "                        label=class_name,\n",
        "                        alpha=0.9,\n",
        "                        s=40,\n",
        "                        edgecolors='black',\n",
        "                        linewidth=0.5\n",
        "                    )\n",
        "\n",
        "            ax.set_xlabel('t-SNE Dimension 1', fontsize=10, fontweight='bold')\n",
        "            ax.set_ylabel('t-SNE Dimension 2', fontsize=10, fontweight='bold')\n",
        "            ax.set_title(f'Perplexity = {perp}\\n({computation_times[perp]:.1f}s)',\n",
        "                        fontsize=12, fontweight='bold', pad=10)\n",
        "            ax.legend(fontsize=9, loc='upper right', framealpha=0.9)\n",
        "            ax.grid(alpha=0.3)\n",
        "\n",
        "            # Silhouette score\n",
        "            if SILHOUETTE_AVAILABLE:\n",
        "                try:\n",
        "                    silhouette = silhouette_score(embedding, y_tsne)\n",
        "                    info_text = f'Samples: {len(y_tsne):,}\\nSilhouette: {silhouette:.3f}'\n",
        "                except:\n",
        "                    info_text = f'Samples: {len(y_tsne):,}'\n",
        "            else:\n",
        "                info_text = f'Samples: {len(y_tsne):,}'\n",
        "\n",
        "            ax.text(0.02, 0.98, info_text, transform=ax.transAxes,\n",
        "                   fontsize=8, verticalalignment='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        # Hide unused\n",
        "        for idx in range(n_plots, len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.suptitle('t-SNE Dimensionality Reduction - Perplexity Comparison\\n' +\n",
        "                     'How does perplexity affect cluster visualization?',\n",
        "                     fontsize=15, fontweight='bold', y=0.995)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "\n",
        "        # 1. PDF OLARAK KAYDET (ƒ∞stediƒüin √∂zellik)\n",
        "        pdf_path = os.path.join(output_dir, '11a_tsne_multiperplexity.pdf')\n",
        "        try:\n",
        "            plt.savefig(pdf_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "            print(f\"    ‚úÖ Saved PDF: {os.path.basename(pdf_path)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ö†Ô∏è Could not save PDF: {e}\")\n",
        "\n",
        "        # 2. PNG OLARAK KAYDET (Fix: save_path deƒüi≈ükenini a√ßƒ±k√ßa tanƒ±mlƒ±yoruz)\n",
        "        save_path = os.path.join(output_dir, '11a_tsne_multiperplexity.png')\n",
        "        try:\n",
        "            plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "            # Rapora i≈üle (PDF raporuna da ekler)\n",
        "            savefig_report('11a_tsne_multiperplexity.png', also_png=False)\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ö†Ô∏è Could not save PNG: {e}\")\n",
        "\n",
        "        plt.close()\n",
        "        print(f\"    ‚úÖ Saved PNG: {os.path.basename(save_path)}\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # STEP 4: RECOMMENDATIONS\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"[RECOMMENDATIONS] Perplexity Selection\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    n_samples = len(X_tsne)\n",
        "\n",
        "    print(f\"\\n  Dataset size: {n_samples:,} samples\")\n",
        "    print(f\"  Recommended range: {max(5, n_samples//100)} - {min(50, n_samples//10)}\")\n",
        "    print(f\"  Default: 30 (balanced)\")\n",
        "\n",
        "    # Find best by silhouette\n",
        "    if SILHOUETTE_AVAILABLE:\n",
        "        try:\n",
        "            silhouette_scores = {}\n",
        "            for perp, embedding in embeddings.items():\n",
        "                try:\n",
        "                    score = silhouette_score(embedding, y_tsne)\n",
        "                    silhouette_scores[perp] = score\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if silhouette_scores:\n",
        "                best_perp = max(silhouette_scores, key=silhouette_scores.get)\n",
        "                best_score = silhouette_scores[best_perp]\n",
        "\n",
        "                print(f\"\\n  Silhouette Scores:\")\n",
        "                for perp in sorted(silhouette_scores.keys()):\n",
        "                    score = silhouette_scores[perp]\n",
        "                    marker = \"üèÜ\" if perp == best_perp else \"  \"\n",
        "                    print(f\"    {marker} Perplexity={perp:2d}: {score:.4f}\")\n",
        "\n",
        "                print(f\"\\n  ‚úÖ Best: perplexity={best_perp} (silhouette={best_score:.4f})\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'embeddings': embeddings,\n",
        "        'computation_times': computation_times,\n",
        "        'sample_size': len(X_tsne),\n",
        "        'n_features': X_tsne.shape[1]\n",
        "    }\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# EXECUTE t-SNE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "try:\n",
        "    if not numeric_cols:\n",
        "        print(\"[WARN] No numeric columns for t-SNE. Skipping.\")\n",
        "        tsne_results = None\n",
        "    elif 'Target' not in eda_data.columns:\n",
        "        print(\"[WARN] Target column missing for t-SNE. Skipping.\")\n",
        "        tsne_results = None\n",
        "    else:\n",
        "        tsne_results = plot_tsne_multiperplexity(\n",
        "            df=eda_data,\n",
        "            target_col='Target',\n",
        "            n_components=2,\n",
        "            perplexities=[5, 15, 30, 50],\n",
        "            random_state=RNG_SEED,\n",
        "            output_dir=IMAGES_DIR\n",
        "        )\n",
        "        print(\"[OK] t-SNE multi-perplexity analysis complete.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"[WARN] Could not import TSNE. Skipping.\")\n",
        "    tsne_results = None\n",
        "except Exception as e_tsne:\n",
        "    print(f\"[ERROR] t-SNE analysis failed: {e_tsne}\")\n",
        "    tsne_results = None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï PHASE 4.9: FEATURE ENGINEERING EXAMPLES & VISUALIZATION\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.9: FEATURE ENGINEERING EXAMPLES\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "def visualize_feature_engineering_pipeline(df, output_dir):\n",
        "    \"\"\"\n",
        "    Visualize derived features from model's feature engineering pipeline\n",
        "\n",
        "    Demonstrates:\n",
        "    1. ELO_Diff: Performance indicator\n",
        "    2. ValueRatio_Diff: Financial advantage\n",
        "    3. Log Transformations: Skewness reduction\n",
        "    4. Normalized Probabilities: Bookmaker odds\n",
        "    5. Diff Features Correlation\n",
        "    6. Lag Features: Temporal patterns\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüîß FEATURE ENGINEERING PIPELINE\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"[INFO] Visualizing model feature engineering\")\n",
        "    print(\"  ‚Üí Goal: Show why engineered features improve prediction\\n\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # CREATE FIGURE (3√ó2 grid)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.25)\n",
        "\n",
        "    class_names = ['Draw', 'Home Win', 'Away Win']\n",
        "    colors = ['#f39c12', '#2ecc71', '#e74c3c']\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 1: ELO_Diff Distribution\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "\n",
        "    if 'ELO_Diff' in df.columns and 'Target' in df.columns:\n",
        "        print(\"[PLOT 1] ELO_Diff distribution...\")\n",
        "\n",
        "        for target_val, color, label in zip([0, 1, 2], colors, class_names):\n",
        "            data = df[df['Target'] == target_val]['ELO_Diff'].dropna()\n",
        "            if len(data) > 0:\n",
        "                ax1.hist(data, bins=40, alpha=1.0, color=color,\n",
        "                        label=f'{label} (n={len(data):,})',\n",
        "                        edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax1.axvline(0, color='black', linestyle='--', linewidth=2.5,\n",
        "                   label='Equal ELO', alpha=0.7)\n",
        "        ax1.set_xlabel('ELO_Diff (Home - Away)', fontsize=12, fontweight='bold')\n",
        "        ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "        ax1.set_title('ELO Difference as Performance Indicator\\n' +\n",
        "                     'Positive = Home advantage',\n",
        "                     fontsize=12, fontweight='bold', pad=15)\n",
        "        ax1.legend(fontsize=9)\n",
        "        ax1.grid(alpha=0.3)\n",
        "\n",
        "        # Medians\n",
        "        for target_val, color in zip([0, 1, 2], colors):\n",
        "            median_val = df[df['Target'] == target_val]['ELO_Diff'].median()\n",
        "            if pd.notna(median_val):\n",
        "                ax1.axvline(median_val, color=color, linestyle=':', linewidth=2, alpha=0.5)\n",
        "    else:\n",
        "        ax1.text(0.5, 0.5, 'ELO_Diff not available',\n",
        "                ha='center', va='center', transform=ax1.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 2: ValueRatio_Diff Distribution\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "\n",
        "    if 'ValueRatio_Diff' in df.columns and 'Target' in df.columns:\n",
        "        print(\"[PLOT 2] ValueRatio_Diff distribution...\")\n",
        "\n",
        "        for target_val, color, label in zip([0, 1, 2], colors, class_names):\n",
        "            data = df[df['Target'] == target_val]['ValueRatio_Diff'].dropna()\n",
        "            if len(data) > 0:\n",
        "                ax2.hist(data, bins=40, alpha=0.9, color=color,\n",
        "                        label=f'{label} (n={len(data):,})',\n",
        "                        edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax2.axvline(0, color='black', linestyle='--', linewidth=2.5, alpha=0.7)\n",
        "        ax2.set_xlabel('ValueRatio_Diff', fontsize=12, fontweight='bold')\n",
        "        ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "        ax2.set_title('Financial Advantage Indicator\\n' +\n",
        "                     'ValueRatio = ClubValue / LeagueValue',\n",
        "                     fontsize=12, fontweight='bold', pad=15)\n",
        "        ax2.legend(fontsize=9)\n",
        "        ax2.grid(alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'ValueRatio_Diff not available',\n",
        "                ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 3: Log Transformation Effect (FIXED & UNIFIED)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    if 'HomeTeam_ClubValue' in df.columns:\n",
        "        print(\"[PLOT 3] Log transformation effect...\")\n",
        "        original = df['HomeTeam_ClubValue'].dropna()\n",
        "\n",
        "        # 1. Orijinal Veri (Gri - Arka Plan)\n",
        "        ax3.hist(original, bins=50,\n",
        "                 alpha=1.0,\n",
        "                 color=UNIFIED_COLORS['gray'],\n",
        "                 label='Original',\n",
        "                 edgecolor='black',\n",
        "                 linewidth=1.2)\n",
        "\n",
        "        # Eksen rengi de Gri olsun ki hangi veriye ait olduƒüu anla≈üƒ±lsƒ±n\n",
        "        ax3.set_ylabel('Frequency (Original)', fontsize=12, fontweight='bold', color=UNIFIED_COLORS['gray'])\n",
        "        ax3.tick_params(axis='y', labelcolor=UNIFIED_COLORS['gray'])\n",
        "\n",
        "        # 2. Log D√∂n√º≈ü√ºm√º (Ana Mavi - √ñn Plan)\n",
        "        ax3_twin = ax3.twinx()\n",
        "        log_transformed = np.log1p(original)\n",
        "\n",
        "        ax3_twin.hist(log_transformed, bins=50,\n",
        "                      alpha=0.8,\n",
        "                      color=UNIFIED_COLORS['main_blue'],\n",
        "                      label='Log',\n",
        "                      edgecolor='black',\n",
        "                      linewidth=1.2)\n",
        "\n",
        "        # HATA D√úZELTƒ∞LDƒ∞: color= parametresi eklendi\n",
        "        ax3_twin.set_ylabel('Frequency (Log)', fontsize=12, fontweight='bold', color=UNIFIED_COLORS['main_blue'])\n",
        "        ax3_twin.tick_params(axis='y', labelcolor=UNIFIED_COLORS['main_blue'])\n",
        "\n",
        "        ax3.set_xlabel('Club Value', fontsize=12, fontweight='bold')\n",
        "        ax3.set_title('Log Transformation Effect\\n' +\n",
        "                      'Reduces skewness for better training',\n",
        "                      fontsize=12, fontweight='bold', pad=15)\n",
        "        ax3.grid(alpha=0.5, axis='x', linestyle='--')\n",
        "\n",
        "        # Legend (Renkler d√ºzeltildi ve color= parametresi eklendi)\n",
        "        from matplotlib.lines import Line2D\n",
        "        legend_elements = [\n",
        "            Line2D([0], [0], color=UNIFIED_COLORS['gray'], lw=6, label='Original (skewed)'),\n",
        "            Line2D([0], [0], color=UNIFIED_COLORS['main_blue'], lw=6, label='Log (normalized)')\n",
        "        ]\n",
        "        ax3.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, 'ClubValue not available',\n",
        "                 ha='center', va='center', transform=ax3.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 4: Normalized Odds\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "    odds_cols = ['OddHome', 'OddDraw', 'OddAway']\n",
        "    norm_cols = ['Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm']\n",
        "\n",
        "    if all(c in df.columns for c in odds_cols):\n",
        "        print(\"[PLOT 4] Normalized odds...\")\n",
        "\n",
        "        sample = df[odds_cols + norm_cols].dropna().sample(min(1000, len(df)), random_state=42)\n",
        "\n",
        "        sample['Prob_H_Raw'] = 1 / sample['OddHome']\n",
        "        sample['Prob_D_Raw'] = 1 / sample['OddDraw']\n",
        "        sample['Prob_A_Raw'] = 1 / sample['OddAway']\n",
        "        sample['Sum_Raw'] = sample[['Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw']].sum(axis=1)\n",
        "        sample['Sum_Norm'] = sample[norm_cols].sum(axis=1)\n",
        "\n",
        "        ax4.hist(sample['Sum_Raw'], bins=40, alpha=0.9, color='coral',\n",
        "                label='Raw (with overround)', edgecolor='black', linewidth=0.5)\n",
        "        ax4.hist(sample['Sum_Norm'], bins=40, alpha=0.9, color='lightgreen',\n",
        "                label='Normalized', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax4.axvline(1.0, color='red', linestyle='--', linewidth=2.5,\n",
        "                   label='Ideal Sum (1.0)', alpha=0.7)\n",
        "\n",
        "        ax4.set_xlabel('Sum of Probabilities', fontsize=12, fontweight='bold')\n",
        "        ax4.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "        ax4.set_title('Bookmaker Odds Normalization\\n' +\n",
        "                     'Remove profit margin',\n",
        "                     fontsize=12, fontweight='bold', pad=15)\n",
        "        ax4.legend(fontsize=9)\n",
        "        ax4.grid(alpha=0.3)\n",
        "\n",
        "        mean_raw = sample['Sum_Raw'].mean()\n",
        "        mean_norm = sample['Sum_Norm'].mean()\n",
        "        overround = (mean_raw - 1.0) * 100\n",
        "\n",
        "        stats_text = f'Avg Sum:\\nRaw: {mean_raw:.4f}\\nNorm: {mean_norm:.4f}\\nOverround: {overround:.2f}%'\n",
        "        ax4.text(0.98, 0.98, stats_text, transform=ax4.transAxes,\n",
        "                ha='right', va='top', fontsize=9,\n",
        "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Odds not available',\n",
        "                ha='center', va='center', transform=ax4.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 5: Diff Features Correlation\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax5 = fig.add_subplot(gs[2, 0])\n",
        "\n",
        "    diff_features = [c for c in df.columns if c.endswith('_Diff')]\n",
        "\n",
        "    if len(diff_features) >= 3:\n",
        "        print(\"[PLOT 5] Diff features correlation...\")\n",
        "\n",
        "        diff_data = df[diff_features].dropna()\n",
        "        variances = diff_data.var().sort_values(ascending=False)\n",
        "        top_diff = variances.head(8).index.tolist()\n",
        "\n",
        "        corr = df[top_diff].corr()\n",
        "\n",
        "        im = ax5.imshow(corr, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "        ax5.set_xticks(range(len(top_diff)))\n",
        "        ax5.set_yticks(range(len(top_diff)))\n",
        "        ax5.set_xticklabels([f[:20] for f in top_diff], rotation=45, ha='right', fontsize=8)\n",
        "        ax5.set_yticklabels([f[:20] for f in top_diff], fontsize=8)\n",
        "\n",
        "        for i in range(len(top_diff)):\n",
        "            for j in range(len(top_diff)):\n",
        "                text_color = 'white' if abs(corr.iloc[i, j]) > 0.5 else 'black'\n",
        "                ax5.text(j, i, f'{corr.iloc[i, j]:.2f}',\n",
        "                        ha='center', va='center', color=text_color,\n",
        "                        fontsize=8, fontweight='bold')\n",
        "\n",
        "        ax5.set_title('Diff Features Correlation\\n(Home - Away)',\n",
        "                     fontsize=12, fontweight='bold', pad=15)\n",
        "        plt.colorbar(im, ax=ax5, label='Correlation', fraction=0.046, pad=0.04)\n",
        "    else:\n",
        "        ax5.text(0.5, 0.5, 'Insufficient Diff features',\n",
        "                ha='center', va='center', transform=ax5.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # PLOT 6: Lag Features (L3 vs L5)\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    ax6 = fig.add_subplot(gs[2, 1])\n",
        "\n",
        "    lag3_features = [c for c in df.columns if 'L3' in c]\n",
        "    lag5_features = [c for c in df.columns if 'L5' in c]\n",
        "\n",
        "    if lag3_features and lag5_features:\n",
        "        print(\"[PLOT 6] Lag features (L3 vs L5)...\")\n",
        "\n",
        "        l3_col = lag3_features[0]\n",
        "        l5_col = l3_col.replace('L3', 'L5')\n",
        "\n",
        "        if l5_col in df.columns:\n",
        "            sample = df[[l3_col, l5_col]].dropna().sample(min(2000, len(df)), random_state=42)\n",
        "\n",
        "            ax6.scatter(sample[l3_col], sample[l5_col],\n",
        "                       alpha=0.4, s=30, c='purple',\n",
        "                       edgecolors='black', linewidth=0.5)\n",
        "\n",
        "            min_val = min(sample[l3_col].min(), sample[l5_col].min())\n",
        "            max_val = max(sample[l3_col].max(), sample[l5_col].max())\n",
        "            ax6.plot([min_val, max_val], [min_val, max_val],\n",
        "                    'r--', linewidth=2.5, label='Perfect', alpha=0.7)\n",
        "\n",
        "            from scipy import stats\n",
        "            slope, intercept, r_value, _, _ = stats.linregress(sample[l3_col], sample[l5_col])\n",
        "            line_x = np.array([min_val, max_val])\n",
        "            line_y = slope * line_x + intercept\n",
        "            ax6.plot(line_x, line_y, 'g-', linewidth=2.5,\n",
        "                    label=f'Fit (r={r_value:.3f})', alpha=0.7)\n",
        "\n",
        "            ax6.set_xlabel(f'{l3_col[:30]}...', fontsize=10, fontweight='bold')\n",
        "            ax6.set_ylabel(f'{l5_col[:30]}...', fontsize=10, fontweight='bold')\n",
        "            ax6.set_title('Lag Features: L3 vs L5\\nWindow Comparison',\n",
        "                         fontsize=12, fontweight='bold', pad=15)\n",
        "            ax6.legend(fontsize=9)\n",
        "            ax6.grid(alpha=0.3)\n",
        "\n",
        "            corr_val = sample[[l3_col, l5_col]].corr().iloc[0, 1]\n",
        "            ax6.text(0.98, 0.02, f'r = {corr_val:.4f}',\n",
        "                    transform=ax6.transAxes, ha='right', va='bottom',\n",
        "                    fontsize=12, fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
        "        else:\n",
        "            ax6.text(0.5, 0.5, 'Paired L5 not found',\n",
        "                    ha='center', va='center', transform=ax6.transAxes)\n",
        "    else:\n",
        "        ax6.text(0.5, 0.5, 'Lag features not available',\n",
        "                ha='center', va='center', transform=ax6.transAxes)\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # SAVE\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    plt.suptitle('Feature Engineering Pipeline (Model v18.0)\\n' +\n",
        "                 'Derived features that improve prediction accuracy',\n",
        "                 fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "\n",
        "    save_path = os.path.join(output_dir, '12_feature_engineering_examples.png')\n",
        "    plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
        "    savefig_report('12_feature_engineering_examples.png', also_png=True)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\n  ‚úÖ Saved: {os.path.basename(save_path)}\")\n",
        "\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    # SUMMARY\n",
        "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"[SUMMARY] Feature Engineering Impact\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n  ‚úÖ Difference Features: ELO_Diff, ValueRatio_Diff, etc.\")\n",
        "    print(f\"  ‚úÖ Ratio Features: ValueRatio = ClubValue / LeagueValue\")\n",
        "    print(f\"  ‚úÖ Log Transformations: Reduce skewness\")\n",
        "    print(f\"  ‚úÖ Normalized Probabilities: Remove overround\")\n",
        "    print(f\"  ‚úÖ Temporal Features: L3/L5 windows with shift(1)\")\n",
        "\n",
        "    n_engineered = len([c for c in df.columns if any(x in c for x in\n",
        "                       ['_Diff', '_Ratio', '_log', 'Prob_', '_L3', '_L5'])])\n",
        "    print(f\"\\n  üìä Total Engineered Features: {n_engineered}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# EXECUTE FEATURE ENGINEERING EXAMPLES\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "try:\n",
        "    visualize_feature_engineering_pipeline(\n",
        "        df=eda_data,\n",
        "        output_dir=IMAGES_DIR\n",
        "    )\n",
        "    print(\"[OK] Feature engineering examples complete.\")\n",
        "except Exception as e_fe:\n",
        "    print(f\"[WARN] Feature engineering visualization failed: {e_fe}\")\n",
        "\n",
        "\n",
        "print(\"\\n[INFO] ========== END OF PART 2A ==========\")\n",
        "print(\"[INFO] Continue with PART 2B for Outlier Detection, Temporal Analysis, Chi-Square, and Summary\")\n",
        "\n",
        "# =============================================================================\n",
        "# EDA V5.0 - PART 2B: OUTLIER DETECTION ‚Üí FINAL SUMMARY\n",
        "# Bu dosya PART 2A'nƒ±n devamƒ±dƒ±r. PART 1 ve 2A'dan sonra √ßalƒ±≈ütƒ±rƒ±n.\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[INFO] ========== STARTING PART 2B ==========\")\n",
        "\n",
        "# =============================================================================\n",
        "# OUTLIER DETECTION & ANALYSIS\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.5: OUTLIER DETECTION & ANALYSIS (TRAIN SET ONLY)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "outlier_found = False\n",
        "outliers_by_feature = pd.Series(dtype=int)\n",
        "outlier_indices = pd.Index([])\n",
        "outlier_mask_df = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    if not numeric_cols:\n",
        "        print(\"[WARN] No numeric columns for Outlier analysis. Skipping.\")\n",
        "    else:\n",
        "        df_for_iqr = eda_data[numeric_cols].copy()\n",
        "\n",
        "        all_nan_cols_iqr = df_for_iqr.columns[df_for_iqr.isnull().all()].tolist()\n",
        "        if all_nan_cols_iqr:\n",
        "            print(f\"[WARN] IQR Outlier Check: Dropping all-NaN columns: {all_nan_cols_iqr}\")\n",
        "            df_for_iqr = df_for_iqr.drop(columns=all_nan_cols_iqr)\n",
        "\n",
        "        if df_for_iqr.empty or df_for_iqr.shape[1] == 0:\n",
        "            print(\"[WARN] No valid numeric data left for IQR calculation. Skipping outlier detection.\")\n",
        "        else:\n",
        "            Q1 = df_for_iqr.quantile(0.25)\n",
        "            Q3 = df_for_iqr.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - (1.5 * IQR)\n",
        "            upper_bound = Q3 + (1.5 * IQR)\n",
        "\n",
        "            outlier_mask_df = (df_for_iqr < lower_bound) | (df_for_iqr > upper_bound)\n",
        "\n",
        "            outlier_mask_any_row = outlier_mask_df.any(axis=1)\n",
        "\n",
        "            outlier_indices = eda_data.index[outlier_mask_any_row]\n",
        "            outlier_count_iqr = len(outlier_indices)\n",
        "            outlier_found = outlier_count_iqr > 0\n",
        "\n",
        "            outlier_threshold_iqr_str = \"IQR (Q1 - 1.5*IQR or Q3 + 1.5*IQR)\"\n",
        "            print(f\"[OK] Outlier detection method: {outlier_threshold_iqr_str}\")\n",
        "            print(f\"     Total samples in training set: {len(eda_data):,}\")\n",
        "            if outlier_found:\n",
        "                print(f\"     Found {outlier_count_iqr:,} samples ({100*outlier_count_iqr/len(eda_data):.2f}%) with at least one outlier feature.\")\n",
        "            else:\n",
        "                print(\"     No outliers found using IQR method.\")\n",
        "\n",
        "            outliers_by_feature = outlier_mask_df.sum(axis=0).sort_values(ascending=False)\n",
        "            outliers_by_feature = outliers_by_feature[outliers_by_feature > 0]\n",
        "\n",
        "            if not outliers_by_feature.empty:\n",
        "                outliers_by_feature_df = pd.DataFrame({\n",
        "                    'Feature': outliers_by_feature.index,\n",
        "                    'Outlier_Count': outliers_by_feature.values,\n",
        "                    'Percentage_of_Samples': (outliers_by_feature.values / len(eda_data) * 100).round(2)\n",
        "                }).reset_index(drop=True)\n",
        "\n",
        "                print(f\"\\n[OK] Features contributing to outliers (Top 15 or all if fewer):\")\n",
        "                print(outliers_by_feature_df.head(15))\n",
        "                outliers_by_feature_df.to_csv(os.path.join(CSV_DIR, \"10a_outliers_by_feature_train_IQR.csv\"), index=False)\n",
        "                print(f\"[OK] Outlier counts by feature (IQR) saved to CSV.\")\n",
        "            else:\n",
        "                print(\"\\n[INFO] No individual features exceeded the IQR threshold.\")\n",
        "\n",
        "            if outlier_found:\n",
        "                outliers_df = eda_data.loc[outlier_indices].copy()\n",
        "\n",
        "                outlier_cols_list = []\n",
        "                relevant_mask = outlier_mask_df.loc[outlier_indices]\n",
        "\n",
        "                for idx, row in relevant_mask.iterrows():\n",
        "                    cols = row[row].index.tolist()\n",
        "                    outlier_cols_list.append(\", \".join(cols))\n",
        "\n",
        "                outliers_df['Outlier_Features_IQR'] = outlier_cols_list\n",
        "\n",
        "                print(f\"\\n[INFO] Saving {len(outliers_df)} outlier samples details to CSV...\")\n",
        "                outliers_df.to_csv(os.path.join(CSV_DIR, \"10b_outlier_samples_train_IQR.csv\"), index=False)\n",
        "                print(f\"[OK] Outlier samples saved.\")\n",
        "\n",
        "                # Visualization - Box plots\n",
        "                print(\"[INFO] Creating outlier visualizations (Boxplots for key features)...\")\n",
        "                features_for_boxplot = outliers_by_feature.head(12).index.tolist()\n",
        "                if not features_for_boxplot:\n",
        "                    key_features_list_check = [f for f in key_features_list if f in eda_data.columns]\n",
        "                    features_for_boxplot = key_features_list_check[:12]\n",
        "\n",
        "                if not features_for_boxplot:\n",
        "                    print(\"[WARN] No features selected for outlier boxplots.\")\n",
        "                else:\n",
        "                    num_box_plots = len(features_for_boxplot)\n",
        "                    cols_box = 4\n",
        "                    rows_box = (num_box_plots + cols_box - 1) // cols_box\n",
        "                    fig_box, axes_box = plt.subplots(rows_box, cols_box, figsize=(4.5 * cols_box, 3.5 * rows_box), squeeze=False)\n",
        "                    axes_box = axes_box.flatten()\n",
        "                    plot_count_box = 0\n",
        "\n",
        "                    for idx, feature in enumerate(features_for_boxplot):\n",
        "                        ax = axes_box[idx]\n",
        "                        if feature not in eda_data.columns:\n",
        "                            continue\n",
        "\n",
        "                        all_data = eda_data[feature].dropna()\n",
        "                        non_outlier_data = eda_data.loc[~eda_data.index.isin(outlier_indices), feature].dropna()\n",
        "                        outlier_data = eda_data.loc[outlier_indices, feature].dropna()\n",
        "\n",
        "                        if not non_outlier_data.empty:\n",
        "                            bp = ax.boxplot(non_outlier_data, positions=[0], widths=0.6, patch_artist=True, showfliers=False,\n",
        "                                   boxprops=dict(facecolor='lightblue', alpha=0.8),\n",
        "                                   medianprops=dict(color='red', linewidth=2))\n",
        "                        else:\n",
        "                            ax.text(0.5, 0.5, \"No non-outlier data\", ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
        "\n",
        "                        if not outlier_data.empty:\n",
        "                            jitter = np.random.normal(0, 0.04, size=len(outlier_data))\n",
        "                            ax.scatter(jitter, outlier_data,\n",
        "                                       color='red', s=40, alpha=0.9, label=f'Outliers (IQR) ({len(outlier_data)})', zorder=3, marker='x')\n",
        "\n",
        "                        ax.set_xticks([])\n",
        "                        ax.set_ylabel(feature, fontsize=9)\n",
        "                        ax.set_title(f'{feature}', fontsize=10, fontweight='bold')\n",
        "                        ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "                        if not outlier_data.empty:\n",
        "                            ax.legend(loc='best', fontsize=7)\n",
        "                        plot_count_box += 1\n",
        "\n",
        "                    for i in range(plot_count_box, len(axes_box)):\n",
        "                        axes_box[i].axis('off')\n",
        "\n",
        "                    if plot_count_box > 0:\n",
        "                        fig_box.suptitle('Distribution of Key Features Highlighting Outliers (IQR Method)', fontsize=14, fontweight='bold', y=1.02)\n",
        "                        plt.tight_layout(pad=2.0)\n",
        "                        savefig_report(\"10c_outlier_boxplots_IQR.png\", also_png=True)\n",
        "                        print(\"[OK] Outlier boxplots (IQR) visualization saved.\")\n",
        "                    else:\n",
        "                        plt.close(fig_box)\n",
        "\n",
        "            else:\n",
        "                print(\"[INFO] No outliers detected with IQR method. Skipping detailed outlier analysis and plots.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"[WARN] Could not import required libraries for IQR (e.g., numpy/pandas). Skipping Outlier detection.\")\n",
        "except Exception as e_outlier:\n",
        "    print(f\"[ERROR] Outlier analysis (IQR) failed: {e_outlier}\")\n",
        "    try:\n",
        "        plt.close(fig_box)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEMPORAL & SEASONALITY ANALYSIS\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 4.7: TEMPORAL & SEASONALITY ANALYSIS (TRAIN SET ONLY)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "try:\n",
        "    if 'Season' not in eda_data.columns:\n",
        "        if 'MatchDate' in eda_data.columns:\n",
        "            eda_data['Season'] = eda_data['MatchDate'].dt.year\n",
        "            print(\"[INFO] 'Season' column (year) created from MatchDate for temporal analysis.\")\n",
        "        else:\n",
        "            print(\"[WARN] 'MatchDate' column not found. Cannot create 'Season' column. Skipping temporal analysis.\")\n",
        "            raise ValueError(\"'MatchDate' column missing.\")\n",
        "\n",
        "    agg_dict_temporal = {\n",
        "        'Target': ['count', lambda x: (x == 1).sum() / len(x) * 100 if len(x) > 0 else 0]\n",
        "    }\n",
        "    if 'HomeElo' in eda_data.columns: agg_dict_temporal['HomeElo'] = 'mean'\n",
        "    if 'AwayElo' in eda_data.columns: agg_dict_temporal['AwayElo'] = 'mean'\n",
        "\n",
        "    # üÜï Temporal averages i√ßin analiz\n",
        "    if 'Home_Target_Avg_L5' in eda_data.columns: agg_dict_temporal['Home_Target_Avg_L5'] = 'mean'\n",
        "    if 'Away_Target_Avg_L5' in eda_data.columns: agg_dict_temporal['Away_Target_Avg_L5'] = 'mean'\n",
        "\n",
        "    print(\"[INFO] Calculating temporal trends by season (year)...\")\n",
        "    seasonal_trends = eda_data.groupby('Season').agg(agg_dict_temporal).round(2)\n",
        "\n",
        "    new_cols = []\n",
        "    for col in seasonal_trends.columns:\n",
        "        if isinstance(col, tuple):\n",
        "            if col[1] == '<lambda_0>': new_cols.append('Home_Win_Pct')\n",
        "            elif col[1] == 'count': new_cols.append('Match_Count')\n",
        "            else: new_cols.append(f\"{col[0]}_{col[1].capitalize()}\")\n",
        "        else:\n",
        "            new_cols.append(col)\n",
        "    seasonal_trends.columns = new_cols\n",
        "\n",
        "    print(f\"\\n[OK] Temporal trends by season (year) for Train Set:\")\n",
        "    print(seasonal_trends)\n",
        "\n",
        "    seasonal_trends.to_csv(os.path.join(CSV_DIR, \"11a_temporal_trends_by_season_train.csv\"))\n",
        "    print(\"[OK] Seasonal trends saved to CSV.\")\n",
        "\n",
        "    print(\"[INFO] Creating temporal analysis visualization (Train Set)...\")\n",
        "    fig_temp, axes_temp = plt.subplots(2, 2, figsize=(15, 11))\n",
        "    axes_temp = axes_temp.flatten()\n",
        "    plotted_temp_any = False\n",
        "\n",
        "    # Plot 1: ELO Trends\n",
        "    ax = axes_temp[0]\n",
        "    plotted_elo = False\n",
        "    if 'HomeElo_Mean' in seasonal_trends.columns:\n",
        "        ax.plot(seasonal_trends.index, seasonal_trends['HomeElo_Mean'],\n",
        "                marker='o', label='Avg Home Elo', linewidth=2, markersize=6, color=UNIFIED_COLORS['main_blue'])\n",
        "        plotted_elo = True\n",
        "    if 'AwayElo_Mean' in seasonal_trends.columns:\n",
        "        ax.plot(seasonal_trends.index, seasonal_trends['AwayElo_Mean'],\n",
        "                marker='s', label='Avg Away Elo', linewidth=2, markersize=6, color='#e74c3c')\n",
        "        plotted_elo = True\n",
        "    if plotted_elo:\n",
        "        ax.set_ylabel('Average Elo Rating', fontweight='bold')\n",
        "        ax.set_title('ELO Rating Trend ', fontsize=12, fontweight='bold')\n",
        "        ax.legend(loc='best')\n",
        "        ax.grid(alpha=0.4, linestyle='--')\n",
        "        ax.set_xlabel('Season (Year)')\n",
        "        plotted_temp_any = True\n",
        "    else: ax.text(0.5, 0.5, 'ELO data not available', ha='center', va='center', transform=ax.transAxes); ax.set_title('ELO Trend')\n",
        "\n",
        "    # Plot 2: Home Advantage Trend\n",
        "    ax = axes_temp[1]\n",
        "    if 'Home_Win_Pct' in seasonal_trends.columns:\n",
        "        bars_ha = ax.bar(seasonal_trends.index, seasonal_trends['Home_Win_Pct'],\n",
        "                         color=UNIFIED_COLORS['main_blue'], alpha=0.9, edgecolor='black', linewidth=1)\n",
        "        ax.set_ylabel('Home Win %', fontweight='bold')\n",
        "        ax.set_title('Home Advantage Trend ', fontsize=12, fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "        ax.set_xlabel('Season (Year)')\n",
        "        min_hw_trend = seasonal_trends['Home_Win_Pct'].min()\n",
        "        max_hw_trend = seasonal_trends['Home_Win_Pct'].max()\n",
        "        ax.set_ylim([max(0, np.floor(min_hw_trend / 5)*5 - 2), min(100, np.ceil(max_hw_trend / 5)*5 + 2)])\n",
        "        plotted_temp_any = True\n",
        "    else: ax.text(0.5, 0.5, 'Home Win % data not available', ha='center', va='center', transform=ax.transAxes); ax.set_title('Home Advantage Trend')\n",
        "\n",
        "    # Plot 3: Temporal Features Trends (NEW!)\n",
        "    ax = axes_temp[2]\n",
        "    plotted_temporal = False\n",
        "    if 'Home_Target_Avg_L5_Mean' in seasonal_trends.columns:\n",
        "        ax.plot(seasonal_trends.index, seasonal_trends['Home_Target_Avg_L5_Mean'],\n",
        "                marker='o', label='Avg Home Target (L5)', linewidth=2, markersize=6, color='#2ecc71')\n",
        "        plotted_temporal = True\n",
        "    if 'Away_Target_Avg_L5_Mean' in seasonal_trends.columns:\n",
        "        ax.plot(seasonal_trends.index, seasonal_trends['Away_Target_Avg_L5_Mean'],\n",
        "                marker='s', label='Avg Away Target (L5)', linewidth=2, markersize=6, color='#f39c12')\n",
        "        plotted_temporal = True\n",
        "    if plotted_temporal:\n",
        "        ax.set_ylabel('Average Shots on Target (L5)', fontweight='bold')\n",
        "        ax.set_title('Temporal Features Trend', fontsize=12, fontweight='bold')\n",
        "        ax.legend(loc='best')\n",
        "        ax.grid(alpha=0.4, linestyle='--')\n",
        "        ax.set_xlabel('Season (Year)')\n",
        "        plotted_temp_any = True\n",
        "    else: ax.text(0.5, 0.5, 'Temporal feature data not available', ha='center', va='center', transform=ax.transAxes); ax.set_title('Temporal Features Trend')\n",
        "\n",
        "    # Plot 4: Data Availability\n",
        "    try:\n",
        "        ax = axes_temp[3]\n",
        "        if 'Match_Count' in seasonal_trends.columns:\n",
        "            ax.bar(seasonal_trends.index, seasonal_trends['Match_Count'],\n",
        "                   color='#95a5a6', alpha=0.7, edgecolor='black', linewidth=1)\n",
        "            ax.set_ylabel('Number of Matches', fontweight='bold')\n",
        "            ax.set_title('Data Availability by Season', fontsize=12, fontweight='bold')\n",
        "            ax.grid(axis='y', alpha=0.4, linestyle='--')\n",
        "            ax.set_xlabel('Season (Year)')\n",
        "            plotted_temp_any = True\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, 'Match count data not available', ha='center', va='center',\n",
        "                    transform=ax.transAxes)\n",
        "            ax.set_title('Data Availability')\n",
        "\n",
        "        if plotted_temp_any:\n",
        "            plt.tight_layout()\n",
        "            savefig_report(\"11b_temporal_analysis_4panel_train.png\", also_png=True)\n",
        "            print(\"[OK] Temporal analysis visualization saved.\")\n",
        "        else:\n",
        "            plt.close(fig_temp)\n",
        "\n",
        "    except Exception as e_plot_4:\n",
        "        print(f\"[WARN] Failed during plot 4 or saving: {e_plot_4}\")\n",
        "        try:\n",
        "            plt.close(fig_temp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "except Exception as e_temporal:\n",
        "    print(f\"[ERROR] Temporal analysis failed: {e_temporal}\")\n",
        "    try:\n",
        "        plt.close(fig_temp)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CHI-SQUARE TEST\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Performing Chi-Square Test for Categorical Features (TRAIN SET ONLY)\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "chi_square_df = pd.DataFrame()\n",
        "chi_square_validation_df = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    if 'Target' not in eda_data.columns:\n",
        "        print(\"[WARN] Target column missing. Skipping Chi-Square test.\")\n",
        "    else:\n",
        "        categorical_features_all = eda_data.select_dtypes(include=['category', 'object']).columns.tolist()\n",
        "\n",
        "        exclude_features_chi = [\n",
        "            'HomeTeam', 'AwayTeam',\n",
        "            'MatchDate',\n",
        "            'YearMonth',\n",
        "            'Season',\n",
        "            'FTResult',\n",
        "            'Target',\n",
        "            'HTResult'\n",
        "        ]\n",
        "        if 'Referee' in categorical_features_all: exclude_features_chi.append('Referee')\n",
        "        for col in ['HomeTeam_ManagerName', 'AwayTeam_ManagerName']:\n",
        "            if col in categorical_features_all: exclude_features_chi.append(col)\n",
        "\n",
        "        categorical_features = [f for f in categorical_features_all if f not in exclude_features_chi]\n",
        "\n",
        "        if 'Target' in categorical_features: categorical_features.remove('Target')\n",
        "        if 'FTResult' in categorical_features: categorical_features.remove('FTResult')\n",
        "\n",
        "        if not categorical_features:\n",
        "            print(\"[INFO] No suitable categorical features found for Chi-Square test after exclusions.\")\n",
        "        else:\n",
        "            print(f\"[INFO] Testing {len(categorical_features)} categorical features against Target:\")\n",
        "            print(f\"         {', '.join(categorical_features)}\\n\")\n",
        "\n",
        "            chi_square_results = []\n",
        "            chi_square_validation = []\n",
        "\n",
        "            bonferroni_alpha = 0.05 / len(categorical_features)\n",
        "            print(f\"[INFO] Multiple Comparison Correction:\")\n",
        "            print(f\"         Original Œ± = 0.05\")\n",
        "            print(f\"         Bonferroni Adjusted Œ± = {bonferroni_alpha:.6f} ({len(categorical_features)} tests)\\n\")\n",
        "\n",
        "            for feature in categorical_features:\n",
        "                if eda_data[feature].nunique() > 50:\n",
        "                    print(f\"  ‚úó {feature}: Skipped (High Cardinality > 50 uniques)\")\n",
        "                    continue\n",
        "                try:\n",
        "                    ct = pd.crosstab(eda_data[feature], eda_data['Target'])\n",
        "                    if ct.empty or ct.shape[0] < 2 or ct.shape[1] < 2:\n",
        "                        print(f\"  ‚úó {feature}: Skipped (Insufficient data for cross-tabulation)\")\n",
        "                        continue\n",
        "\n",
        "                    chi2, p_value, dof, expected = chi2_contingency(ct)\n",
        "\n",
        "                    valid_test = True\n",
        "                    warning_msg = \"\"\n",
        "                    if expected.min() < 1:\n",
        "                        valid_test = False\n",
        "                        warning_msg = \"‚ö† (Expected < 1)\"\n",
        "                    elif (expected < 5).sum() / expected.size > 0.2:\n",
        "                        valid_test = False\n",
        "                        warning_msg = \"‚ö† (>20% Expected < 5)\"\n",
        "\n",
        "                    significant_standard = p_value < 0.05\n",
        "                    significant_bonferroni = p_value < bonferroni_alpha\n",
        "\n",
        "                    result = {\n",
        "                        'Feature': feature,\n",
        "                        'Chi2': chi2,\n",
        "                        'p-value': p_value,\n",
        "                        'df': dof,\n",
        "                        'Significant (Œ±=0.05)': 'Yes' if significant_standard else 'No',\n",
        "                        'Significant (Bonferroni)': 'Yes' if significant_bonferroni else 'No',\n",
        "                        'Test_Valid': 'Yes' if valid_test else 'No'\n",
        "                    }\n",
        "                    chi_square_results.append(result)\n",
        "\n",
        "                    validation = {\n",
        "                        'Feature': feature,\n",
        "                        'Min_Expected_Freq': expected.min(),\n",
        "                        'Pct_Expected_LT5': (expected < 5).sum() / expected.size * 100,\n",
        "                        'Valid': 'Yes' if valid_test else 'No'\n",
        "                    }\n",
        "                    chi_square_validation.append(validation)\n",
        "\n",
        "                    status_icon = \"‚úì\" if significant_standard else \"‚úó\"\n",
        "                    print(f\"  {status_icon} {feature}: Chi2={chi2:.2f}, p={p_value:.4f}, Valid={valid_test} {warning_msg}\")\n",
        "\n",
        "                except Exception as e_chi:\n",
        "                    print(f\"  [ERROR] Chi-Square failed for {feature}: {str(e_chi)}\")\n",
        "\n",
        "            if chi_square_results:\n",
        "                chi_square_df = pd.DataFrame(chi_square_results)\n",
        "                chi_square_validation_df = pd.DataFrame(chi_square_validation)\n",
        "\n",
        "                chi_square_df.round(4).to_csv(os.path.join(CSV_DIR, \"02a_chi_square_results_train.csv\"), index=False)\n",
        "                chi_square_validation_df.round(2).to_csv(os.path.join(CSV_DIR, \"02b_chi_square_validation_train.csv\"), index=False)\n",
        "                print(\"\\n[OK] Chi-Square results and validation saved to CSV.\")\n",
        "\n",
        "                save_df_as_image(chi_square_df.round(4), \"02a_chi_square_results_table.png\",\n",
        "                                     title='Table 2: Chi-Square Test of Independence ')\n",
        "                save_df_as_image(chi_square_validation_df.round(2), \"02b_chi_square_validation_table.png\",\n",
        "                                     title='Chi-Square Assumption Validation ')\n",
        "\n",
        "                fig_chi_bars, axes_chi_bars = plt.subplots(1, 2, figsize=(16, max(6, 0.4 * len(chi_square_df))))\n",
        "                colors_chi = ['#2ecc71' if row['Significant (Œ±=0.05)'] == 'Yes' else '#95a5a6' for _, row in chi_square_df.iterrows()]\n",
        "                bars_chi_val = axes_chi_bars[0].barh(chi_square_df['Feature'], chi_square_df['Chi2'], color=colors_chi)\n",
        "                axes_chi_bars[0].set_xlabel('Chi-Square Statistic', fontweight='bold')\n",
        "                axes_chi_bars[0].set_title('Chi-Square Test Statistic ', fontweight='bold', fontsize=12)\n",
        "                axes_chi_bars[0].invert_yaxis()\n",
        "                axes_chi_bars[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "                log_p_values = -np.log10(chi_square_df['p-value'].replace(0, 1e-300))\n",
        "                bars_chi_p = axes_chi_bars[1].barh(chi_square_df['Feature'], log_p_values, color=colors_chi)\n",
        "                axes_chi_bars[1].set_xlabel('-log10(p-value)', fontweight='bold')\n",
        "                axes_chi_bars[1].set_title('Statistical Significance ', fontweight='bold', fontsize=12)\n",
        "                axes_chi_bars[1].axvline(-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='Œ± = 0.05')\n",
        "                axes_chi_bars[1].axvline(-np.log10(bonferroni_alpha), color='orange', linestyle=':', linewidth=2, label=f'Bonferroni Œ± ({bonferroni_alpha:.2E})')\n",
        "                axes_chi_bars[1].invert_yaxis()\n",
        "                axes_chi_bars[1].grid(axis='x', alpha=0.3)\n",
        "                axes_chi_bars[1].legend()\n",
        "                plt.tight_layout()\n",
        "                savefig_report(\"12_chi_square_test_bars.png\", also_png=True)\n",
        "                print(\"[OK] Chi-Square results visualized.\")\n",
        "\n",
        "            else:\n",
        "                print(\"[INFO] No Chi-Square tests were successfully performed.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"[WARN] Could not import chi2_contingency from scipy.stats. Skipping Chi-Square test.\")\n",
        "except Exception as e_chisq_main:\n",
        "    print(f\"[ERROR] Chi-Square analysis failed: {e_chisq_main}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA QUALITY ANALYSIS (Missing Values)\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] Performing Data Quality Analysis (Missing Values, TRAIN SET ONLY)...\")\n",
        "\n",
        "try:\n",
        "    if not numeric_cols:\n",
        "         print(\"[WARN] No numeric columns to check for missing values. Skipping.\")\n",
        "    else:\n",
        "        missing_pct = (eda_data[numeric_cols].isnull().sum() / len(eda_data) * 100).sort_values(ascending=False)\n",
        "        missing_pct = missing_pct[missing_pct > 0]\n",
        "\n",
        "        if missing_pct.empty:\n",
        "            print(\"[OK] No missing values found in the selected numeric features for the training set.\")\n",
        "        else:\n",
        "            print(f\"[WARN] Found {len(missing_pct)} numeric features with missing values in the training set (Top 15):\")\n",
        "            print(missing_pct.head(15))\n",
        "\n",
        "            fig_missing, ax_missing = plt.subplots(figsize=(12, 8))\n",
        "            n_show_missing = min(20, len(missing_pct))\n",
        "            missing_data_plot = missing_pct.head(n_show_missing)\n",
        "\n",
        "            colors_quality = ['#e74c3c' if x > 10 else '#f39c12' if x > 5 else '#2ecc71'\n",
        "                              for x in missing_data_plot.values]\n",
        "\n",
        "            bars_missing = ax_missing.barh(range(n_show_missing), missing_data_plot.values,\n",
        "                                       color=colors_quality, height=0.7, edgecolor='black')\n",
        "            ax_missing.set_yticks(range(n_show_missing))\n",
        "            ax_missing.set_yticklabels(missing_data_plot.index, fontsize=10)\n",
        "            ax_missing.invert_yaxis()\n",
        "            ax_missing.set_xlabel('Percentage of Missing Values (%)', fontweight='bold', fontsize=12)\n",
        "            ax_missing.set_title(f'Data Quality - Top {n_show_missing} Numeric Features with Missing Values',\n",
        "                                 fontweight='bold', fontsize=12, pad=15)\n",
        "            max_missing = missing_data_plot.max()\n",
        "            ax_missing.set_xlim([0, max(10, max_missing * 1.1)])\n",
        "            ax_missing.grid(axis='x', alpha=0.3)\n",
        "\n",
        "            for i, (bar, val) in enumerate(zip(bars_missing, missing_data_plot.values)):\n",
        "                 ax_missing.text(val + max(1, max_missing*0.01), i, f'{val:.1f}%', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            savefig_report(\"13_data_quality_missing_values.png\", also_png=True)\n",
        "            print(\"[OK] Missing values analysis plot saved.\")\n",
        "\n",
        "            missing_pct.to_csv(os.path.join(CSV_DIR, \"missing_values_train.csv\"))\n",
        "            print(\"[OK] Missing values percentages saved to CSV.\")\n",
        "\n",
        "except Exception as e_missing:\n",
        "    print(f\"[ERROR] Missing value analysis failed: {e_missing}\")\n",
        "    try: plt.close(fig_missing)\n",
        "    except: pass\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 5: SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n[INFO] PHASE 5: GENERATING EDA SUMMARY (Based on Train Set Analysis)\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "try:\n",
        "    train_n = len(train_matches) if 'train_matches' in locals() else 'N/A'\n",
        "    val_n = len(val_matches) if 'val_matches' in locals() else 'N/A'\n",
        "    test_n = len(test_matches) if 'test_matches' in locals() else 'N/A'\n",
        "    total_n = len(matches_all) if 'matches_all' in locals() else 'N/A'\n",
        "\n",
        "    target_dist_summary = \"N/A\"\n",
        "    if 'vc' in locals() and isinstance(vc, pd.Series):\n",
        "        target_dist_summary = f\"Home Win: {vc.get(1,0):.1f}%, Draw: {vc.get(0,0):.1f}%, Away Win: {vc.get(2,0):.1f}%\"\n",
        "\n",
        "    num_feat_summary = len(numeric_cols) if 'numeric_cols' in locals() else 'N/A'\n",
        "    key_feat_summary = len(key_features) if 'key_features' in locals() else 'N/A'\n",
        "\n",
        "    # üÜï Temporal features count\n",
        "    temporal_feat_summary = len(temporal_features_created) if 'temporal_features_created' in locals() else 0\n",
        "\n",
        "    top_feature_1, top_feature_2, top_feature_3 = \"N/A\", \"N/A\", \"N/A\"\n",
        "    if 'mi_scores' in locals() and isinstance(mi_scores, pd.Series) and not mi_scores.empty and len(mi_scores) >= 3:\n",
        "        top_feature_1 = f\"{mi_scores.index[0]} (MI: {mi_scores.values[0]:.3f})\"\n",
        "        top_feature_2 = f\"{mi_scores.index[1]} (MI: {mi_scores.values[1]:.3f})\"\n",
        "        top_feature_3 = f\"{mi_scores.index[2]} (MI: {mi_scores.values[2]:.3f})\"\n",
        "\n",
        "    avg_pre_summary = f\"{avg_pre:.1f}%\" if 'avg_pre' in locals() and pd.notna(avg_pre) else \"N/A\"\n",
        "    avg_pandemic_summary = \"N/A\"\n",
        "    avg_post_summary = \"N/A\"\n",
        "    pandemic_impact_str = \"\"\n",
        "    post_impact_str = \"\"\n",
        "    if 'avg_pandemic' in locals() and pd.notna(avg_pandemic):\n",
        "        avg_pandemic_summary = f\"{avg_pandemic:.1f}%\"\n",
        "        if 'avg_pre' in locals() and pd.notna(avg_pre):\n",
        "            pandemic_impact_str = f\" ({avg_pandemic - avg_pre:+.1f}pp vs Pre)\"\n",
        "    if 'avg_post' in locals() and pd.notna(avg_post):\n",
        "        avg_post_summary = f\"{avg_post:.1f}%\"\n",
        "        if 'avg_pre' in locals() and pd.notna(avg_pre):\n",
        "            post_impact_str = f\" ({avg_post - avg_pre:+.1f}pp vs Pre)\"\n",
        "\n",
        "    cat_feat_summary = ', '.join(categorical_features) if 'categorical_features' in locals() and categorical_features else 'None'\n",
        "    sig_std_summary = \"N/A\"\n",
        "    sig_bonf_summary = \"N/A\"\n",
        "    bonf_alpha_summary = \"N/A\"\n",
        "    if 'chi_square_df' in locals() and isinstance(chi_square_df, pd.DataFrame) and not chi_square_df.empty:\n",
        "        sig_std_list = chi_square_df[chi_square_df['Significant (Œ±=0.05)'] == 'Yes']['Feature'].tolist()\n",
        "        sig_std_summary = ', '.join(sig_std_list) if sig_std_list else 'None'\n",
        "        sig_bonf_list = chi_square_df[chi_square_df['Significant (Bonferroni)'] == 'Yes']['Feature'].tolist()\n",
        "        sig_bonf_summary = ', '.join(sig_bonf_list) if sig_bonf_list else 'None'\n",
        "        if 'bonferroni_alpha' in locals():\n",
        "            bonf_alpha_summary = f\"{bonferroni_alpha:.4f}\"\n",
        "\n",
        "    high_corr_summary = \"N/A\"\n",
        "    features_dropped_count = 0\n",
        "    if 'high_corr_df' in locals() and isinstance(high_corr_df, pd.DataFrame) and not high_corr_df.empty:\n",
        "        high_corr_summary = f\"{len(high_corr_df)} pairs found (see '09_high_correlation_pairs.csv')\"\n",
        "        if 'features_to_drop_list' in locals():\n",
        "            features_dropped_count = len(features_to_drop_list)\n",
        "\n",
        "    tsne_summary = \"Suggests non-linear separation needed\"\n",
        "\n",
        "    summary_insights = f\"\"\"\n",
        "    =============================================================\n",
        "    EDA SUMMARY - V5.0 (DATA LEAKAGE CLEANED + TEMPORAL FEATURES)\n",
        "    =============================================================\n",
        "\n",
        "    üöÄ MAJOR CHANGES IN V5.0:\n",
        "       ‚úÖ ALL match-event features converted to temporal averages (L3/L5)\n",
        "       ‚úÖ Data leakage completely eliminated\n",
        "       ‚úÖ {temporal_feat_summary} temporal features created\n",
        "       ‚úÖ {features_dropped_count} features identified for removal (multicollinearity)\n",
        "\n",
        "    1. DATASET OVERVIEW\n",
        "       - Total Processed Matches (2015-Present): {total_n:,}\n",
        "       - Training Set (Used for this EDA): {train_n:,} matches\n",
        "       - Validation Set (Pandemic Gap Excluded): {val_n:,} matches\n",
        "       - Test Set (Most Recent): {test_n:,} matches\n",
        "       - Leagues Analyzed: 5 (PL, Bundesliga, Serie A, La Liga, Ligue 1)\n",
        "\n",
        "    2. TARGET DISTRIBUTION (Train Set)\n",
        "       - {target_dist_summary}\n",
        "       - Imbalance observed: Home wins are most frequent.\n",
        "\n",
        "    3. DATA QUALITY (Train Set)\n",
        "       - Duplicates: Checked and likely 0 after processing.\n",
        "       - Missing Values: Analyzed in numeric features (see '13_data_quality...' plot/CSV).\n",
        "\n",
        "    4. FEATURE ANALYSIS (Train Set)\n",
        "       - Total Numeric Features Analyzed: {num_feat_summary}\n",
        "       - Key Features Examined: {key_feat_summary}\n",
        "       - Temporal Features Created: {temporal_feat_summary}\n",
        "\n",
        "       üìä Top 3 Features by Relevance (Mutual Information - LEAKAGE-FREE):\n",
        "         1. {top_feature_1}\n",
        "         2. {top_feature_2}\n",
        "         3. {top_feature_3}\n",
        "\n",
        "       ‚ö†Ô∏è  CRITICAL - MULTICOLLINEARITY DETECTED:\n",
        "         * High correlations (>0.85): {high_corr_summary}\n",
        "         * Features to DROP: {features_dropped_count} (see '09b_features_to_drop_code.py')\n",
        "         * MANDATORY: Apply these removals BEFORE modeling\n",
        "         * Rationale: Redundant normalized probabilities, Max vs Odd duplicates\n",
        "\n",
        "       ‚úÖ DATA LEAKAGE PREVENTION:\n",
        "         * Original match-event features (HomeTarget, HomeShots, etc.) ‚Üí REMOVED\n",
        "         * Replaced with: Historical averages (L3, L5 windows)\n",
        "         * Example: HomeTarget ‚Üí Home_Target_Avg_L5\n",
        "         * This ensures NO future information leaks into predictions\n",
        "\n",
        "       - Outliers: Detected via IQR (see '10a/b/c...' plots/CSVs).\n",
        "         Strategy: RobustScaler + log transformation for skewed features recommended.\n",
        "\n",
        "    5. PANDEMIC IMPACT (Full Data Context)\n",
        "       - Average Home Win %:\n",
        "         - Pre-Pandemic: {avg_pre_summary}\n",
        "         - During Pandemic: {avg_pandemic_summary}{pandemic_impact_str}\n",
        "         - Post-Pandemic: {avg_post_summary}{post_impact_str}\n",
        "       - Conclusion: Pandemic significantly reduced home advantage, partially recovered post-pandemic.\n",
        "\n",
        "    6. TEMPORAL TRENDS (Train Set)\n",
        "       - ELO: Slight upward trend over seasons (see '11b...' plot).\n",
        "       - Home Advantage: Fluctuates seasonally (see '11b...' plot).\n",
        "       - Temporal Features: Stable patterns in L5 averages (see '11b...' plot).\n",
        "\n",
        "    7. CATEGORICAL FEATURES (Train Set vs Target)\n",
        "       - Features Tested: {cat_feat_summary}\n",
        "       - Significant Features (Œ±=0.05): {sig_std_summary}\n",
        "       - Significant Features (Bonferroni Œ±‚âà{bonf_alpha_summary}): {sig_bonf_summary}\n",
        "       - Validation: Chi-Square assumptions checked (see '02b...' CSV/Table).\n",
        "\n",
        "    8. DIMENSIONALITY & SEPARATION (Train Set)\n",
        "       - t-SNE Plot: {tsne_summary}. Classes appear highly overlapping.\n",
        "\n",
        "    9. üéØ CRITICAL ACTION ITEMS FOR MODELING\n",
        "\n",
        "       ‚≠ê PRIORITY 1: REMOVE MULTICOLLINEAR FEATURES\n",
        "       ```python\n",
        "       # Load the auto-generated drop list\n",
        "       exec(open('09b_features_to_drop_code.py').read())\n",
        "       X_train = X_train.drop(columns=features_to_drop_multicollinearity)\n",
        "       X_val = X_val.drop(columns=features_to_drop_multicollinearity)\n",
        "       X_test = X_test.drop(columns=features_to_drop_multicollinearity)\n",
        "       ```\n",
        "\n",
        "       ‚≠ê PRIORITY 2: VERIFY NO LEAKAGE FEATURES REMAIN\n",
        "       ```python\n",
        "       leakage_check = ['FTHome', 'FTAway', 'HTHome', 'HTAway',\n",
        "                        'HomeTarget', 'AwayTarget', 'HomeShots', 'AwayShots',\n",
        "                        'HomeCorners', 'AwayCorners', 'HomeFouls', 'AwayFouls']\n",
        "       assert not any(feat in X_train.columns for feat in leakage_check), \"LEAKAGE DETECTED!\"\n",
        "       ```\n",
        "\n",
        "       ‚≠ê PRIORITY 3: FEATURE SELECTION (MI-Based)\n",
        "       ```python\n",
        "       # Use features with MI > 0.04 (see '02_feature_relevance...' CSV)\n",
        "       selected_features = mi_scores[mi_scores > 0.04].index.tolist()\n",
        "       ```\n",
        "\n",
        "       ‚≠ê PRIORITY 4: HANDLE OUTLIERS & SKEWNESS\n",
        "       ```python\n",
        "       from sklearn.preprocessing import RobustScaler\n",
        "       import numpy as np\n",
        "\n",
        "       # Log transform skewed financial features\n",
        "       skewed_features = ['ClubValue', 'MaxPlayerValue', 'ManagerTrophies']\n",
        "       for feat in skewed_features:\n",
        "           for prefix in ['HomeTeam_', 'AwayTeam_']:\n",
        "               col = f'{{prefix}}{{feat}}'\n",
        "               if col in X_train.columns:\n",
        "                   X_train[f'{{col}}_log'] = np.log1p(X_train[col])\n",
        "\n",
        "       # Apply RobustScaler (IQR-based, outlier-resistant)\n",
        "       scaler = RobustScaler()\n",
        "       X_train_scaled = scaler.fit_transform(X_train)\n",
        "       ```\n",
        "\n",
        "       OTHER RECOMMENDATIONS:\n",
        "       - Address Class Imbalance: Use class_weight='balanced' (PREFERRED)\n",
        "       - SMOTE (if needed): Apply ONLY in modeling pipeline, NEVER in EDA\n",
        "       - Model Selection: Non-linear models (XGBoost, LGBM, RF) recommended\n",
        "       - Reason: EDA analyzes REAL data, SMOTE creates SYNTHETIC data\n",
        "       - Evaluation Metrics: F1-weighted, Kappa, Brier Score, RPS, ECE\n",
        "       - TEMPORAL SPLIT IS MANDATORY: NO random shuffling allowed\n",
        "\n",
        "    10. FILES GENERATED (V5.0)\n",
        "        - PNG Visualizations: ~27+ (in {IMAGES_DIR})\n",
        "        - CSV Data Exports: ~15+ (in {CSV_DIR})\n",
        "        - PDF Report(s): (in {COMPACT_DIR})\n",
        "        - Python Code: features_to_drop_multicollinearity list\n",
        "\n",
        "        **CRITICAL FILES FOR MODELING:**\n",
        "        1. ‚≠ê 09b_features_to_drop_code.py ‚Üí MANDATORY: Remove multicollinear features\n",
        "        2. ‚≠ê 02_feature_relevance_mutual_information_train.csv ‚Üí Feature selection guide\n",
        "        3. 09_high_correlation_pairs.csv ‚Üí Review correlation issues\n",
        "        4. 09c_multicollinearity_recommendations.csv ‚Üí Detailed removal rationale\n",
        "        5. 01_descriptive_statistics_train.csv ‚Üí Understand feature scales\n",
        "        6. 10a_outliers_by_feature_train_IQR.csv ‚Üí Confirm robust scaling needs\n",
        "\n",
        "    11. ‚úÖ VALIDATION CHECKLIST BEFORE MODELING:\n",
        "        ‚ñ° Loaded '09b_features_to_drop_code.py' and applied removals\n",
        "        ‚ñ° Verified NO leakage features (FTHome, HomeTarget, etc.) in X_train\n",
        "        ‚ñ° Confirmed temporal features (L3/L5 averages) are present\n",
        "        ‚ñ° Applied log transformation to skewed financial features\n",
        "        ‚ñ° Used RobustScaler for outlier-resistant scaling\n",
        "        ‚ñ° Selected top N features based on MI scores (>0.04 threshold)\n",
        "        ‚ñ° Used temporal split (NO random shuffling)\n",
        "        ‚ñ° Set appropriate class weights for imbalance\n",
        "    \"\"\"\n",
        "\n",
        "    print(summary_insights)\n",
        "\n",
        "    summary_path = os.path.join(OUT_DIR, \"eda_summary_v5_0_leakage_cleaned.txt\")\n",
        "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(summary_insights)\n",
        "    print(f\"\\n[OK] EDA Summary saved to: {summary_path}\")\n",
        "\n",
        "except Exception as e_summary:\n",
        "    print(f\"[ERROR] Failed to generate summary: {e_summary}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL CLEANUP\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"[DONE] FULL EDA COMPLETE - V5.0 (DATA LEAKAGE CLEANED + TEMPORAL FEATURES)\")\n",
        "print(\"Updates: All match-event features converted to temporal averages\")\n",
        "print(\"         Multicollinearity automatically detected and drop list generated\")\n",
        "print(\"         Complete leakage prevention implemented\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "try:\n",
        "    output_stats = {\n",
        "        \"PNG Images\": len(glob.glob(os.path.join(IMAGES_DIR, \"*.png\"))),\n",
        "        \"CSV Files\": len(glob.glob(os.path.join(CSV_DIR, \"*.csv\"))),\n",
        "        \"HTML Files\": len(glob.glob(os.path.join(IMAGES_DIR, \"*.html\"))),\n",
        "        \"PDF Files\": len(glob.glob(os.path.join(COMPACT_DIR, \"*.pdf\"))),\n",
        "        \"Text Files\": len(glob.glob(os.path.join(OUT_DIR, \"*.txt\"))),\n",
        "        \"Python Code Files\": len(glob.glob(os.path.join(CSV_DIR, \"*.py\")))\n",
        "    }\n",
        "    print(f\"\\n[FINAL OUTPUT STATS]\")\n",
        "    for key, value in output_stats.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(f\"\\nOutput directories:\")\n",
        "    print(f\"  - Images: {IMAGES_DIR}\")\n",
        "    print(f\"  - Data Exports: {CSV_DIR}\")\n",
        "    print(f\"  - PDF Reports: {COMPACT_DIR}\")\n",
        "    print(f\"  - Summary Text: {OUT_DIR}\\n\")\n",
        "except Exception as e_stats:\n",
        "    print(f\"[WARN] Could not gather final output stats: {e_stats}\")\n",
        "\n",
        "try:\n",
        "    _pdf.close()\n",
        "    print(\"[OK] Final PDF file closed successfully.\")\n",
        "except Exception as e_pdf_close:\n",
        "    print(f\"[WARN] PDF close error (might be already closed or empty): {e_pdf_close}\")\n",
        "\n",
        "SCRIPT_END = time.time()\n",
        "elapsed = SCRIPT_END - SCRIPT_START\n",
        "print(f\"\\n[TIMER] Total EDA execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
        "print(f\"[INFO] Script V5.1 ended at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# üÜï FINAL: EKLEME KONTROL√ú (V5.1 Validation)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìã EKLEME KONTROL√ú (V5.1 New Features)\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "checks = {\n",
        "    'SMOTE Available': SMOTE_AVAILABLE,\n",
        "    'VIF Available': VIF_AVAILABLE,\n",
        "    'Silhouette Available': SILHOUETTE_AVAILABLE,\n",
        "    'Temporal Validation': 'validation_results' in locals() and validation_results is not None,\n",
        "    'SMOTE Metrics': 'smote_metrics' in locals() and smote_metrics is not None,\n",
        "    'VIF Results': 'vif_results' in locals() and vif_results is not None,\n",
        "    't-SNE Results': 'tsne_results' in locals() and tsne_results is not None\n",
        "}\n",
        "\n",
        "print(\"\\n[K√úT√úPHANE DURUMU]\")\n",
        "for check_name in ['SMOTE Available', 'VIF Available', 'Silhouette Available']:\n",
        "    status = checks[check_name]\n",
        "    symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
        "    print(f\"  {symbol} {check_name:25s}: {status}\")\n",
        "\n",
        "print(\"\\n[ANALƒ∞Z SONU√áLARI]\")\n",
        "for check_name in ['Temporal Validation', 'SMOTE Metrics', 'VIF Results', 't-SNE Results']:\n",
        "    status = checks[check_name]\n",
        "    symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
        "    print(f\"  {symbol} {check_name:25s}: {status}\")\n",
        "\n",
        "# Detaylƒ± sonu√ßlar\n",
        "all_passed = all(checks.values())\n",
        "\n",
        "if all_passed:\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"üéâ T√úM EKLEMELER BA≈ûARIYLA √áALI≈ûTI!\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Ekstra detaylar\n",
        "    if 'validation_results' in locals() and validation_results:\n",
        "        print(f\"\\n[Temporal Validation Details]\")\n",
        "        print(f\"  ‚Ä¢ Leakage Detected: {validation_results.get('leakage_detected', 'N/A')}\")\n",
        "        print(f\"  ‚Ä¢ Min Coverage: {validation_results.get('min_coverage', 0):.1f}%\")\n",
        "        print(f\"  ‚Ä¢ Overall Pass: {validation_results.get('overall_pass', False)}\")\n",
        "\n",
        "    if 'smote_metrics' in locals() and smote_metrics:\n",
        "        print(f\"\\n[SMOTE Metrics Details]\")\n",
        "        print(f\"  ‚Ä¢ Success: {smote_metrics.get('smote_success', False)}\")\n",
        "        print(f\"  ‚Ä¢ Imbalance Before: {smote_metrics.get('imbalance_before', 0):.2f}x\")\n",
        "        print(f\"  ‚Ä¢ Imbalance After: {smote_metrics.get('imbalance_after', 0):.2f}x\")\n",
        "\n",
        "    if 'vif_results' in locals() and vif_results is not None:\n",
        "        print(f\"\\n[VIF Results Details]\")\n",
        "        n_high_vif = (vif_results['VIF'] >= 10).sum()\n",
        "        print(f\"  ‚Ä¢ Features Analyzed: {len(vif_results)}\")\n",
        "        print(f\"  ‚Ä¢ High VIF (‚â•10): {n_high_vif}\")\n",
        "        if 'vif_drop_list' in locals():\n",
        "            print(f\"  ‚Ä¢ Recommended Drops: {len(vif_drop_list)}\")\n",
        "\n",
        "    if 'tsne_results' in locals() and tsne_results:\n",
        "        print(f\"\\n[t-SNE Results Details]\")\n",
        "        print(f\"  ‚Ä¢ Embeddings Computed: {len(tsne_results.get('embeddings', {}))}\")\n",
        "        print(f\"  ‚Ä¢ Sample Size: {tsne_results.get('sample_size', 0):,}\")\n",
        "        print(f\"  ‚Ä¢ Features Used: {tsne_results.get('n_features', 0)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"‚ö†Ô∏è  BAZI EKLEMELER √áALI≈ûMADI - KONTROL GEREKLƒ∞\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    print(\"\\n[SORUN TESPƒ∞Tƒ∞]\")\n",
        "\n",
        "    if not checks['SMOTE Available']:\n",
        "        print(\"  ‚ùå SMOTE k√ºt√ºphanesi y√ºkl√º deƒüil\")\n",
        "        print(\"     ‚Üí √á√∂z√ºm: pip install imbalanced-learn\")\n",
        "\n",
        "    if not checks['VIF Available']:\n",
        "        print(\"  ‚ùå VIF (statsmodels) y√ºkl√º deƒüil\")\n",
        "        print(\"     ‚Üí √á√∂z√ºm: pip install statsmodels\")\n",
        "\n",
        "    if not checks['Silhouette Available']:\n",
        "        print(\"  ‚ùå Silhouette Score y√ºkl√º deƒüil\")\n",
        "        print(\"     ‚Üí √á√∂z√ºm: pip install scikit-learn (g√ºncel versiyon)\")\n",
        "\n",
        "    if not checks['Temporal Validation']:\n",
        "        print(\"  ‚ùå Temporal Validation √ßalƒ±≈ümadƒ±\")\n",
        "        print(\"     ‚Üí Kontrol: Phase 1.6 kodu doƒüru eklenmi≈ü mi?\")\n",
        "        print(\"     ‚Üí validation_results deƒüi≈ükeni olu≈üturulmu≈ü mu?\")\n",
        "\n",
        "    if not checks['SMOTE Metrics']:\n",
        "        print(\"  ‚ùå SMOTE Metrics √ßalƒ±≈ümadƒ±\")\n",
        "        print(\"     ‚Üí Kontrol: Phase 4.3 kodu doƒüru eklenmi≈ü mi?\")\n",
        "        print(\"     ‚Üí SMOTE_AVAILABLE = True mu?\")\n",
        "\n",
        "    if not checks['VIF Results']:\n",
        "        print(\"  ‚ùå VIF Results √ßalƒ±≈ümadƒ±\")\n",
        "        print(\"     ‚Üí Kontrol: Phase 4.4 kodu doƒüru eklenmi≈ü mi?\")\n",
        "        print(\"     ‚Üí VIF_AVAILABLE = True mu?\")\n",
        "        print(\"     ‚Üí numeric_cols listesi yeterli mi? (>5 √∂zellik gerekli)\")\n",
        "\n",
        "    if not checks['t-SNE Results']:\n",
        "        print(\"  ‚ùå t-SNE Results √ßalƒ±≈ümadƒ±\")\n",
        "        print(\"     ‚Üí Kontrol: t-SNE kodu doƒüru replace edilmi≈ü mi?\")\n",
        "        print(\"     ‚Üí Target column mevcut mu?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "\n",
        "# Dosya √ßƒ±ktƒ± kontrol√º\n",
        "try:\n",
        "    output_files = {\n",
        "        'PNG Images': len(glob.glob(os.path.join(IMAGES_DIR, \"*.png\"))),\n",
        "        'CSV Files': len(glob.glob(os.path.join(CSV_DIR, \"*.csv\"))),\n",
        "        'PDF Files': len(glob.glob(os.path.join(COMPACT_DIR, \"*.pdf\"))),\n",
        "        'Text Files': len(glob.glob(os.path.join(OUT_DIR, \"*.txt\")))\n",
        "    }\n",
        "\n",
        "    print(\"\\n[DOSYA √áIKTI ƒ∞STATƒ∞STƒ∞KLERƒ∞]\")\n",
        "    for file_type, count in output_files.items():\n",
        "        print(f\"  üìÅ {file_type:15s}: {count:3d} dosya\")\n",
        "\n",
        "    # Yeni eklemelere √∂zel dosyalar\n",
        "    print(\"\\n[YENƒ∞ EKLEME DOSYALARI (V5.1)]\")\n",
        "    new_files = {\n",
        "        '01d_temporal_features_validation.png': 'Temporal Validation',\n",
        "        '04c_class_imbalance_smote.png': 'SMOTE Analysis',\n",
        "        '09f_vif_analysis.png': 'VIF Analysis',\n",
        "        '09g_vif_results.csv': 'VIF Results CSV',\n",
        "        '11a_tsne_multiperplexity.png': 't-SNE Multi-Perplexity',\n",
        "        '12_feature_engineering_examples.png': 'Feature Engineering'\n",
        "    }\n",
        "\n",
        "    for filename, description in new_files.items():\n",
        "        # PNG i√ßin IMAGES_DIR, CSV i√ßin CSV_DIR kontrol et\n",
        "        if filename.endswith('.png'):\n",
        "            filepath = os.path.join(IMAGES_DIR, filename)\n",
        "        else:\n",
        "            filepath = os.path.join(CSV_DIR, filename)\n",
        "\n",
        "        exists = os.path.exists(filepath)\n",
        "        symbol = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "        print(f\"  {symbol} {description:30s}: {filename}\")\n",
        "\n",
        "except Exception as e_files:\n",
        "    print(f\"\\n[WARN] Dosya istatistikleri alƒ±namadƒ±: {e_files}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "plt.close('all')\n",
        "print(\"\\n[INFO] Cleanup complete. EDA V5.1 Finished.\")\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"üéâ THANK YOU FOR USING EDA V5.1!\")\n",
        "print(\"   ‚ú® New Features Added:\")\n",
        "print(\"      ‚Ä¢ Temporal Features Validation (Phase 1.6)\")\n",
        "print(\"      ‚Ä¢ SMOTE Class Imbalance Analysis (Phase 4.3)\")\n",
        "print(\"      ‚Ä¢ VIF Multicollinearity Detection (Phase 4.4)\")\n",
        "print(\"      ‚Ä¢ t-SNE Multi-Perplexity Comparison (Enhanced)\")\n",
        "print(\"      ‚Ä¢ Feature Engineering Examples (Phase 4.9)\")\n",
        "print(\"\\n   Next steps: Review generated files and apply recommendations to your modeling pipeline.\")\n",
        "print(\"=\"*90)\n"
      ]
    }
  ]
}