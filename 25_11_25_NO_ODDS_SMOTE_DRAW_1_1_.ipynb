{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5836afcf09f145a29f0bcf7405df84bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f03abe59360d4e7f8b35123620f7e2cb",
              "IPY_MODEL_bfaf3c5bc73f4713bd46656ccd5949bc",
              "IPY_MODEL_0c3af48d838b4a9d8d9c44044b070525"
            ],
            "layout": "IPY_MODEL_6c9291166d564d278d494b30c2304e6a"
          }
        },
        "f03abe59360d4e7f8b35123620f7e2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6561f4e8b2ea49059f6410e6655af57c",
            "placeholder": "​",
            "style": "IPY_MODEL_32084dc25a1d4c3691dbb090b838a5c8",
            "value": "Best trial: 19. Best value: 0.499167: 100%"
          }
        },
        "bfaf3c5bc73f4713bd46656ccd5949bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b81de39413541ddb2a9178c5a0043b2",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c09a49d121e24c808701597a0922dd01",
            "value": 20
          }
        },
        "0c3af48d838b4a9d8d9c44044b070525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78c4b522fd0946eda92460aedf45dc67",
            "placeholder": "​",
            "style": "IPY_MODEL_92cbae169592483fb34c02bcf66c3ef9",
            "value": " 20/20 [00:39&lt;00:00,  1.63s/it, 39.60/2000 seconds]"
          }
        },
        "6c9291166d564d278d494b30c2304e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6561f4e8b2ea49059f6410e6655af57c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32084dc25a1d4c3691dbb090b838a5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b81de39413541ddb2a9178c5a0043b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09a49d121e24c808701597a0922dd01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78c4b522fd0946eda92460aedf45dc67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cbae169592483fb34c02bcf66c3ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e280b1ed66c4b8ebd8905e0f1de0ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14d235be52794703b419159daff5b133",
              "IPY_MODEL_c81ac46e3a12479695fe25480c003e30",
              "IPY_MODEL_7fe75b79ee29452587f223258a5df6f2"
            ],
            "layout": "IPY_MODEL_fef5bf7440eb483bb1698517e061e6be"
          }
        },
        "14d235be52794703b419159daff5b133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aacdb50ff2cc4d92b69346bc9b07f699",
            "placeholder": "​",
            "style": "IPY_MODEL_df6986774b484079a9bb35bb1b886707",
            "value": "  0%"
          }
        },
        "c81ac46e3a12479695fe25480c003e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373b580f43644869b703a8e0c23dfc33",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccc0d634d650460c8a22340ecc6960b0",
            "value": 0
          }
        },
        "7fe75b79ee29452587f223258a5df6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c42c6f9c2fcb41b0abfad0be40e62339",
            "placeholder": "​",
            "style": "IPY_MODEL_dd8a826f4cc7477faaa96ddfbc582ac4",
            "value": " 0/20 [00:00&lt;?, ?it/s]"
          }
        },
        "fef5bf7440eb483bb1698517e061e6be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aacdb50ff2cc4d92b69346bc9b07f699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6986774b484079a9bb35bb1b886707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373b580f43644869b703a8e0c23dfc33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc0d634d650460c8a22340ecc6960b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c42c6f9c2fcb41b0abfad0be40e62339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd8a826f4cc7477faaa96ddfbc582ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6d16d071a374dbd9610883ef618db56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67bf6ceaca8147a5af9b2d1680359c8a",
              "IPY_MODEL_03e15d3aba684c04b58ac99de6265726",
              "IPY_MODEL_8af8e6f3450748429c0dc09f9f5b20b0"
            ],
            "layout": "IPY_MODEL_3d0763be2c7541ee881a05a1d1f3be2a"
          }
        },
        "67bf6ceaca8147a5af9b2d1680359c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4aee55ef9f46e884d513857dd1b8d8",
            "placeholder": "​",
            "style": "IPY_MODEL_a4c2b4b85f3a493d930faa7ca98f3c63",
            "value": "Best trial: 7. Best value: 0.495599:  80%"
          }
        },
        "03e15d3aba684c04b58ac99de6265726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1060aaf1d88c407aa00eacbdd4cf0149",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d92dd80965574906974c2c67fc4a780b",
            "value": 16
          }
        },
        "8af8e6f3450748429c0dc09f9f5b20b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b07befeb2854b87820d4c868225de61",
            "placeholder": "​",
            "style": "IPY_MODEL_00246fc326514998b16bdb282f1ec5a8",
            "value": " 16/20 [33:57&lt;06:29, 97.47s/it, 2037.76/2000 seconds]"
          }
        },
        "3d0763be2c7541ee881a05a1d1f3be2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4aee55ef9f46e884d513857dd1b8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c2b4b85f3a493d930faa7ca98f3c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1060aaf1d88c407aa00eacbdd4cf0149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d92dd80965574906974c2c67fc4a780b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b07befeb2854b87820d4c868225de61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00246fc326514998b16bdb282f1ec5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed7cd2c99ae346f5aae3192da5535b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c2e5f4be6ec49118f389f41cb390728",
              "IPY_MODEL_a418f08633674a91b0eaeba2dd8936a8",
              "IPY_MODEL_c8dfac42d50043c9b22dbcd08899a8a3"
            ],
            "layout": "IPY_MODEL_5d22d46c2d40412896e29a8658404094"
          }
        },
        "7c2e5f4be6ec49118f389f41cb390728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_604cd415e0a04cc8be4a72cffda9976f",
            "placeholder": "​",
            "style": "IPY_MODEL_2ba6b6d1d566480ea123a7bfc0e9519c",
            "value": "Best trial: 1. Best value: 0.501827:  70%"
          }
        },
        "a418f08633674a91b0eaeba2dd8936a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12ad4b45e9b468da561adf51301f61d",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e7408046f154bbe895a1130b1d4c673",
            "value": 14
          }
        },
        "c8dfac42d50043c9b22dbcd08899a8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_762da8a6f6854517bcea416d987f4d9b",
            "placeholder": "​",
            "style": "IPY_MODEL_85d071727fe04c0182e4df55b7588e6f",
            "value": " 14/20 [33:43&lt;12:15, 122.61s/it, 2023.78/2000 seconds]"
          }
        },
        "5d22d46c2d40412896e29a8658404094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "604cd415e0a04cc8be4a72cffda9976f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba6b6d1d566480ea123a7bfc0e9519c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f12ad4b45e9b468da561adf51301f61d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e7408046f154bbe895a1130b1d4c673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762da8a6f6854517bcea416d987f4d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d071727fe04c0182e4df55b7588e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "832340b1f04e41fdbb367a9cd7c48212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ec4ab3cdfff478c8d4f1d30ed19a871",
              "IPY_MODEL_8784938e44454b1aa4e269382bc9fdc4",
              "IPY_MODEL_dbd6220e5186469caa972147a7c53043"
            ],
            "layout": "IPY_MODEL_e2dd0a6962bd47ca8fa18f8902c2db1c"
          }
        },
        "9ec4ab3cdfff478c8d4f1d30ed19a871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b43c55326c114f3fb8bb07678129fa77",
            "placeholder": "​",
            "style": "IPY_MODEL_b8a7d460be5a47ad86f676b34930d6f0",
            "value": "Best trial: 4. Best value: 0.502871:  80%"
          }
        },
        "8784938e44454b1aa4e269382bc9fdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ede4d59b310e4fbba3a86276ea4d9e69",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fb7f6f6a3e94e739491a9a1288dce6d",
            "value": 16
          }
        },
        "dbd6220e5186469caa972147a7c53043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de66c92a03e24597bcc8051eeea8decf",
            "placeholder": "​",
            "style": "IPY_MODEL_01e9e68e47e44845aae58ded4b8a7541",
            "value": " 16/20 [35:34&lt;11:25, 171.32s/it, 2134.62/2000 seconds]"
          }
        },
        "e2dd0a6962bd47ca8fa18f8902c2db1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43c55326c114f3fb8bb07678129fa77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8a7d460be5a47ad86f676b34930d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ede4d59b310e4fbba3a86276ea4d9e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb7f6f6a3e94e739491a9a1288dce6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de66c92a03e24597bcc8051eeea8decf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01e9e68e47e44845aae58ded4b8a7541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bfec02e36d64860a9a52b5ad3379810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9a415b4271441a0b8f8eee7a21e60da",
              "IPY_MODEL_09f77909fa2740238662c0d3a00d2f96",
              "IPY_MODEL_10a9d56c97314ed080b02f06fae79f94"
            ],
            "layout": "IPY_MODEL_94b95c45ce924756b1c4162fcbb9ebec"
          }
        },
        "e9a415b4271441a0b8f8eee7a21e60da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5df7416cdb41f88dbb1250520d0747",
            "placeholder": "​",
            "style": "IPY_MODEL_c6bbb3d362b74d64abb480a2d8314c72",
            "value": "Best trial: 7. Best value: 0.450778:  55%"
          }
        },
        "09f77909fa2740238662c0d3a00d2f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae21cd784fd4bbe9535d1ceae8a4527",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6570c887666470fbb922664b84f9e77",
            "value": 11
          }
        },
        "10a9d56c97314ed080b02f06fae79f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cff2c3886da48208c2b226c82aea05a",
            "placeholder": "​",
            "style": "IPY_MODEL_3249d0bd5a274ecc8cd19b3d598a3f4a",
            "value": " 11/20 [41:34&lt;45:10, 301.17s/it, 2494.22/2000 seconds]"
          }
        },
        "94b95c45ce924756b1c4162fcbb9ebec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5df7416cdb41f88dbb1250520d0747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6bbb3d362b74d64abb480a2d8314c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dae21cd784fd4bbe9535d1ceae8a4527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6570c887666470fbb922664b84f9e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cff2c3886da48208c2b226c82aea05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3249d0bd5a274ecc8cd19b3d598a3f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff4bac4f0d084b4ea78c30ee16620f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93dccfe4d2024174ba960b59b7f286dd",
              "IPY_MODEL_813fb59a38df4d60b906ffde247e55f4",
              "IPY_MODEL_fa6cf4cbc54c49e190f98934b741e367"
            ],
            "layout": "IPY_MODEL_29e283fb46c84411bf74e14f29907897"
          }
        },
        "93dccfe4d2024174ba960b59b7f286dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22d833a771fc4df8aa7282d1368481ac",
            "placeholder": "​",
            "style": "IPY_MODEL_73f2c0d8f24849ffa0d2e3bb9ade9be9",
            "value": "100%"
          }
        },
        "813fb59a38df4d60b906ffde247e55f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e505017180874e719518c1e00a9cdce6",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20726c9e90e14838a68b51bf87a7f11c",
            "value": 100
          }
        },
        "fa6cf4cbc54c49e190f98934b741e367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e603d5e4dc4ddcb9323e9ed0c98ee1",
            "placeholder": "​",
            "style": "IPY_MODEL_6af27bd784bf49d1838e441d9ab44f8d",
            "value": " 100/100 [01:38&lt;00:00,  1.09it/s]"
          }
        },
        "29e283fb46c84411bf74e14f29907897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d833a771fc4df8aa7282d1368481ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f2c0d8f24849ffa0d2e3bb9ade9be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e505017180874e719518c1e00a9cdce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20726c9e90e14838a68b51bf87a7f11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4e603d5e4dc4ddcb9323e9ed0c98ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af27bd784bf49d1838e441d9ab44f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5836afcf09f145a29f0bcf7405df84bb",
            "f03abe59360d4e7f8b35123620f7e2cb",
            "bfaf3c5bc73f4713bd46656ccd5949bc",
            "0c3af48d838b4a9d8d9c44044b070525",
            "6c9291166d564d278d494b30c2304e6a",
            "6561f4e8b2ea49059f6410e6655af57c",
            "32084dc25a1d4c3691dbb090b838a5c8",
            "7b81de39413541ddb2a9178c5a0043b2",
            "c09a49d121e24c808701597a0922dd01",
            "78c4b522fd0946eda92460aedf45dc67",
            "92cbae169592483fb34c02bcf66c3ef9",
            "1e280b1ed66c4b8ebd8905e0f1de0ca9",
            "14d235be52794703b419159daff5b133",
            "c81ac46e3a12479695fe25480c003e30",
            "7fe75b79ee29452587f223258a5df6f2",
            "fef5bf7440eb483bb1698517e061e6be",
            "aacdb50ff2cc4d92b69346bc9b07f699",
            "df6986774b484079a9bb35bb1b886707",
            "373b580f43644869b703a8e0c23dfc33",
            "ccc0d634d650460c8a22340ecc6960b0",
            "c42c6f9c2fcb41b0abfad0be40e62339",
            "dd8a826f4cc7477faaa96ddfbc582ac4",
            "525a62d2b6ae4c76aa0b1ad97ad52b63",
            "e30a4e2c86aa4ab99abeb6929b8fb3a6",
            "a6d16d071a374dbd9610883ef618db56",
            "67bf6ceaca8147a5af9b2d1680359c8a",
            "03e15d3aba684c04b58ac99de6265726",
            "8af8e6f3450748429c0dc09f9f5b20b0",
            "3d0763be2c7541ee881a05a1d1f3be2a",
            "6f4aee55ef9f46e884d513857dd1b8d8",
            "a4c2b4b85f3a493d930faa7ca98f3c63",
            "1060aaf1d88c407aa00eacbdd4cf0149",
            "d92dd80965574906974c2c67fc4a780b",
            "4b07befeb2854b87820d4c868225de61",
            "00246fc326514998b16bdb282f1ec5a8",
            "ed7cd2c99ae346f5aae3192da5535b03",
            "7c2e5f4be6ec49118f389f41cb390728",
            "a418f08633674a91b0eaeba2dd8936a8",
            "c8dfac42d50043c9b22dbcd08899a8a3",
            "5d22d46c2d40412896e29a8658404094",
            "604cd415e0a04cc8be4a72cffda9976f",
            "2ba6b6d1d566480ea123a7bfc0e9519c",
            "f12ad4b45e9b468da561adf51301f61d",
            "1e7408046f154bbe895a1130b1d4c673",
            "762da8a6f6854517bcea416d987f4d9b",
            "85d071727fe04c0182e4df55b7588e6f",
            "832340b1f04e41fdbb367a9cd7c48212",
            "9ec4ab3cdfff478c8d4f1d30ed19a871",
            "8784938e44454b1aa4e269382bc9fdc4",
            "dbd6220e5186469caa972147a7c53043",
            "e2dd0a6962bd47ca8fa18f8902c2db1c",
            "b43c55326c114f3fb8bb07678129fa77",
            "b8a7d460be5a47ad86f676b34930d6f0",
            "ede4d59b310e4fbba3a86276ea4d9e69",
            "9fb7f6f6a3e94e739491a9a1288dce6d",
            "de66c92a03e24597bcc8051eeea8decf",
            "01e9e68e47e44845aae58ded4b8a7541",
            "4bfec02e36d64860a9a52b5ad3379810",
            "e9a415b4271441a0b8f8eee7a21e60da",
            "09f77909fa2740238662c0d3a00d2f96",
            "10a9d56c97314ed080b02f06fae79f94",
            "94b95c45ce924756b1c4162fcbb9ebec",
            "dc5df7416cdb41f88dbb1250520d0747",
            "c6bbb3d362b74d64abb480a2d8314c72",
            "dae21cd784fd4bbe9535d1ceae8a4527",
            "c6570c887666470fbb922664b84f9e77",
            "0cff2c3886da48208c2b226c82aea05a",
            "3249d0bd5a274ecc8cd19b3d598a3f4a",
            "ff4bac4f0d084b4ea78c30ee16620f14",
            "93dccfe4d2024174ba960b59b7f286dd",
            "813fb59a38df4d60b906ffde247e55f4",
            "fa6cf4cbc54c49e190f98934b741e367",
            "29e283fb46c84411bf74e14f29907897",
            "22d833a771fc4df8aa7282d1368481ac",
            "73f2c0d8f24849ffa0d2e3bb9ade9be9",
            "e505017180874e719518c1e00a9cdce6",
            "20726c9e90e14838a68b51bf87a7f11c",
            "a4e603d5e4dc4ddcb9323e9ed0c98ee1",
            "6af27bd784bf49d1838e441d9ab44f8d"
          ]
        },
        "id": "S5UjbfXJ5-G8",
        "outputId": "bc6d21cc-4c39-4a0e-d0fc-5e62ff3ad2f7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Installing and Importing libraries.\n",
            "\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "✅ All libraries loaded successfully!\n",
            "\n",
            "✅ Global Matplotlib ayarları uygulandı (Vektörel PDF çıkışı zorunlu kılındı).\n",
            "🔧 Google Colab tespit edildi - Google Drive mount ediliyor...\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive başarıyla mount edildi!\n",
            "\n",
            "\n",
            "[EDA-GUIDED] Loading EDA artifacts...\n",
            "\n",
            "  📁 Paths:\n",
            "     EDA Base:   /content/drive/My Drive/Thesis Data/EDA_Outputs_11.11.25 V7\n",
            "     Drop list:  /content/drive/My Drive/Thesis Data/EDA_Outputs_11.11.25 V7/Data_Exports/09b_features_to_drop_code_REVISED.py\n",
            "     MI scores:  /content/drive/My Drive/Thesis Data/EDA_Outputs_11.11.25 V7/Data_Exports/02_feature_relevance_mutual_information_train.csv\n",
            "\n",
            "  🔄 Loading feature drop list: 09b_features_to_drop_code_REVISED.py\n",
            "  ✅ Features to drop: 27\n",
            "     Sample (first 5):\n",
            "       • AwayTeam_ClubValue\n",
            "       • AwayTeam_LeagueValue\n",
            "       • AwayTeam_ManagerTenureDays\n",
            "       • AwayTeam_ManagerTrophies\n",
            "       • AwayTeam_MaxPlayerValue\n",
            "  ✅ Diff features to keep: 9\n",
            "     Sample:\n",
            "       • ClubValue_Diff\n",
            "       • MaxPlayerValue_Diff\n",
            "       • ManagerTrophies_Diff\n",
            "       • ManagerTenureDays_Diff\n",
            "       • NetTransferSpending_Diff\n",
            "\n",
            "  🔄 Loading MI scores: 02_feature_relevance_mutual_information_train.csv\n",
            "  ✅ Loaded MI scores for 72 features\n",
            "     Columns: ['Feature', 'Mutual_Information', 'Rank']\n",
            "  ⚠️  Could not identify feature/score columns\n",
            "     Using first two columns as feature/score\n",
            "\n",
            "  📊 EDA Artifacts Summary:\n",
            "     Features to drop:  27\n",
            "     Diff features:     9\n",
            "     MI scores loaded:  ✅ Yes\n",
            "\n",
            "  ✅ EDA artifacts loading complete\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "📁 DOSYA YOLLARI VE KONTROL\n",
            "====================================================================================================\n",
            "📁 BASE_PATH: /content/drive/My Drive/Thesis Data/\n",
            "📄 MATCHES_PATH: /content/drive/My Drive/Thesis Data/Matches.csv\n",
            "   Status: ✅ BULUNDU\n",
            "📄 ELOR_PATH: /content/drive/My Drive/Thesis Data/EloRatings.csv\n",
            "   Status: ✅ BULUNDU\n",
            "📄 TEAM_FEATURES_PATH: /content/drive/My Drive/Thesis Data/data.xlsx\n",
            "   Status: ✅ BULUNDU\n",
            "\n",
            "[OUTPUT] /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25   Main Model\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "🚀 FOOTBALL MATCH PREDICTION SYSTEM v18.0 (Single Scenario Mode)\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "1️⃣ DATA INTEGRATION...\n",
            "\n",
            "✅ Dosya başarıyla 'latin-1' ile okundu\n",
            "Manüel kod içi override kuralları uygulanıyor...\n",
            "  Override: 'ajaccio' -> 'ac ajaccio' (AC Ajaccio)\n",
            "  Override: 'ajaccio gfco' -> 'ajaccio gfc' (GFC Ajaccio)\n",
            "  Override: 'cordoba' -> 'cordoba' (Córdoba CF)\n",
            "[OVERRIDE RULES] 190 takım CSV dosyasından dinamik ve normalize olarak tanımlandı\n",
            "\n",
            "====================================================================================================\n",
            "[STEP 5] 🔀 MERGE İŞLEMİ\n",
            "\n",
            "[STEP 1] 📊 VERİ YÜKLEME\n",
            "\n",
            "📄 Matches yüklüyor...\n",
            "  ✓ 230,557 satır, 48 sütun\n",
            "\n",
            "📄 ELO Ratings yüklüyor...\n",
            "  ✓ 245,033 satır, 4 sütun\n",
            "\n",
            "📄 Transfermarkt yüklüyor...\n",
            "  ✓ 12,019 satır, 17 sütun\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[STEP 3] 🔧 VERİ TEMİZLEME VE FİLTRELEME\n",
            "\n",
            "Matches temizleniyor...\n",
            "   TOP 5 LİG filtresi: 230,557 → 43,708\n",
            "   Tarih filtresi (2015+): 43,708 → 18,985\n",
            "   Null takımlar/tarihler kaldırıldı: 18,985 → 18,985\n",
            "   ✅ Final Matches: 18,985 maç\n",
            "\n",
            "Transfermarkt temizleniyor...\n",
            "   TOP 5 LİG filtresi: 12,019 → 12,019\n",
            "   Null ClubName kaldırıldı: 12,019 → 12,019\n",
            "   ✅ Final Transfermarkt: 12,019 satır\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[STEP 2] 🔗 TAKIMLAR EŞLEŞTIRILMESI (FUZZY MATCHING)\n",
            "\n",
            "Matches'te unique takımlar: 166\n",
            "Transfermarkt'ta unique takımlar: 163\n",
            "\n",
            "🔄 Takım adı eşleştirmesi yapılıyor...\n",
            "\n",
            "   ✓ Mapping tamamlandı (166 takım)                 \n",
            "\n",
            "✅ Eşleştirilen takımlar: 161/166 (97.0%)\n",
            "\n",
            "⚠️ Eşleşmeyen takımlar (5):\n",
            "  • Ajaccio GFCO\n",
            "  • Cesena\n",
            "  • Cordoba\n",
            "  • Evian Thonon Gaillard\n",
            "  • QPR\n",
            "[STEP 4] ⏰ SEZON VE TARIH BİLGİSİ EKLEME\n",
            "\n",
            "Unique seasons (sample): ['2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n",
            "\n",
            "Unique YearMonths (sample): ['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02']\n",
            "\n",
            "✅ Season ve YearMonth eklendi\n",
            "\n",
            "[MERGE] Fuzzy mapping uygulanıyor...\n",
            "\n",
            "Eşleşmeyen maçlar: 121\n",
            "\n",
            "[FIX] Level 1 (YearMonth) verisi hazırlanıyor...\n",
            "   ✓ 12,019 unique satır\n",
            "\n",
            "[FIX] Level 2 (Season) verisi hazırlanıyor...\n",
            "   ✓ 1,100 unique satır\n",
            "\n",
            "   🏠 [LEVEL 1] HomeTeam merge...\n",
            "       [LEVEL 2] 1,225 satırda Season fallback...\n",
            "       Non-null HomeTeam: 17,760\n",
            "   ✈️  [LEVEL 1] AwayTeam merge...\n",
            "       [LEVEL 2] 1,226 satırda Season fallback...\n",
            "       Non-null AwayTeam: 17,759\n",
            "\n",
            "   ✅ Merge tamamlandı: 18,985 × 82\n",
            "\n",
            "[FEATURE ENGINEERING]\n",
            "✓ ELO_Diff oluşturuldu\n",
            "✓ ClubValue_Diff oluşturuldu\n",
            "✓ MaxPlayerValue_Diff oluşturuldu\n",
            "✓ ManagerTrophies_Diff oluşturuldu\n",
            "✓ ManagerTenureDays_Diff oluşturuldu\n",
            "✓ NetTransferSpending_Diff oluşturuldu\n",
            "✓ n_players_injured_Diff oluşturuldu\n",
            "\n",
            "[INFO] Creating ValueRatio features...\n",
            "✓ ValueRatio features created\n",
            "\n",
            "[MISSING VALUES HANDLING]\n",
            "✓ Missing values filled\n",
            "\n",
            "\n",
            "[NORMALIZED ODDS]\n",
            "✓ Normalized odds created\n",
            "====================================================================================================\n",
            "✅ DATA INTEGRATION COMPLETE\n",
            "\n",
            "Final dataset: 18,985 × 95\n",
            "✅ Saved: merged_final_complete.csv\n",
            "\n",
            "✅ Data merged: 18,985 matches × 95 features\n",
            "\n",
            "\n",
            "1️⃣.5️⃣ LAG FEATURES CREATION (Leakage-Free)...\n",
            "\n",
            "  🔄 Converting 12 in-game features to lag features:\n",
            "     • HomeTarget\n",
            "     • AwayTarget\n",
            "     • HomeShots\n",
            "     • AwayShots\n",
            "     • HomeCorners\n",
            "     • AwayCorners\n",
            "     • HomeFouls\n",
            "     • AwayFouls\n",
            "     • HomeYellow\n",
            "     • AwayYellow\n",
            "     • HomeRed\n",
            "     • AwayRed\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "🔄 CREATING LAG FEATURES (Leakage-Free Historical Averages)\n",
            "====================================================================================================\n",
            "\n",
            "  Processing: HomeTarget\n",
            "  Processing: AwayTarget\n",
            "  Processing: HomeShots\n",
            "  Processing: AwayShots\n",
            "  Processing: HomeCorners\n",
            "  Processing: AwayCorners\n",
            "  Processing: HomeFouls\n",
            "  Processing: AwayFouls\n",
            "  Processing: HomeYellow\n",
            "  Processing: AwayYellow\n",
            "  Processing: HomeRed\n",
            "  Processing: AwayRed\n",
            "\n",
            "  Handling NaN values (early season matches)...\n",
            "\n",
            "  ✅ Created 96 lag features\n",
            "  📊 Original features: 12\n",
            "  📊 Lag features: 96\n",
            "  📊 Windows used: [3, 5]\n",
            "  📊 Home/Away split: True\n",
            "\n",
            "  Sample lag features created:\n",
            "     • Home_HomeTarget_AtHome_Last3_Avg                   (mean:   4.84, coverage: 100.0%)\n",
            "     • Home_HomeTarget_Last3_Avg                          (mean:   4.84, coverage: 100.0%)\n",
            "     • Home_HomeTarget_AtHome_Last5_Avg                   (mean:   4.84, coverage: 100.0%)\n",
            "     • Home_HomeTarget_Last5_Avg                          (mean:   4.84, coverage: 100.0%)\n",
            "     • Away_HomeTarget_AtAway_Last3_Avg                   (mean:   3.99, coverage: 100.0%)\n",
            "     ... and 91 more\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "  🗑️  Removing original in-game features (now replaced with lags)...\n",
            "     ✅ Removed 12 original features\n",
            "     ✅ Added 96 lag features\n",
            "     📊 Net change: +84 features\n",
            "\n",
            "     🔒 LEAKAGE PREVENTED: 4 card features removed:\n",
            "        • HomeYellow → Replaced with lag features\n",
            "        • AwayYellow → Replaced with lag features\n",
            "        • HomeRed → Replaced with lag features\n",
            "        • AwayRed → Replaced with lag features\n",
            "\n",
            "     📄 Lag features list saved: lag_features_created.txt\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "2️⃣ FEATURE PREPARATION...\n",
            "\n",
            "\n",
            "2️⃣ FEATURE PREPARATION...\n",
            "\n",
            "  Features: 154\n",
            "  Target encoding: {'A': np.int64(0), 'D': np.int64(1), 'H': np.int64(2)}\n",
            "✅ Features: 18985 × 154\n",
            "\n",
            "\n",
            "📊 SCENARIO: ODDS-PLUS\n",
            "\n",
            "\n",
            "3️⃣ TRAIN/TEST SPLIT...\n",
            "\n",
            "  Train: 13,838 samples\n",
            "  Test:  5,147 samples\n",
            "\n",
            "3️⃣.5️⃣ EDA-GUIDED FEATURE PREPARATION...\n",
            "\n",
            "⚠️ [EXPERIMENT] Removing 22 betting features to force raw stats learning...\n",
            "   ✅ Dropped odds columns (16 features removed).\n",
            "\n",
            "[EDA-GUIDED] Applying Diff-Only Strategy...\n",
            "  → Dropping additional 21 features from EDA list\n",
            "  ✅ After Diff-Only: 120 features\n",
            "\n",
            "✅ Final feature count (Synced): 120\n",
            "✅ DataFrame shape: (13838, 120)\n",
            "\n",
            "\n",
            "[SUB-STEP 1.6] 🔄 Recalculating MI scores after feature removal...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-25 16:56:39,902] A new study created in memory with name: no-name-5e9c01af-3ef9-4331-90fb-dc981d1bfc0b\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  📊 MI Score Statistics (After Drop):\n",
            "     Mean:   0.010560\n",
            "     Max:    0.088097\n",
            "\n",
            "  🏆 Top 10 Features by MI (After Drop):\n",
            "  ------------------------------------------------------------\n",
            "     ELO_Diff                                 → MI = 0.088097\n",
            "     ValueRatio_Diff                          → MI = 0.075683\n",
            "     MaxPlayerValue_Diff                      → MI = 0.067898\n",
            "     ClubValue_Diff                           → MI = 0.064310\n",
            "     NetTransferSpending_Diff                 → MI = 0.047673\n",
            "     AwayElo                                  → MI = 0.037888\n",
            "     HomeElo                                  → MI = 0.031542\n",
            "     ManagerTrophies_Diff                     → MI = 0.029667\n",
            "     HomeTeam_ContinentalParticipation        → MI = 0.025922\n",
            "     Away_AwayTarget_AtAway_Last5_Avg         → MI = 0.024907\n",
            "  ------------------------------------------------------------\n",
            "\n",
            "  🗑️ AUTO-DROP: 24 features with MI < 0.001\n",
            "  ✅ Final features after MI drop: 96\n",
            "\n",
            "[SUB-STEP 3] Leakage check...\n",
            "     ✓ No leakage\n",
            "\n",
            "[SUB-STEP 4] Log transform skewed features...\n",
            "     ✓ Transformed 0 features\n",
            "     ✓ Final features: 96\n",
            "\n",
            "3️⃣.6️⃣ CLASS BALANCING (Single Scenario Mode)...\n",
            "\n",
            "    📊 Scenario: WITH SMOTE\n",
            "       Strategy: targeted\n",
            "\n",
            "\n",
            "3️⃣.7️⃣ VALIDATION SPLIT (Pre-SMOTE - For Early Stopping)...\n",
            "\n",
            "  🎯 Early Stopping: ENABLED\n",
            "     Validation split: 15%\n",
            "     Split method: TEMPORAL (last 15% of train data)\n",
            "\n",
            "  ✅ Temporal split successful:\n",
            "     • Train samples:      11,763\n",
            "     • Validation samples: 2,075\n",
            "\n",
            "  📅 Temporal boundaries:\n",
            "     • Train period:      2015-01-01 → 2021-05-16\n",
            "     • Validation period: 2021-05-16 → 2022-08-30\n",
            "     ⚠️  WARNING: 0 days overlap detected!\n",
            "        This may cause leakage with lag features\n",
            "\n",
            "  📊 Class distribution (after temporal split):\n",
            "     Train:\n",
            "       • Away Win (0): 3,581 (30.4%)\n",
            "       • Draw (1):     2,927 (24.9%)\n",
            "       • Home Win (2): 5,255 (44.7%)\n",
            "     Validation:\n",
            "       • Away Win (0): 658 (31.7%)\n",
            "       • Draw (1):     538 (25.9%)\n",
            "       • Home Win (2): 879 (42.4%)\n",
            "\n",
            "  ✅ Validation set characteristics:\n",
            "     • PRE-SMOTE (contains ZERO synthetic samples)\n",
            "     • Temporally AFTER all training data\n",
            "     • Safe for lag features (no data leakage)\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "4️⃣ SCALING PIPELINE (SMOTE will be applied in CV)...\n",
            "\n",
            "================================================================================\n",
            "📊 PIPELINE: SCALE ONLY (SMOTE deferred to CV)\n",
            "================================================================================\n",
            "\n",
            "  ℹ️  SMOTE will be applied inside ImbPipeline during cross-validation\n",
            "  ℹ️  This ensures validation folds never see synthetic samples\n",
            "\n",
            "[STEP 1/3] Fitting scaler on original train data...\n",
            "  📊 Train samples: 11,763\n",
            "  📊 Features: 96\n",
            "\n",
            "  📌 Scaler parameters (sample: ClubValue_Diff):\n",
            "     • Center (median): 0.00\n",
            "     • Scale (IQR):     181,250,000.00\n",
            "\n",
            "[STEP 2/3] Scaling all datasets...\n",
            "  ✅ Train scaled: (11763, 96)\n",
            "  ✅ Test scaled: (5147, 96)\n",
            "  ✅ Validation scaled: (2075, 96)\n",
            "\n",
            "[STEP 3/3] Converting to DataFrames...\n",
            "  ✅ Train: (11763, 96) (PRE-SMOTE)\n",
            "  ✅ Test:  (5147, 96)\n",
            "  ✅ Validation: (2075, 96) (PRE-SMOTE)\n",
            "[INFO] Class distribution (PRE-SMOTE):\n",
            "  • Away Win (0): 3,581 (30.4%)\n",
            "  • Draw (1):     2,927 (24.9%)\n",
            "  • Home Win (2): 5,255 (44.7%)\n",
            "\n",
            "  ℹ️  SMOTE will balance this inside each CV fold\n",
            "\n",
            "[VERIFICATION] Sanity checks...\n",
            "  ✅ No NaN/Inf in train data\n",
            "  ✅ Sample feature (scaled): mean=0.1151, std=0.8068\n",
            "  ✅ Class balance ratio: 1.80:1 (lower is better)\n",
            "     ✅ Reasonably balanced (<3:1)\n",
            "\n",
            "\n",
            "[VALIDATION TEST]\n",
            "  ✅ Shape match: 2075 == 2075\n",
            "  ✅ Scaled correctly: mean = 0.1856 (close to 0)\n",
            "  ✅ No NaN values\n",
            "  ✅ Columns match: 96 features\n",
            "  ✅ All validation tests passed!\n",
            "\n",
            "\n",
            "5️⃣ TRAINING MODELS...\n",
            "\n",
            "================================================================================\n",
            "📊 VALIDATION STATUS BEFORE TRAINING\n",
            "================================================================================\n",
            "✅ X_val: (2075, 96) (SCALED)\n",
            "✅ y_val: (2075,)\n",
            "✅ Sample scaled value: -0.2335 (should be ~0)\n",
            "\n",
            "📊 Validation class distribution:\n",
            "   • Away Win (0): 658 (31.7%)\n",
            "   • Draw (1):     538 (25.9%)\n",
            "   • Home Win (2): 879 (42.4%)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "5️⃣ TRAINING MODELS...\n",
            "\n",
            "  One-hot encoded (3 classes)\n",
            "================================================================================\n",
            "🔍 VALIDATION SET STATUS\n",
            "================================================================================\n",
            "\n",
            "✅ Validation set received from main execution\n",
            "   • Samples: 2,075\n",
            "   • Features: 96\n",
            "   • Source: PRE-SMOTE split (ADIM 3.7)\n",
            "   • Contains ZERO synthetic samples ✅\n",
            "\n",
            "   📊 Validation class distribution:\n",
            "      • Away Win (0): 658 (31.7%)\n",
            "      • Draw (1):     538 (25.9%)\n",
            "      • Home Win (2): 879 (42.4%)\n",
            "\n",
            "   🎯 Will be used for:\n",
            "      • Early stopping (XGBoost, LightGBM, CatBoost)\n",
            "      • Ablation analysis (G28, G29)\n",
            "      • XAI verification\n",
            "\n",
            "   📊 Training data (post-SMOTE):\n",
            "      • Samples: 11,763\n",
            "      • May contain synthetic samples (correct!)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "🔬 CROSS-VALIDATION STRATEGY\n",
            "================================================================================\n",
            "  ✅ SMOTE will be applied PER FOLD (ImbPipeline)\n",
            "  ✅ Validation folds will be SYNTHETIC-FREE\n",
            "  ✅ This prevents overfitting to synthetic samples\n",
            "\n",
            "  📊 SMOTE Strategy: targeted\n",
            "  📊 K-Neighbors: 5\n",
            "\n",
            "  🎯 Targeted Sampling Strategy:\n",
            "     • Away Win (0): 3,581 → 3,581 (no change)\n",
            "     • Draw (1):     2,927 → 4,859 (+1,932)\n",
            "     • Home Win (2): 5,255 → 5,255 (no change)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Optimizing LR...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5836afcf09f145a29f0bcf7405df84bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Best CV F1: 0.4992\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✅ Final model trained successfully\n",
            "  ✓ Per-class metrics calculated\n",
            "  Time: 40.94s\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-25 16:57:21,105] A new study created in memory with name: no-name-0a0cf93b-e89b-4023-8b97-84a71b97e276\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimizing RF...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e280b1ed66c4b8ebd8905e0f1de0ca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Best CV F1: 0.4989\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-25 17:13:10,879] A new study created in memory with name: no-name-46dfb768-195b-47ea-bc3b-ad97b6bb27a9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 949.50s\n",
            "\n",
            "Optimizing GB...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "525a62d2b6ae4c76aa0b1ad97ad52b63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Best CV F1: 0.4904\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-25 17:24:24,646] A new study created in memory with name: no-name-84cdef04-99ef-482e-a264-ae0d7f5aab89\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 673.48s\n",
            "\n",
            "Optimizing XGBOOST...\n",
            "  [EARLY STOP] Using PRE-SMOTE validation set: 2,075 samples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e30a4e2c86aa4ab99abeb6929b8fb3a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Best CV F1: 0.4974\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✓ Best iteration: 493\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-25 17:50:07,141] A new study created in memory with name: no-name-14c528d6-e438-4624-b7ef-e614257b5962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 1542.24s\n",
            "\n",
            "Optimizing LIGHTGBM...\n",
            "  [EARLY STOP] Using PRE-SMOTE validation set: 2,075 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6d16d071a374dbd9610883ef618db56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Best CV F1: 0.4956\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✓ Best iteration: 479 (out of 500)\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-25 18:24:13,409] A new study created in memory with name: no-name-db82509f-8482-427d-b24f-a76fb5cf1313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 2046.01s\n",
            "\n",
            "Optimizing CATBOOST...\n",
            "  [EARLY STOP] Using PRE-SMOTE validation set: 2,075 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed7cd2c99ae346f5aae3192da5535b03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Best CV F1: 0.5018\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✓ Best iteration: 407 (out of 500)\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-25 18:58:06,207] A new study created in memory with name: no-name-84850140-8517-4327-bad0-d38742b6448b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 2032.46s\n",
            "\n",
            "Optimizing ADA...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "832340b1f04e41fdbb367a9cd7c48212"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Best CV F1: 0.5029\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✅ Final model trained successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-25 19:34:08,588] A new study created in memory with name: no-name-40356afe-3d5d-4579-b8f3-06d7243157e5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Per-class metrics calculated\n",
            "  Time: 2162.14s\n",
            "\n",
            "Optimizing LTCN...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bfec02e36d64860a9a52b5ad3379810"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ Best CV F1: 0.4508\n",
            "  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\n",
            "  [FINAL] Training final model with best params...\n",
            "  [SMOTE] Applying SMOTE to full train set...\n",
            "  [SMOTE] 11,763 → 13,695 samples\n",
            "  ✅ Final model trained successfully\n",
            "  ✓ Per-class metrics calculated\n",
            "  Time: 2564.53s\n",
            "\n",
            "====================================================================================================\n",
            "🎯 XAI ANALYSIS - BEST MODEL SELECTION\n",
            "====================================================================================================\n",
            "\n",
            "📊 Model Rankings (Top 5):\n",
            "\n",
            "  🥇 #1: RF (Optuna)               → F1 = 0.493521\n",
            "  🥈 #2: XGBOOST (Optuna)          → F1 = 0.492202\n",
            "  🥉 #3: ADA (Optuna)              → F1 = 0.491634\n",
            "     #4: CATBOOST (Optuna)         → F1 = 0.490132\n",
            "     #5: LIGHTGBM (Optuna)         → F1 = 0.486287\n",
            "\n",
            "🏆 SELECTED BEST MODEL: RF (Optuna)\n",
            "📊 Test F1-Score: 0.493521\n",
            "🔬 Running Explainable AI Analysis...\n",
            "\n",
            "   XAI Methods:\n",
            "   ├─ Permutation Feature Importance (PFI)\n",
            "   ├─ Predictive Mutual Information (PMI)\n",
            "   ├─ Sensitivity-based Feature Importance (SOFI)\n",
            "   ├─ SHAP Values (if applicable)\n",
            "   ├─ Model-based Importance\n",
            "   └─ Aggregated Analysis\n",
            "\n",
            "\n",
            "  [XAI CONFIG] PFI repeats: 5, Seed: 42\n",
            "  [PFI] Computing...\n",
            "  ✓ PFI completed in 76.42s\n",
            "    [PMI] Computing...\n",
            "       ✓ PMI completed in 3.95s\n",
            "    [SOFI] Computing...\n",
            "    [SOFI] Using GA-based optimization (pygad)\n",
            "       GA params: gen=25, pop=50, mut=0.05, seed=42\n",
            "       ✓ SOFI completed in 7293.91s\n",
            "    [SHAP] Computing feature importance...\n",
            "       [DEBUG] Expected features: 96\n",
            "       [DEBUG] X shape: (11763, 96)\n",
            "       [DEBUG] Samples: 100, Background: 50\n",
            "       Using TreeExplainer (Tree-based)\n",
            "       [DEBUG] SHAP output type: <class 'numpy.ndarray'>\n",
            "       [DEBUG] 3D array shape: (100, 96, 3)\n",
            "       [DEBUG] SHAP importance statistics:\n",
            "         Shape:  (96,)\n",
            "         Min:    0.000252\n",
            "         Max:    0.021761\n",
            "         Mean:   0.001892\n",
            "         Median: 0.000902\n",
            "         Sum:    0.181679\n",
            "       [DEBUG] Top 5 features (raw SHAP values):\n",
            "         #1: ELO_Diff                                 = 0.021761\n",
            "         #2: ValueRatio_Diff                          = 0.013184\n",
            "         #3: ClubValue_Diff                           = 0.011653\n",
            "         #4: MaxPlayerValue_Diff                      = 0.010490\n",
            "         #5: HomeElo                                  = 0.008938\n",
            "       [DEBUG] After normalization:\n",
            "         Sum: 1.0000000000 (should be ≈ 1.0)\n",
            "       ✓ SHAP completed in 1.06s\n",
            "\n",
            "  [XAI TIMING SUMMARY]\n",
            "  ============================================================\n",
            "    SOFI      : 7293.91s ( 98.9%)\n",
            "    PFI       :  76.42s (  1.0%)\n",
            "    PMI       :   3.95s (  0.1%)\n",
            "    SHAP      :   1.06s (  0.0%)\n",
            "  ============================================================\n",
            "    TOTAL:      7375.33s\n",
            "\n",
            "\n",
            "   ✅ XAI Analysis Completed Successfully!\n",
            "   ⏱️  Duration: 7375.42s (122.92 minutes)\n",
            "   📋 Methods Analyzed: 6\n",
            "\n",
            "   Method Results:\n",
            "   ├─ PFI     : ELO_Diff, Home_HomeRed_AtHome_Last3_Avg...\n",
            "   ├─ PMI     : ELO_Diff, ValueRatio_Diff...\n",
            "   ├─ SOFI    : Away_HomeFouls_Last3_Avg, Away_AwayFouls_AtAway_Last5_Avg...\n",
            "   ├─ SHAP    : ELO_Diff, ValueRatio_Diff...\n",
            "   ├─ LTCN    : ELO_Diff, ValueRatio_Diff...\n",
            "   ├─ XGBoost : ELO_Diff, ValueRatio_Diff...\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "📊 PER-CLASS PERFORMANCE ANALYSIS\n",
            "====================================================================================================\n",
            "\n",
            "🏆 BEST MODEL: RF (Optuna)\n",
            "\n",
            "  Class-wise Performance:\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  ✈️ Away Win  :\n",
            "     • Accuracy:  0.4250 (42.50%)\n",
            "     • Precision: 0.5449\n",
            "     • Recall:    0.4250\n",
            "     • F1-Score:  0.4775\n",
            "     • Support:   1,600 samples\n",
            "\n",
            "  🤝 Draw      :\n",
            "     • Accuracy:  0.2820 (28.20%)\n",
            "     • Precision: 0.2789\n",
            "     • Recall:    0.2820\n",
            "     • F1-Score:  0.2804\n",
            "     • Support:   1,291 samples\n",
            "\n",
            "  🏠 Home Win  :\n",
            "     • Accuracy:  0.6738 (67.38%)\n",
            "     • Precision: 0.5860\n",
            "     • Recall:    0.6738\n",
            "     • F1-Score:  0.6268\n",
            "     • Support:   2,256 samples\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  📈 Overall Performance:\n",
            "     • Weighted Accuracy: 0.4982 (49.82%)\n",
            "     • Weighted F1-Score: 0.4935\n",
            "\n",
            "  🎯 Insights:\n",
            "     • Best Predicted:  Home Win (67.38%)\n",
            "     • Worst Predicted: Draw (28.20%)\n",
            "     • Class Imbalance: 0.3918\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "✅ Hyperparameters saved: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model/best_hyperparameters.json\n",
            "\n",
            "====================================================================================================\n",
            "📦 PREPARING RETURN VALUES\n",
            "====================================================================================================\n",
            "\n",
            "[RETURN INFO] Function will return 4 values:\n",
            "  1. results:       Performance metrics for 8 models\n",
            "  2. models_dict:   8 trained model objects\n",
            "  3. xai_results:   1 XAI analyses\n",
            "  4. validation_data: Tuple of (X_val, y_val)\n",
            "\n",
            "  ✅ Validation set prepared:\n",
            "     • X_val shape: (2075, 96)\n",
            "     • y_val length: 2075\n",
            "     • Source: PRE-SMOTE split (ADIM 3.7)\n",
            "     • Contains: 100% REAL data (zero synthetic samples)\n",
            "\n",
            "     📊 Validation class distribution:\n",
            "        • Away Win (0): 658 (31.7%)\n",
            "        • Draw (1):     538 (25.9%)\n",
            "        • Home Win (2): 879 (42.4%)\n",
            "\n",
            "     🎯 Usage:\n",
            "        • Ablation analysis (G28: XAI comparison)\n",
            "        • Ablation analysis (G29: Cumulative importance)\n",
            "        • XAI method verification\n",
            "        • Early stopping (already used during training)\n",
            "\n",
            "[SANITY CHECKS]\n",
            "  ✅ Results: 8 models trained\n",
            "  ✅ Models dict: Consistent with results\n",
            "  ✅ XAI results: 1 analyses\n",
            "  ✅ Validation data: Consistent\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "📊 POST-TRAINING VALIDATION STATUS\n",
            "================================================================================\n",
            "✅ Validation set received: 2,075 samples\n",
            "   • Contains NO synthetic samples (correct!)\n",
            "   • Was used for early stopping (XGBoost, LightGBM, CatBoost)\n",
            "   • Will be used for ablation analysis (G28, G29)\n",
            "================================================================================\n",
            "\n",
            "✅ _WITH_SMOTE training complete: 8 models\n",
            "\n",
            "\n",
            "6️⃣ CREATING TABLES...\n",
            "\n",
            "\n",
            "6️⃣ CREATING TABLES...\n",
            "\n",
            "  ✓ T1\n",
            "  ✓ T3\n",
            "  ✓ T4\n",
            "  ✓ T7\n",
            "  ✓ T8\n",
            "  ✓ T9 (RF (Optuna))\n",
            "  ✓ T10\n",
            "\n",
            "✅ Created 7 tables\n",
            "\n",
            "\n",
            "[SAVE] Saving tables as PNG...\n",
            "\n",
            "  ✓ T1_Model_Performance -> PNG\n",
            "  ✓ T3_AUC_Calibration -> PNG\n",
            "  ✓ T4_Generalization -> PNG\n",
            "  ✓ T7_Kappa_Analysis -> PNG\n",
            "  ✓ T8_Summary_Stats -> PNG\n",
            "  ✓ T9_Best_Model_Performance -> PNG\n",
            "  ✓ T10_Per_Class_Metrics -> PNG\n",
            "\n",
            "✅ Tables saved to: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model/tables\n",
            "   Total saved: 7 PNG files\n",
            "\n",
            "\n",
            "7️⃣ CREATING GRAPHICS...\n",
            "\n",
            "================================================================================\n",
            "📊 VALIDATION STATUS FOR GRAPHICS\n",
            "================================================================================\n",
            "✅ Will send validation to graphics:\n",
            "   • X_val shape: (2075, 96)\n",
            "   • y_val length: 2075\n",
            "   • For use in: G28 (XAI comparison), G29 (Cumulative importance)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "7️⃣ CREATING GRAPHICS (Thesis-Compliant)...\n",
            "\n",
            "  📐 Format: Dual (PNG + PDF)\n",
            "  🎨 Color Palette: Blue tones (G28 exception)\n",
            "  📊 Resolution: 300 DPI\n",
            "\n",
            "  [G1] Creating Model Accuracy Comparison...\n",
            "    ✓ G1 saved (PDF: 29.2 KB)\n",
            "  [G2] Creating F1-Score Comparison...\n",
            "    ✓ G2 saved (PDF: 29.4 KB)\n",
            "  [G3] Creating AUC-ROC Comparison...\n",
            "    ✓ G3 saved (PDF: 30.2 KB)\n",
            "  [G5] Creating Cohen's Kappa Score...\n",
            "    ✓ G5 saved (PDF: 30.0 KB)\n",
            "  [G7] Creating Ranked Probability Score...\n",
            "    ✓ G7 saved (PDF: 25.1 KB)\n",
            "  [G8] Creating Expected Calibration Error...\n",
            "    ✓ G8 saved (PDF: 26.9 KB)\n",
            "  [G9] Creating Probabilistic Metrics Comparison...\n",
            "    ✓ G9 saved (PDF: 31.0 KB)\n",
            "  [G11] Creating Generalization Gap...\n",
            "    ✓ G11 saved (PDF: 28.8 KB)\n",
            "  [G12] Creating Model Ranking...\n",
            "    ✓ G12 saved (PDF: 25.0 KB)\n",
            "  [G13] Creating Accuracy vs Loss...\n",
            "    ✓ G13 saved (PDF: 26.6 KB)\n",
            "    ⏸️  G14 - Skipped (CONFIG)\n",
            "    ⏸️  G15 - Skipped (CONFIG)\n",
            "  [G16] Creating Feature Importance Agreement...\n",
            "    ⏸️  G17 - Skipped (CONFIG)\n",
            "    ⏸️  G18a - Skipped (CONFIG)\n",
            "  [G18b] Creating Aggregated Top 10 Features...\n",
            "    ✓ G18b saved (PDF: 26.2 KB)\n",
            "  [G18c] Creating SHAP Summary Plot...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4bac4f0d084b4ea78c30ee16620f14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ✓ G18c saved (PDF: 5.7 KB)\n",
            "  [G20a] Creating Classification Metrics Radar...\n",
            "    ✓ G20a saved (PDF: 29.2 KB)\n",
            "  [G20b] Creating Probabilistic Metrics Radar...\n",
            "    ✓ G20b saved (PDF: 29.6 KB)\n",
            "  [G24] Creating Executive Summary...\n",
            "    ✓ G24 saved (PDF: 40.6 KB)\n",
            "  [G25] Creating Confusion Matrices...\n",
            "    ✓ G25 - 8 Confusion Matrices saved (PDF format)\n",
            "  [G28] Creating XAI Method Comparison...\n",
            "    ⚠️  Exception: Using original multi-color palette\n",
            "    🔄 Creating G28...\n",
            "\n",
            "[G28-v19] Creating XAI Method Comparison (Ablation-Based)...\n",
            "  Best Model: RF (Optuna) (F1: 0.4935)\n",
            "  ✅ Using validation set for ablation: 2,075 samples\n",
            "    ⏸️  G29 - Skipped (CONFIG)\n",
            "  [G31] Creating Per-Class Accuracy Heatmap...\n",
            "    ✓ G31 saved (PDF: 63.7 KB)\n",
            "\n",
            "================================================================================\n",
            "✅ GRAPHICS CREATION COMPLETE\n",
            "================================================================================\n",
            "  📊 Total graphics created: 24\n",
            "  📐 Format: Dual (PNG + PDF)\n",
            "  🎨 Color scheme: Thesis-compliant blue tones\n",
            "  ⚠️  Exception: G28 (multi-color preserved)\n",
            "  📁 Output directory: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model/graphics\n",
            "  💾 Total PDF size: 680.9 KB (0.7 MB)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "✅ Total graphics created: 24\n",
            "\n",
            "8️⃣ CREATING PDF...\n",
            "\n",
            "\n",
            "8️⃣ CREATING PDF (Text Summary Only)...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "✅ PDF TEXT SUMMARY CREATED SUCCESSFULLY\n",
            "================================================================================\n",
            "   📄 Path: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model/Football_Prediction_v18_TEXT_SUMMARY.pdf\n",
            "   💾 Size: 0.04 MB\n",
            "   📊 Pages: 1 (Title page only)\n",
            "   🎨 Format: Vector (Matplotlib)\n",
            "\n",
            "   📦 ADDITIONAL FILES (for Overleaf):\n",
            "      • 24 graphics (separate vector PDFs)\n",
            "      • 7 tables (separate PNG files)\n",
            "\n",
            "   ⚠️  NEXT STEP:\n",
            "      Upload all vector PDF graphics to Overleaf for final document\n",
            "================================================================================\n",
            "\n",
            "✅ PDF Report Renamed: Football_Prediction_v18_WITH_SMOTE.pdf\n",
            "\n",
            "9️⃣ SAVING RESULTS...\n",
            "\n",
            "✅ Results saved: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model/results.json\n",
            "✅ Results Renamed: results_WITH_SMOTE.json\n",
            "\n",
            "====================================================================================================\n",
            "✨ v18.0 COMPLETE - SCENARIO: _WITH_SMOTE\n",
            "====================================================================================================\n",
            "\n",
            "📊 EXECUTION SUMMARY:\n",
            "\n",
            "✅ Data: 18,985 matches\n",
            "✅ Team Mapping: 161/166 (97.0%)\n",
            "\n",
            "📈 FEATURE STATISTICS:\n",
            "  • Initial features (pre-lag): 154\n",
            "  • Final features (post-MI): 96\n",
            "  • Reduction: 58 (37.7%)\n",
            "\n",
            "🎯 MODEL PERFORMANCE:\n",
            "  ✅ Best Model: RF (Optuna) (F1 = 0.4935)\n",
            "\n",
            "📁 OUTPUTS:\n",
            "  ✅ Directory: /content/drive/My Drive/Thesis Data/ NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model\n",
            "  ✅ Graphics: 24\n",
            "  ✅ Tables: 7\n",
            "  ✅ PDF Report: Football_Prediction_v18_WITH_SMOTE.pdf\n",
            "  ✅ Results JSON: results_WITH_SMOTE.json\n",
            "\n",
            "⏱️  Total Execution Time (_WITH_SMOTE):\n",
            "   19531.25s (325.52 minutes)\n",
            "\n",
            "====================================================================================================\n",
            "✅ v18.0 EXECUTION COMPLETE (SCENARIO: _WITH_SMOTE)\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FOOTBALL MATCH PREDICTION SYSTEM v18.0 - ODDS-PLUS WITH EDA PIPELINE\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "\"\"\"\n",
        "\n",
        "print(\"✓ Installing and Importing libraries.\\n\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn optuna matplotlib seaborn scipy shap xgboost lightgbm catboost rapidfuzz unidecode openpyxl reportlab joblib pygad   -q\n",
        "!pip install imbalanced-learn pygad -q\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"--upgrade\", \"pip\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n",
        "                          \"pandas\", \"numpy\", \"scikit-learn\", \"optuna\", \"matplotlib\", \"seaborn\",\n",
        "                          \"scipy\", \"shap\", \"joblib\", \"tqdm\", \"rapidfuzz\", \"unidecode\", \"pillow\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n",
        "                          \"xgboost\", \"lightgbm\", \"catboost\", \"statsmodels\", \"openpyxl\",\n",
        "                          \"reportlab\", \"plotly\", \"kaleido\"])\n",
        "except:\n",
        "    print(\"[WARNING] Some libraries may not have installed properly\")\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import traceback\n",
        "import gc\n",
        "import csv\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from pathlib import Path\n",
        "from functools import lru_cache\n",
        "from joblib import Parallel, delayed\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "from rapidfuzz import fuzz\n",
        "from unidecode import unidecode\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import joblib\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, TimeSeriesSplit, cross_val_score\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder, label_binarize\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, log_loss,\n",
        "                             brier_score_loss, confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
        "                             classification_report)\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Bu özel pipeline, SMOTE işlemini sadece fit (eğitim) anında yapar, transform (tahmin/validasyon) anında yapmaz. Böylece validasyon seti hep \"temiz\" ve \"gerçek\" kalır.\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import entropy, skellam, poisson\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from scipy.ndimage import uniform_filter1d  # ✅ Ekle\n",
        "\n",
        "\n",
        "# Import bölümüne:\n",
        "try:\n",
        "    from pygad import GA\n",
        "    HAS_PYGAD = True\n",
        "except ImportError:\n",
        "    HAS_PYGAD = False\n",
        "    print(\"⚠️ pygad not installed. SOFI analysis will be skipped.\")\n",
        "# ✅ YENİ EKLEME: SHAP için paralel hesaplama\n",
        "from joblib import Parallel, delayed  # ← Zaten var ama emin olun\n",
        "\n",
        "# Optional libraries\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except ImportError:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except ImportError:\n",
        "    HAS_LGB = False\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    HAS_CB = True\n",
        "except ImportError:\n",
        "    HAS_CB = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "except ImportError:\n",
        "    HAS_SHAP = False\n",
        "\n",
        "print(\"\\n✅ All libraries loaded successfully!\\n\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# ====================================================================\n",
        "# ✅ YÜKSEK KALİTELİ MATPLOTLIB ÇIKTI AYARLARI (GLOBAL)\n",
        "# ====================================================================\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",         # PDF uyumluluğu için\n",
        "    \"font.size\": 12,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"savefig.dpi\": 600,             # Yüksek çözünürlük DPI\n",
        "    \"savefig.format\": \"pdf\",        # Çıktı formatını PDF olarak zorla (vektörel)\n",
        "    \"text.usetex\": False,\n",
        "    \"figure.autolayout\": True\n",
        "})\n",
        "print(\"✅ Global Matplotlib ayarları uygulandı (Vektörel PDF çıkışı zorunlu kılındı).\")\n",
        "# ====================================================================\n",
        "\n",
        "# Tez terminolojisi ile kod senkronizasyonu\n",
        "TERMINOLOGY = {\n",
        "    'methodology': {\n",
        "        'validation': 'Temporal Split (Leakage-Free)',\n",
        "        'tuning': 'Shared Tuning Protocol (Optuna + TimeSeriesSplit)',\n",
        "        'cv': 'Time-Series Cross-Validation',\n",
        "        'preprocessing': 'Leakage-Free Preprocessing Pipeline'\n",
        "    },\n",
        "    'phases': {\n",
        "        'rq1': 'RQ1: Predictive Performance Evaluation',\n",
        "        'rq2': 'RQ2: Explainability Analysis',\n",
        "        'rq3': 'RQ3: Feature Importance Comparison'\n",
        "    },\n",
        "    'components': {\n",
        "        'data_integration': 'Multi-Source Data Integration (Fuzzy Matching)',\n",
        "        'feature_engineering': 'EDA-Guided Feature Engineering',\n",
        "        'xai': 'Explainable AI Analysis (6 Methods)'\n",
        "    }\n",
        "}\n",
        "\n",
        "# CONFIGURATION\n",
        "CONFIG = {\n",
        "    \"filters\": {\n",
        "        \"leagues\": [\"E0\", \"D1\", \"I1\", \"SP1\", \"F1\"],\n",
        "        \"min_date\": \"2015-07-01\"\n",
        "    },\n",
        "    \"temporal\": {\n",
        "        \"cutoff_year\": 2015,\n",
        "        \"elo_tolerance_days\": 30\n",
        "    },\n",
        "    \"data_integration\": {\n",
        "        \"enabled\": True,\n",
        "        \"elo_tolerance_days\": 30,\n",
        "        \"split_method\": \"temporal\",  # ← YENİ: Açık belirtim\n",
        "        \"validation_desc\": TERMINOLOGY['methodology']['validation'],  # ← YENİ\n",
        "        \"use_transfermarkt_features\": True\n",
        "    },\n",
        "    \"eda_integration\": {\n",
        "        \"enabled\": True,\n",
        "        \"strategy\": \"diff_only\",\n",
        "        \"base_path\": \"EDA_Outputs_11.11.25 V7\",\n",
        "        \"load_preprocessed_data\": True,\n",
        "        \"preprocessed_data_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/merged_final_with_lags_cleaned.csv\",\n",
        "        \"skip_lag_features\": True,\n",
        "        \"skip_smart_selection\": True,\n",
        "        \"skip_mi_recalculation\": True,\n",
        "        \"feature_list_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/final_feature_list.txt\",\n",
        "        \"dropped_features_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/features_dropped.csv\",\n",
        "        \"mi_scores_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/02_feature_relevance_mutual_information_train.csv\",\n",
        "        \"validation\": {\n",
        "            \"check_feature_count\": True,\n",
        "            \"expected_features\": 83,\n",
        "            \"check_leakage\": True,\n",
        "            \"check_data_shape\": True\n",
        "        }\n",
        "    },\n",
        "    \"feature_toggles\": {\n",
        "        \"use_feature_selection\": False,\n",
        "        \"n_features_to_select\": 20,\n",
        "    },\n",
        "    \"cv\": {\n",
        "        \"folds\": 5,\n",
        "        \"method\": \"TimeSeriesSplit\",  # ← YENİ\n",
        "         \"desc\": TERMINOLOGY['methodology']['cv']  # ← YENİ\n",
        "    },\n",
        "    \"optuna\": {\n",
        "        \"n_trials\": 20,\n",
        "        \"timeout\": 2000,\n",
        "        \"desc\": TERMINOLOGY['methodology']['tuning']  # ← YENİ\n",
        "    },\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ EKLEME: EARLY STOPPING CONFİGURATİON\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    \"early_stopping\": {\n",
        "        \"enabled\": True,              # Master switch - early stopping kullan\n",
        "        \"rounds\": 50,                 # 50 iteration boyunca iyileşme yoksa dur\n",
        "        \"val_split\": 0.15,            # Train setinin %15'i validation olarak ayrılır\n",
        "        \"verbose\": False,             # Debug output gösterme\n",
        "        \"models\": [\"xgboost\", \"lightgbm\", \"catboost\"]  # Hangi modellere uygulanacak\n",
        "    },\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    \"graphics_to_skip\": [14, 15, 17, \"18a\", 29],\n",
        "    \"models\": [\"lr\", \"rf\", \"gb\", \"xgboost\", \"lightgbm\", \"catboost\", \"ada\", \"ltcn\"],\n",
        "    \"xai\": {\n",
        "        \"n_samples\": 100,\n",
        "        \"n_shap_background\": 50,\n",
        "        \"n_jobs_shap\": 16,\n",
        "        \"use_shap\": True,\n",
        "        \"use_permutation\": True,\n",
        "        \"pfi_n_repeats\": 5,\n",
        "        \"ablation_max_features\": 30,\n",
        "        \"random_state\": 42,\n",
        "    },\n",
        "    \"sofi\": {\n",
        "        \"n_generations\": 25,\n",
        "        \"sol_per_pop\": 50,\n",
        "        \"mutation_probability\": 0.05,\n",
        "        \"random_seed\": 42\n",
        "    },\n",
        "    \"smart_feature_selection\": {\n",
        "        \"enabled\": True,\n",
        "        \"mi_threshold\": 0.001,\n",
        "        \"corr_threshold\": 0.85,\n",
        "        \"fallback_to_external\": True\n",
        "    },\n",
        "    \"class_balancing\": {\n",
        "        \"use_smote\": True,\n",
        "        \"strategy\": \"targeted\",\n",
        "        \"min_class_ratio\": 0.25,\n",
        "        \"target_draw_ratio\": 1.1,\n",
        "        \"k_neighbors\": 5,\n",
        "        \"use_class_weights\": True\n",
        "    },\n",
        "    \"eda_pipeline\": {\n",
        "        \"use_elo_diff\": True,\n",
        "        \"use_mi_selection\": False,\n",
        "        \"mi_threshold\": 0.04,\n",
        "        \"drop_list_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/09b_features_to_drop_code_REVISED.py\",\n",
        "        \"mi_scores_path\": \"EDA_Outputs_11.11.25 V7/Data_Exports/02_feature_relevance_mutual_information_train.csv\"\n",
        "    }\n",
        "}\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "N_CLASSES = 3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# ✅ LEAKAGE FEATURES DEFINITION\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "LEAKAGE_FEATURES = [\n",
        "    # ══════════════════════════════════════════════════════════════════\n",
        "    # CRITICAL: In-game statistics that reveal match outcome\n",
        "    # ══════════════════════════════════════════════════════════════════\n",
        "    'HomeTarget',       # Home team shots on target\n",
        "    'AwayTarget',       # Away team shots on target\n",
        "    'HomeShot',\n",
        "    'HomeShots',\n",
        "    'AwayShots',\n",
        "    'AwayShot',         # Away team total shots\n",
        "    'HomeCorners',      # Home team corners\n",
        "    'AwayCorners',      # Away team corners\n",
        "    'HomeFouls',        # Home team fouls\n",
        "    'AwayFouls',        # Away team fouls\n",
        "    'HomeYellow',       # Home team yellow cards\n",
        "    'AwayYellow',       # Away team yellow cards\n",
        "    'HomeRed',          # Home team red cards\n",
        "    'AwayRed',          # Away team red cards\n",
        "\n",
        "    # ══════════════════════════════════════════════════════════════════\n",
        "    # OUTCOME FEATURES (should never be in feature set)\n",
        "    # ══════════════════════════════════════════════════════════════════\n",
        "    'FTHome',           # Full-time home goals (TARGET LEAKAGE!)\n",
        "    'FTAway',           # Full-time away goals (TARGET LEAKAGE!)\n",
        "    'HTHome',           # Half-time home goals\n",
        "    'HTAway',           # Half-time away goals\n",
        "    'FTResult',         # Match result (HOME/DRAW/AWAY)\n",
        "    'HTResult',         # Half-time result\n",
        "]\n",
        "\n",
        "DATASET_1_NAME = \"Matches.csv\"\n",
        "DATASET_1_COLOR = \"#3498db\"\n",
        "DATASET_2_NAME = \"ELO Ratings\"\n",
        "DATASET_2_COLOR = \"#e74c3c\"\n",
        "DATASET_3_NAME = \"Transfermarkt\"\n",
        "DATASET_3_COLOR = \"#2ecc71\"\n",
        "DATASET_4_NAME = \"Derived/Other\"\n",
        "DATASET_4_COLOR = \"#f39c12\"\n",
        "\n",
        "DATASETS = {\n",
        "    DATASET_1_NAME: {'color': DATASET_1_COLOR},\n",
        "    DATASET_2_NAME: {'color': DATASET_2_COLOR},\n",
        "    DATASET_3_NAME: {'color': DATASET_3_COLOR},\n",
        "    DATASET_4_NAME: {'color': DATASET_4_COLOR}\n",
        "}\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 🎨 THESIS-COMPLIANT COLOR PALETTE (Global Definition)\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "THESIS_COLORS = {\n",
        "    'primary': {\n",
        "        'dark_blue': '#1f3a93',      # Ana mavi (en koyu)\n",
        "        'medium_blue': '#2e59d9',    # Orta mavi\n",
        "        'light_blue': '#6c8cd5',     # Açık mavi\n",
        "        'sky_blue': '#a4c2f4',       # Çok açık mavi\n",
        "    },\n",
        "    'accent': {\n",
        "        'highlight': '#ffa500',      # Turuncu (vurgu için)\n",
        "        'warning': '#ff6b6b',        # Açık kırmızı (uyarı)\n",
        "    },\n",
        "    'neutral': {\n",
        "        'dark_gray': '#2c3e50',      # Koyu gri (text)\n",
        "        'medium_gray': '#7f8c8d',    # Orta gri\n",
        "        'light_gray': '#ecf0f1',     # Açık gri (background)\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# FILE PATHS\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"🔧 Google Colab tespit edildi - Google Drive mount ediliyor...\")\n",
        "    drive.mount('/content/drive', timeout_ms=120000, force_remount=False)\n",
        "    BASE_PATH = \"/content/drive/My Drive/Thesis Data/\"\n",
        "    print(\"✅ Google Drive başarıyla mount edildi!\\n\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ Google Colab bulunamadı - Yerel ortam kullanılıyor\\n\")\n",
        "    BASE_PATH = \"./Thesis Data/\"\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Google Drive mount hatası: {e}\")\n",
        "    print(\"Yerel ortama geçiş yapılıyor...\\n\")\n",
        "    BASE_PATH = \"./Thesis Data/\"\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 🆕 LOAD EDA-GUIDED ARTIFACTS (FULLY FIXED)\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "eda_config = CONFIG.get(\"eda_integration\", {})\n",
        "\n",
        "if eda_config.get(\"enabled\", False):\n",
        "    print(\"\\n[EDA-GUIDED] Loading EDA artifacts...\")\n",
        "\n",
        "    try:\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "        # ✅ PATH CONSTRUCTION\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # Base path (eda_integration'dan)\n",
        "        eda_base = os.path.join(BASE_PATH, eda_config.get(\"base_path\", \"\"))\n",
        "\n",
        "        # Drop list path (eda_pipeline'dan - çünkü orada .py dosyası var!)\n",
        "        pipeline_config = CONFIG.get(\"eda_pipeline\", {})\n",
        "        drop_list_file = os.path.join(BASE_PATH, pipeline_config.get(\"drop_list_path\", \"\"))\n",
        "\n",
        "        # MI scores path (eda_integration'dan)\n",
        "        mi_scores_file = os.path.join(BASE_PATH, eda_config.get(\"mi_scores_path\", \"\"))\n",
        "\n",
        "        print(f\"\\n  📁 Paths:\")\n",
        "        print(f\"     EDA Base:   {eda_base}\")\n",
        "        print(f\"     Drop list:  {drop_list_file}\")\n",
        "        print(f\"     MI scores:  {mi_scores_file}\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "        # ✅ LOAD DROP LIST (FIXED: Global scope için)\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # Default values (eğer dosya yoksa)\n",
        "        features_to_drop_multicollinearity_REVISED = []\n",
        "        features_to_keep_diff = []\n",
        "\n",
        "        if os.path.exists(drop_list_file):\n",
        "            print(f\"\\n  🔄 Loading feature drop list: {os.path.basename(drop_list_file)}\")\n",
        "\n",
        "            try:\n",
        "                # ✅ FIX: exec() yerine compile + exec with globals()\n",
        "                with open(drop_list_file, 'r', encoding='utf-8') as f:\n",
        "                    code_content = f.read()\n",
        "\n",
        "                # Global namespace'e yükle\n",
        "                exec_globals = {}\n",
        "                exec(compile(code_content, drop_list_file, 'exec'), exec_globals)\n",
        "\n",
        "                # Değişkenleri al\n",
        "                if 'features_to_drop_multicollinearity_REVISED' in exec_globals:\n",
        "                    features_to_drop_multicollinearity_REVISED = exec_globals['features_to_drop_multicollinearity_REVISED']\n",
        "                    print(f\"  ✅ Features to drop: {len(features_to_drop_multicollinearity_REVISED)}\")\n",
        "\n",
        "                    # Sample features göster\n",
        "                    if len(features_to_drop_multicollinearity_REVISED) > 0:\n",
        "                        print(f\"     Sample (first 5):\")\n",
        "                        for feat in list(features_to_drop_multicollinearity_REVISED)[:5]:\n",
        "                            print(f\"       • {feat}\")\n",
        "                else:\n",
        "                    print(f\"  ⚠️  features_to_drop_multicollinearity_REVISED not found\")\n",
        "\n",
        "                if 'features_to_keep_diff' in exec_globals:\n",
        "                    features_to_keep_diff = exec_globals['features_to_keep_diff']\n",
        "                    print(f\"  ✅ Diff features to keep: {len(features_to_keep_diff)}\")\n",
        "\n",
        "                    # Sample diff features\n",
        "                    if len(features_to_keep_diff) > 0:\n",
        "                        print(f\"     Sample:\")\n",
        "                        for feat in features_to_keep_diff[:5]:\n",
        "                            print(f\"       • {feat}\")\n",
        "                else:\n",
        "                    print(f\"  ⚠️  features_to_keep_diff not found\")\n",
        "\n",
        "            except Exception as e_drop:\n",
        "                print(f\"  ❌ Error executing drop list file: {e_drop}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(f\"  ⚠️  Drop list not found: {drop_list_file}\")\n",
        "            print(f\"     File exists: {os.path.exists(drop_list_file)}\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "        # ✅ LOAD MI SCORES\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        mi_scores_eda_df = None\n",
        "\n",
        "        if os.path.exists(mi_scores_file):\n",
        "            print(f\"\\n  🔄 Loading MI scores: {os.path.basename(mi_scores_file)}\")\n",
        "\n",
        "            try:\n",
        "                mi_scores_eda_df = pd.read_csv(mi_scores_file)\n",
        "                print(f\"  ✅ Loaded MI scores for {len(mi_scores_eda_df)} features\")\n",
        "\n",
        "                # Column kontrolü\n",
        "                print(f\"     Columns: {list(mi_scores_eda_df.columns)}\")\n",
        "\n",
        "                if len(mi_scores_eda_df) > 0:\n",
        "                    # Column adını kontrol et (Feature vs feature vs MI_Feature)\n",
        "                    feature_col = None\n",
        "                    score_col = None\n",
        "\n",
        "                    for col in mi_scores_eda_df.columns:\n",
        "                        if 'feature' in col.lower():\n",
        "                            feature_col = col\n",
        "                        if 'mi' in col.lower() or 'score' in col.lower():\n",
        "                            score_col = col\n",
        "\n",
        "                    if feature_col and score_col:\n",
        "                        top_feature = mi_scores_eda_df.iloc[0]\n",
        "                        print(f\"  ℹ️  Top feature: {top_feature[feature_col]} ({score_col}={top_feature[score_col]:.4f})\")\n",
        "                    else:\n",
        "                        print(f\"  ⚠️  Could not identify feature/score columns\")\n",
        "                        # Fallback: ilk iki column'u kullan\n",
        "                        if len(mi_scores_eda_df.columns) >= 2:\n",
        "                            print(f\"     Using first two columns as feature/score\")\n",
        "\n",
        "            except Exception as e_mi:\n",
        "                print(f\"  ❌ Error loading MI scores: {e_mi}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(f\"  ⚠️  MI scores not found: {mi_scores_file}\")\n",
        "            print(f\"     File exists: {os.path.exists(mi_scores_file)}\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "        # ✅ SUMMARY\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        print(f\"\\n  📊 EDA Artifacts Summary:\")\n",
        "        print(f\"     Features to drop:  {len(features_to_drop_multicollinearity_REVISED)}\")\n",
        "        print(f\"     Diff features:     {len(features_to_keep_diff)}\")\n",
        "        print(f\"     MI scores loaded:  {'✅ Yes' if mi_scores_eda_df is not None else '❌ No'}\")\n",
        "        print(f\"\\n  ✅ EDA artifacts loading complete\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Critical error loading EDA artifacts: {e}\")\n",
        "        print(f\"  Continuing without EDA guidance...\\n\")\n",
        "\n",
        "        # Full traceback\n",
        "        import traceback\n",
        "        print(f\"  📋 Full error:\")\n",
        "        traceback.print_exc()\n",
        "        print()\n",
        "\n",
        "        # Fallback values\n",
        "        features_to_drop_multicollinearity_REVISED = []\n",
        "        features_to_keep_diff = []\n",
        "        mi_scores_eda_df = None\n",
        "\n",
        "else:\n",
        "    print(\"\\n[EDA-GUIDED] EDA integration disabled in CONFIG\\n\")\n",
        "    features_to_drop_multicollinearity_REVISED = []\n",
        "    features_to_keep_diff = []\n",
        "    mi_scores_eda_df = None\n",
        "\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "MATCHES_PATH = os.path.join(BASE_PATH, \"Matches.csv\")\n",
        "ELOR_PATH = os.path.join(BASE_PATH, \"EloRatings.csv\")\n",
        "TEAM_FEATURES_PATH = os.path.join(BASE_PATH, \"data.xlsx\")\n",
        "TEAM_LIST_PATH = os.path.join(BASE_PATH, \"Takim_Listesi_Temiz_1.csv\")\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"📁 DOSYA YOLLARI VE KONTROL\")\n",
        "print(\"=\"*100)\n",
        "print(f\"📁 BASE_PATH: {BASE_PATH}\")\n",
        "print(f\"📄 MATCHES_PATH: {MATCHES_PATH}\")\n",
        "print(f\"   Status: {'✅ BULUNDU' if os.path.exists(MATCHES_PATH) else '❌ BULUNAMADI'}\")\n",
        "print(f\"📄 ELOR_PATH: {ELOR_PATH}\")\n",
        "print(f\"   Status: {'✅ BULUNDU' if os.path.exists(ELOR_PATH) else '❌ BULUNAMADI'}\")\n",
        "print(f\"📄 TEAM_FEATURES_PATH: {TEAM_FEATURES_PATH}\")\n",
        "print(f\"   Status: {'✅ BULUNDU' if os.path.exists(TEAM_FEATURES_PATH) else '❌ BULUNAMADI'}\")\n",
        "print(f\"\\n[OUTPUT] {BASE_PATH} NO ODDS  SMOTE DRAW 1.1 V2 25.11.25   Main Model\\n\")\n",
        "\n",
        "OUT_DIR = os.path.join(BASE_PATH, \" NO ODDS  SMOTE DRAW 1.1 V2 25.11.25    Main Model\")\n",
        "DATA_DIR = os.path.join(OUT_DIR, \"data\")\n",
        "GRAPHICS_DIR = os.path.join(OUT_DIR, \"graphics\")\n",
        "TABLES_DIR = os.path.join(OUT_DIR, \"tables\")\n",
        "\n",
        "for d in [OUT_DIR, DATA_DIR, GRAPHICS_DIR, TABLES_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "# DATA INTEGRATION CLASS\n",
        "class DataIntegration:\n",
        "    \"\"\"Advanced data integration with fuzzy matching\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=BASE_PATH):\n",
        "        self.base_path = base_path\n",
        "        self.df_matches = None\n",
        "        self.df_elo = None\n",
        "        self.df_tm = None\n",
        "        self.team_mapping = {}\n",
        "        self.team_scores = {}\n",
        "\n",
        "        def _helper_normalize_for_key(s):\n",
        "            if pd.isna(s):\n",
        "                return None\n",
        "            s = str(s).strip()\n",
        "            s = unidecode(s)\n",
        "\n",
        "            suffixes = [\n",
        "                ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc', ' SK', ' sk', ' Sk',\n",
        "                ' FK', ' fk', ' Fk', ' AC', ' ac', ' Ac', ' AS', ' as', ' As',\n",
        "                ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf', ' HSC', ' hsc', ' Hsc',\n",
        "                ' BC', ' bc', ' Bc', ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',\n",
        "                ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',\n",
        "                ' 1909', ' 1846', ' 1860', ' 2010', ' 1913', ' 1907', ' 1936', ' 1919', ' 2025',\n",
        "                ' spor', ' kulübü', ' klub',\n",
        "            ]\n",
        "\n",
        "            for suffix in suffixes:\n",
        "                s = s.replace(suffix, '')\n",
        "\n",
        "            s = s.replace(\"'\", \"\")\n",
        "            s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            words = sorted(s.lower().split())\n",
        "            s = ' '.join(words)\n",
        "            s = re.sub(r'\\s+', ' ', s).strip()\n",
        "            return s if s else None\n",
        "\n",
        "        self.MANUAL_OVERRIDES = {}\n",
        "        team_list_csv_path = TEAM_LIST_PATH\n",
        "\n",
        "        try:\n",
        "            encoding_options = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "            df_temp = None\n",
        "            for encoding in encoding_options:\n",
        "                try:\n",
        "                    df_temp = pd.read_csv(\n",
        "                        team_list_csv_path,\n",
        "                        header=None,\n",
        "                        skiprows=1,\n",
        "                        quotechar='\"',\n",
        "                        sep='|',\n",
        "                        encoding=encoding\n",
        "                    )\n",
        "                    print(f\"✅ Dosya başarıyla '{encoding}' ile okundu\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if df_temp is None:\n",
        "                raise Exception(\"CSV dosyası hiçbir encoding ile okunamadı!\")\n",
        "\n",
        "            df_team_list = df_temp[0].str.split('\\t', expand=True)\n",
        "            df_team_list.columns = ['Matches', 'Transfermarkt']\n",
        "\n",
        "            def _process_team_row(row):\n",
        "                matches_name = row['Matches']\n",
        "                transfermarkt_name = row['Transfermarkt']\n",
        "\n",
        "                if pd.notna(matches_name) and pd.notna(transfermarkt_name):\n",
        "                    normalized_key = _helper_normalize_for_key(matches_name)\n",
        "                    normalized_value = _helper_normalize_for_key(transfermarkt_name)\n",
        "\n",
        "                    if normalized_key and normalized_value:\n",
        "                        return normalized_key, normalized_value\n",
        "                return None\n",
        "\n",
        "            for result in df_team_list.apply(_process_team_row, axis=1):\n",
        "                if result is not None:\n",
        "                    key, value = result\n",
        "                    self.MANUAL_OVERRIDES[key] = value\n",
        "\n",
        "            print(\"Manüel kod içi override kuralları uygulanıyor...\")\n",
        "\n",
        "            tm_ac_ajaccio_normalized = _helper_normalize_for_key(\"AC Ajaccio\")\n",
        "            tm_gfc_ajaccio_normalized = _helper_normalize_for_key(\"GFC Ajaccio\")\n",
        "            tm_cordoba_normalized = _helper_normalize_for_key(\"Córdoba CF\")\n",
        "\n",
        "            key_aja = _helper_normalize_for_key(\"Ajaccio\")\n",
        "            if key_aja and tm_ac_ajaccio_normalized:\n",
        "                self.MANUAL_OVERRIDES.pop(key_aja, None)\n",
        "                self.MANUAL_OVERRIDES[key_aja] = tm_ac_ajaccio_normalized\n",
        "                print(f\"  Override: '{key_aja}' -> '{tm_ac_ajaccio_normalized}' (AC Ajaccio)\")\n",
        "\n",
        "            key_gfco = _helper_normalize_for_key(\"Ajaccio GFCO\")\n",
        "            if key_gfco and tm_gfc_ajaccio_normalized:\n",
        "                self.MANUAL_OVERRIDES[key_gfco] = tm_gfc_ajaccio_normalized\n",
        "                print(f\"  Override: '{key_gfco}' -> '{tm_gfc_ajaccio_normalized}' (GFC Ajaccio)\")\n",
        "\n",
        "            key_cor = _helper_normalize_for_key(\"Cordoba\")\n",
        "            if key_cor and tm_cordoba_normalized:\n",
        "                self.MANUAL_OVERRIDES[key_cor] = tm_cordoba_normalized\n",
        "                print(f\"  Override: '{key_cor}' -> '{tm_cordoba_normalized}' (Córdoba CF)\")\n",
        "\n",
        "            print(f\"[OVERRIDE RULES] {len(self.MANUAL_OVERRIDES)} takım CSV dosyasından dinamik ve normalize olarak tanımlandı\\n\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ UYARI: Takım listesi CSV'si bulunamadı: {team_list_csv_path}\")\n",
        "            print(\"Manuel override listesi boş olacak! Eşleşme oranı çok düşük kalacak.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ HATA: Takım listesi CSV'si okunurken hata: {e}\")\n",
        "            print(\"Manuel override listesi boş olabilir.\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load all three data sources\"\"\"\n",
        "        print(\"[STEP 1] 📊 VERİ YÜKLEME\\n\")\n",
        "\n",
        "        try:\n",
        "            print(\"📄 Matches yüklüyor...\")\n",
        "            self.df_matches = pd.read_csv(os.path.join(self.base_path, \"Matches.csv\"), low_memory=False)\n",
        "            print(f\"  ✓ {len(self.df_matches):,} satır, {len(self.df_matches.columns)} sütun\")\n",
        "\n",
        "            print(\"\\n📄 ELO Ratings yüklüyor...\")\n",
        "            self.df_elo = pd.read_csv(os.path.join(self.base_path, \"EloRatings.csv\"), low_memory=False)\n",
        "            print(f\"  ✓ {len(self.df_elo):,} satır, {len(self.df_elo.columns)} sütun\")\n",
        "\n",
        "            print(\"\\n📄 Transfermarkt yüklüyor...\")\n",
        "            self.df_tm = pd.read_excel(os.path.join(self.base_path, \"data.xlsx\"))\n",
        "            print(f\"  ✓ {len(self.df_tm):,} satır, {len(self.df_tm.columns)} sütun\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ HATA: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            raise\n",
        "\n",
        "    def normalize_team_name(self, s):\n",
        "        \"\"\"Normalize team names for matching\"\"\"\n",
        "        if pd.isna(s):\n",
        "            return None\n",
        "\n",
        "        s = str(s).strip()\n",
        "        s = unidecode(s)\n",
        "\n",
        "        suffixes = [\n",
        "            ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc', ' SK', ' sk', ' Sk',\n",
        "            ' FK', ' fk', ' Fk', ' AC', ' ac', ' Ac', ' AS', ' as', ' As',\n",
        "            ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf', ' HSC', ' hsc', ' Hsc',\n",
        "            ' BC', ' bc', ' Bc', ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',\n",
        "            ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',\n",
        "            ' 1909', ' 1846', ' 1860', ' 2010', ' 1913', ' 1907', ' 1936', ' 1919', ' 2025',\n",
        "            ' spor', ' kulübü', ' klub',\n",
        "        ]\n",
        "\n",
        "        for suffix in suffixes:\n",
        "            s = s.replace(suffix, '')\n",
        "\n",
        "        s = s.replace(\"'\", \"\")\n",
        "        s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        words = sorted(s.lower().split())\n",
        "        s = ' '.join(words)\n",
        "        s = re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "        if s in self.MANUAL_OVERRIDES:\n",
        "            s = self.MANUAL_OVERRIDES[s]\n",
        "\n",
        "        return s if s else None\n",
        "\n",
        "    def find_best_match(self, match_team, tm_teams, threshold=80):\n",
        "        \"\"\"Find best team match using fuzzy matching\"\"\"\n",
        "        if pd.isna(match_team):\n",
        "            return None, 0\n",
        "\n",
        "        match_norm = self.normalize_team_name(match_team)\n",
        "\n",
        "        if not match_norm:\n",
        "            return None, 0\n",
        "\n",
        "        tm_norm_dict = {norm: orig for orig, norm in tm_teams if norm}\n",
        "\n",
        "        if match_norm in tm_norm_dict:\n",
        "            return tm_norm_dict[match_norm], 100\n",
        "\n",
        "        best_score = 0\n",
        "        best_match = None\n",
        "\n",
        "        for tm_team_orig, tm_norm in tm_teams:\n",
        "            if not tm_norm:\n",
        "                continue\n",
        "\n",
        "            score = fuzz.token_sort_ratio(match_norm, tm_norm)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = tm_team_orig\n",
        "\n",
        "        if best_score >= threshold:\n",
        "            return best_match, best_score\n",
        "        return None, best_score\n",
        "\n",
        "    def create_team_mapping(self):\n",
        "        \"\"\"Create team mapping between Matches and Transfermarkt\"\"\"\n",
        "        print(\"\\n\" + \"-\"*100)\n",
        "        print(\"[STEP 2] 🔗 TAKIMLAR EŞLEŞTIRILMESI (FUZZY MATCHING)\\n\")\n",
        "\n",
        "        matches_teams = sorted(\n",
        "            set(self.df_matches['HomeTeam'].dropna().unique()) | \\\n",
        "            set(self.df_matches['AwayTeam'].dropna().unique())\n",
        "        )\n",
        "\n",
        "        tm_teams_orig = sorted(self.df_tm['ClubName'].dropna().unique())\n",
        "\n",
        "        tm_teams_normalized = []\n",
        "        for team in tm_teams_orig:\n",
        "            tm_teams_normalized.append((team, self.normalize_team_name(team)))\n",
        "\n",
        "        tm_teams_norm_set = set(n for o, n in tm_teams_normalized if n)\n",
        "\n",
        "        print(f\"Matches'te unique takımlar: {len(matches_teams)}\")\n",
        "        print(f\"Transfermarkt'ta unique takımlar: {len(tm_teams_orig)}\\n\")\n",
        "\n",
        "        print(\"🔄 Takım adı eşleştirmesi yapılıyor...\\n\")\n",
        "\n",
        "        for idx, team in enumerate(matches_teams):\n",
        "            if idx % 50 == 0 and idx > 0:\n",
        "                print(f\"  Progress: {idx}/{len(matches_teams)}\", end='\\r')\n",
        "\n",
        "            mapped_team, score = self.find_best_match(team, tm_teams_normalized, threshold=80)\n",
        "\n",
        "            self.team_mapping[team] = mapped_team\n",
        "            self.team_scores[team] = score\n",
        "\n",
        "        print(f\"   ✓ Mapping tamamlandı ({len(matches_teams)} takım)                 \\n\")\n",
        "\n",
        "        matched_count = sum(1 for v in self.team_mapping.values() if v is not None)\n",
        "        print(f\"✅ Eşleştirilen takımlar: {matched_count}/{len(self.team_mapping)} ({matched_count/len(self.team_mapping)*100:.1f}%)\")\n",
        "\n",
        "        unmatched = {k: v for k, v in self.team_mapping.items() if v is None}\n",
        "        if unmatched:\n",
        "            print(f\"\\n⚠️ Eşleşmeyen takımlar ({len(unmatched)}):\")\n",
        "            unmatched_sorted = sorted(unmatched.keys())\n",
        "            for i, team in enumerate(unmatched_sorted[:15]):\n",
        "                print(f\"  • {team}\")\n",
        "            if len(unmatched) > 15:\n",
        "                print(f\"  ... ve {len(unmatched)-15} daha\")\n",
        "\n",
        "    def clean_and_filter_data(self):\n",
        "        \"\"\"Clean and filter data\"\"\"\n",
        "        print(\"\\n\" + \"-\"*100)\n",
        "        print(\"[STEP 3] 🔧 VERİ TEMİZLEME VE FİLTRELEME\\n\")\n",
        "\n",
        "        print(\"Matches temizleniyor...\")\n",
        "        self.df_matches['MatchDate'] = pd.to_datetime(self.df_matches['MatchDate'], errors='coerce')\n",
        "\n",
        "        TOP_5_DIVISIONS = ['D1', 'F1', 'E0', 'I1', 'SP1']\n",
        "        before = len(self.df_matches)\n",
        "        self.df_matches = self.df_matches[self.df_matches['Division'].isin(TOP_5_DIVISIONS)].copy()\n",
        "        after = len(self.df_matches)\n",
        "        print(f\"   TOP 5 LİG filtresi: {before:,} → {after:,}\")\n",
        "\n",
        "        before = len(self.df_matches)\n",
        "        self.df_matches = self.df_matches[self.df_matches['MatchDate'] >= '2015-01-01'].copy()\n",
        "        after = len(self.df_matches)\n",
        "        print(f\"   Tarih filtresi (2015+): {before:,} → {after:,}\")\n",
        "\n",
        "        before = len(self.df_matches)\n",
        "        self.df_matches = self.df_matches.dropna(subset=['HomeTeam', 'AwayTeam', 'MatchDate']).copy()\n",
        "        after = len(self.df_matches)\n",
        "        print(f\"   Null takımlar/tarihler kaldırıldı: {before:,} → {after:,}\")\n",
        "        print(f\"   ✅ Final Matches: {len(self.df_matches):,} maç\\n\")\n",
        "\n",
        "        print(\"Transfermarkt temizleniyor...\")\n",
        "        TOP_5_LEAGUES_TM = ['ES1', 'FR1', 'GB1', 'IT1', 'L1']\n",
        "        before = len(self.df_tm)\n",
        "        self.df_tm = self.df_tm[self.df_tm['wettbewerb_id'].isin(TOP_5_LEAGUES_TM)].copy()\n",
        "        after = len(self.df_tm)\n",
        "        print(f\"   TOP 5 LİG filtresi: {before:,} → {after:,}\")\n",
        "\n",
        "        before = len(self.df_tm)\n",
        "        self.df_tm = self.df_tm.dropna(subset=['ClubName']).copy()\n",
        "        after = len(self.df_tm)\n",
        "        print(f\"   Null ClubName kaldırıldı: {before:,} → {after:,}\")\n",
        "        print(f\"   ✅ Final Transfermarkt: {len(self.df_tm):,} satır\\n\")\n",
        "\n",
        "    def add_season_info(self):\n",
        "        \"\"\"Add season info to matches\"\"\"\n",
        "        print(\"[STEP 4] ⏰ SEZON VE TARIH BİLGİSİ EKLEME\\n\")\n",
        "\n",
        "        self.df_matches['Season'] = self.df_matches['MatchDate'].dt.year.astype(str) + '-' + \\\n",
        "                                    (self.df_matches['MatchDate'].dt.year + 1).astype(str)\n",
        "\n",
        "        mask = self.df_matches['MatchDate'].dt.month < 7\n",
        "        self.df_matches.loc[mask, 'Season'] = \\\n",
        "            (self.df_matches.loc[mask, 'MatchDate'].dt.year - 1).astype(str) + '-' + \\\n",
        "            self.df_matches.loc[mask, 'MatchDate'].dt.year.astype(str)\n",
        "\n",
        "        self.df_matches['YearMonth'] = self.df_matches['MatchDate'].dt.to_period('M').astype(str)\n",
        "\n",
        "        print(f\"Unique seasons (sample): {sorted(self.df_matches['Season'].unique())[:10]}\\n\")\n",
        "        print(f\"Unique YearMonths (sample): {sorted(self.df_matches['YearMonth'].unique())[:12]}\\n\")\n",
        "        print(f\"✅ Season ve YearMonth eklendi\\n\")\n",
        "\n",
        "    def merge_data(self):\n",
        "        \"\"\"Merge all data sources\"\"\"\n",
        "        print(\"=\"*100)\n",
        "        print(\"[STEP 5] 🔀 MERGE İŞLEMİ\\n\")\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        if self.df_tm.columns.duplicated().any():\n",
        "            print(\"[FIX] ⚠️ Transfermarkt'ta duplicate column names tespit edildi, temizleniyor...\")\n",
        "            cols = pd.Series(self.df_tm.columns)\n",
        "            for dup in cols[cols.duplicated(keep=False)].unique():\n",
        "                dups = np.where(cols == dup)[0]\n",
        "                cols.iloc[dups] = [f\"{dup}_{i}\" for i in range(len(dups))]\n",
        "            self.df_tm.columns = cols\n",
        "            print(f\"    ✅ Temizlendi!\\n\")\n",
        "\n",
        "        self.clean_and_filter_data()\n",
        "        self.create_team_mapping()\n",
        "        self.add_season_info()\n",
        "\n",
        "        print(\"[MERGE] Fuzzy mapping uygulanıyor...\\n\")\n",
        "        self.df_matches['HomeTeam_TM'] = self.df_matches['HomeTeam'].map(self.team_mapping)\n",
        "        self.df_matches['AwayTeam_TM'] = self.df_matches['AwayTeam'].map(self.team_mapping)\n",
        "\n",
        "        unmapped_matches = self.df_matches[\n",
        "            (self.df_matches['HomeTeam_TM'].isna()) | (self.df_matches['AwayTeam_TM'].isna())\n",
        "        ]\n",
        "        print(f\"Eşleşmeyen maçlar: {len(unmapped_matches):,}\\n\")\n",
        "\n",
        "        print(\"[FIX] Level 1 (YearMonth) verisi hazırlanıyor...\")\n",
        "        df_tm_yearmonth = self.df_tm.drop_duplicates(\n",
        "            subset=['ClubName', 'YearMonth'],\n",
        "            keep='last'\n",
        "        ).copy()\n",
        "        print(f\"   ✓ {len(df_tm_yearmonth):,} unique satır\")\n",
        "\n",
        "        print(\"\\n[FIX] Level 2 (Season) verisi hazırlanıyor...\")\n",
        "\n",
        "        numeric_tm_cols = self.df_tm.select_dtypes(include=np.number).columns\n",
        "        numeric_tm_cols = [c for c in numeric_tm_cols if c not in ['club_id']]\n",
        "\n",
        "        non_numeric_tm_cols = self.df_tm.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "        if 'club_id' in self.df_tm.columns and 'club_id' not in non_numeric_tm_cols:\n",
        "            non_numeric_tm_cols.append('club_id')\n",
        "\n",
        "        non_numeric_tm_cols = [c for c in non_numeric_tm_cols if c not in ['ClubName', 'Season', 'YearMonth', 'wettbewerb_id', 'LeagueName']]\n",
        "\n",
        "        agg_dict = {}\n",
        "        for col in numeric_tm_cols:\n",
        "            agg_dict[col] = 'mean'\n",
        "        for col in non_numeric_tm_cols:\n",
        "            if col in self.df_tm.columns:\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        grouping_cols = ['ClubName', 'Season']\n",
        "        valid_agg_dict = {k: v for k, v in agg_dict.items() if k in self.df_tm.columns}\n",
        "\n",
        "        df_tm_season = self.df_tm.groupby(grouping_cols, as_index=False).agg(valid_agg_dict)\n",
        "        print(f\"   ✓ {len(df_tm_season):,} unique satır\\n\")\n",
        "\n",
        "        df_merged = self.df_matches.copy()\n",
        "\n",
        "        print(\"   🏠 [LEVEL 1] HomeTeam merge...\")\n",
        "        df_tm_home_l1 = df_tm_yearmonth.rename(columns={'ClubName': 'HomeTeam_TM'})\n",
        "\n",
        "        df_merged = df_merged.merge(\n",
        "            df_tm_home_l1,\n",
        "            on=['HomeTeam_TM', 'YearMonth'],\n",
        "            how='left',\n",
        "            suffixes=('', '_home_tm')\n",
        "        )\n",
        "\n",
        "        home_cols = [col for col in df_tm_home_l1.columns if col not in ['HomeTeam_TM', 'YearMonth']]\n",
        "\n",
        "        unmatched_l1_mask = pd.Series(False, index=df_merged.index)\n",
        "        if home_cols:\n",
        "            unmatched_l1_mask = df_merged[home_cols[0]].isna()\n",
        "        unmatched_l1_indices = df_merged[unmatched_l1_mask].index\n",
        "\n",
        "        if len(unmatched_l1_indices) > 0:\n",
        "            print(f\"       [LEVEL 2] {len(unmatched_l1_indices):,} satırda Season fallback...\")\n",
        "\n",
        "            fallback_data = df_merged.loc[unmatched_l1_indices, ['HomeTeam_TM', 'Season']]\n",
        "            df_tm_home_l2 = df_tm_season.rename(columns={'ClubName': 'HomeTeam_TM'})\n",
        "\n",
        "            fallback_merged = fallback_data.merge(\n",
        "                df_tm_home_l2,\n",
        "                on=['HomeTeam_TM', 'Season'],\n",
        "                how='left',\n",
        "                suffixes=('', '_home_tm')\n",
        "            )\n",
        "\n",
        "            for col in home_cols:\n",
        "                if col in fallback_merged.columns:\n",
        "                    fallback_merged.index = fallback_data.index\n",
        "                    df_merged.loc[unmatched_l1_indices, col] = fallback_merged[col]\n",
        "\n",
        "        non_null_home = df_merged[home_cols[0]].notna().sum() if home_cols else 0\n",
        "        print(f\"       Non-null HomeTeam: {non_null_home:,}\")\n",
        "\n",
        "        rename_dict_home = {col: 'HomeTeam_' + col for col in home_cols if col in df_merged.columns}\n",
        "        df_merged = df_merged.rename(columns=rename_dict_home)\n",
        "\n",
        "        print(\"   ✈️  [LEVEL 1] AwayTeam merge...\")\n",
        "        df_tm_away_l1 = df_tm_yearmonth.rename(columns={'ClubName': 'AwayTeam_TM'})\n",
        "\n",
        "        df_merged = df_merged.merge(\n",
        "            df_tm_away_l1,\n",
        "            on=['AwayTeam_TM', 'YearMonth'],\n",
        "            how='left',\n",
        "            suffixes=('', '_away_tm')\n",
        "        )\n",
        "\n",
        "        away_cols = [col for col in df_tm_away_l1.columns if col not in ['AwayTeam_TM', 'YearMonth']]\n",
        "\n",
        "        unmatched_l1_mask_away = pd.Series(False, index=df_merged.index)\n",
        "        if away_cols:\n",
        "            away_check_col = away_cols[0]\n",
        "            if away_check_col + '_away_tm' in df_merged.columns:\n",
        "                away_check_col = away_check_col + '_away_tm'\n",
        "            elif away_check_col not in df_merged.columns:\n",
        "                for c in away_cols[1:]:\n",
        "                    if c in df_merged.columns:\n",
        "                        away_check_col = c\n",
        "                        break\n",
        "                    elif c + '_away_tm' in df_merged.columns:\n",
        "                        away_check_col = c + '_away_tm'\n",
        "                        break\n",
        "                else:\n",
        "                    away_check_col = None\n",
        "\n",
        "            if away_check_col:\n",
        "                unmatched_l1_mask_away = df_merged[away_check_col].isna()\n",
        "\n",
        "        unmatched_l1_indices_away = df_merged[unmatched_l1_mask_away].index\n",
        "\n",
        "        if len(unmatched_l1_indices_away) > 0:\n",
        "            print(f\"       [LEVEL 2] {len(unmatched_l1_indices_away):,} satırda Season fallback...\")\n",
        "\n",
        "            fallback_data_away = df_merged.loc[unmatched_l1_indices_away, ['AwayTeam_TM', 'Season']]\n",
        "            df_tm_away_l2 = df_tm_season.rename(columns={'ClubName': 'AwayTeam_TM'})\n",
        "\n",
        "            fallback_merged_away = fallback_data_away.merge(\n",
        "                df_tm_away_l2,\n",
        "                on=['AwayTeam_TM', 'Season'],\n",
        "                how='left',\n",
        "                suffixes=('', '_away_tm_l2')\n",
        "            )\n",
        "\n",
        "            for col in away_cols:\n",
        "                target_col = col\n",
        "                source_col = col\n",
        "\n",
        "                if col + '_away_tm' in df_merged.columns:\n",
        "                    target_col = col + '_away_tm'\n",
        "                if col + '_away_tm_l2' in fallback_merged_away.columns:\n",
        "                    source_col = col + '_away_tm_l2'\n",
        "\n",
        "                if source_col in fallback_merged_away.columns and target_col in df_merged.columns:\n",
        "                    fallback_merged_away.index = fallback_data_away.index\n",
        "                    df_merged.loc[unmatched_l1_indices_away, target_col] = fallback_merged_away[source_col]\n",
        "\n",
        "        non_null_away = 0\n",
        "        if away_cols:\n",
        "            away_check_col = away_cols[0]\n",
        "            if away_check_col + '_away_tm' in df_merged.columns:\n",
        "                away_check_col = away_check_col + '_away_tm'\n",
        "            elif away_check_col not in df_merged.columns:\n",
        "                away_check_col = None\n",
        "            if away_check_col:\n",
        "                non_null_away = df_merged[away_check_col].notna().sum()\n",
        "        print(f\"       Non-null AwayTeam: {non_null_away:,}\")\n",
        "\n",
        "        rename_dict_away = {}\n",
        "        for col in away_cols:\n",
        "            suffixed_col = col + '_away_tm'\n",
        "            if suffixed_col in df_merged.columns:\n",
        "                rename_dict_away[suffixed_col] = 'AwayTeam_' + col\n",
        "            elif col in df_merged.columns and not col.startswith('HomeTeam_'):\n",
        "                rename_dict_away[col] = 'AwayTeam_' + col\n",
        "\n",
        "        df_merged = df_merged.rename(columns=rename_dict_away)\n",
        "        df_merged = df_merged.sort_values('MatchDate').reset_index(drop=True)\n",
        "\n",
        "        print(f\"\\n   ✅ Merge tamamlandı: {len(df_merged):,} × {len(df_merged.columns)}\\n\")\n",
        "\n",
        "        print(\"[FEATURE ENGINEERING]\")\n",
        "\n",
        "        if 'HomeElo' in df_merged.columns and 'AwayElo' in df_merged.columns:\n",
        "            df_merged['ELO_Diff'] = df_merged['HomeElo'] - df_merged['AwayElo']\n",
        "            print(\"✓ ELO_Diff oluşturuldu\")\n",
        "        else:\n",
        "            print(\"✗ ELO_Diff oluşturulamadı\")\n",
        "\n",
        "        def create_diff_feature(df, feature_base_name):\n",
        "            home_col = f'HomeTeam_{feature_base_name}'\n",
        "            away_col = f'AwayTeam_{feature_base_name}'\n",
        "            diff_col = f'{feature_base_name}_Diff'\n",
        "            if home_col in df.columns and away_col in df.columns:\n",
        "                df[diff_col] = df[home_col] - df[away_col]\n",
        "                if 'Manager' in feature_base_name:\n",
        "                    df[diff_col] = df[diff_col].fillna(0)\n",
        "                print(f\"✓ {diff_col} oluşturuldu\")\n",
        "            else:\n",
        "                print(f\"✗ {diff_col} oluşturulamadı\")\n",
        "\n",
        "        create_diff_feature(df_merged, 'ClubValue')\n",
        "        create_diff_feature(df_merged, 'MaxPlayerValue')\n",
        "        create_diff_feature(df_merged, 'ManagerTrophies')\n",
        "        create_diff_feature(df_merged, 'ManagerTenureDays')\n",
        "        create_diff_feature(df_merged, 'NetTransferSpending')\n",
        "        create_diff_feature(df_merged, 'n_players_injured')\n",
        "\n",
        "        print(\"\\n[INFO] Creating ValueRatio features...\")\n",
        "        try:\n",
        "            df_merged['HomeTeam_ValueRatio'] = (\n",
        "                df_merged['HomeTeam_ClubValue'] / df_merged['HomeTeam_LeagueValue'].replace(0, np.nan)\n",
        "            ).fillna(0)\n",
        "\n",
        "            df_merged['AwayTeam_ValueRatio'] = (\n",
        "                df_merged['AwayTeam_ClubValue'] / df_merged['AwayTeam_LeagueValue'].replace(0, np.nan)\n",
        "            ).fillna(0)\n",
        "\n",
        "            df_merged['ValueRatio_Diff'] = df_merged['HomeTeam_ValueRatio'] - df_merged['AwayTeam_ValueRatio']\n",
        "            print(\"✓ ValueRatio features created\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ ValueRatio error: {e}\")\n",
        "\n",
        "        print(\"\\n[MISSING VALUES HANDLING]\")\n",
        "        numeric_cols = df_merged.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        cols_to_exclude_fill = ['FTHome', 'FTAway', 'HTHome', 'HTAway',\n",
        "                                'HomeTeam_club_id', 'AwayTeam_club_id']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in cols_to_exclude_fill:\n",
        "                continue\n",
        "\n",
        "            if df_merged[col].isnull().sum() > 0:\n",
        "                median_val = df_merged[col].median()\n",
        "                if pd.isna(median_val):\n",
        "                    median_val = 0\n",
        "                df_merged[col] = df_merged[col].fillna(median_val)\n",
        "\n",
        "        print(\"✓ Missing values filled\\n\")\n",
        "\n",
        "        print(\"\\n[NORMALIZED ODDS]\")\n",
        "        odds_cols_main = ['OddHome', 'OddDraw', 'OddAway']\n",
        "        if all(c in df_merged.columns for c in odds_cols_main):\n",
        "            try:\n",
        "                df_merged['Prob_H_Raw'] = np.where(df_merged['OddHome'].fillna(0) > 0, 1 / df_merged['OddHome'], np.nan)\n",
        "                df_merged['Prob_D_Raw'] = np.where(df_merged['OddDraw'].fillna(0) > 0, 1 / df_merged['OddDraw'], np.nan)\n",
        "                df_merged['Prob_A_Raw'] = np.where(df_merged['OddAway'].fillna(0) > 0, 1 / df_merged['OddAway'], np.nan)\n",
        "\n",
        "                df_merged['Overround'] = df_merged[['Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw']].fillna(0).sum(axis=1)\n",
        "\n",
        "                valid_overround_mask = (df_merged['Overround'].notna()) & (df_merged['Overround'] > 0.1)\n",
        "                df_merged['Prob_H_Norm'] = np.where(valid_overround_mask, df_merged['Prob_H_Raw'] / df_merged['Overround'], df_merged['Prob_H_Raw'])\n",
        "                df_merged['Prob_D_Norm'] = np.where(valid_overround_mask, df_merged['Prob_D_Raw'] / df_merged['Overround'], df_merged['Prob_D_Raw'])\n",
        "                df_merged['Prob_A_Norm'] = np.where(valid_overround_mask, df_merged['Prob_A_Raw'] / df_merged['Overround'], df_merged['Prob_A_Raw'])\n",
        "\n",
        "                df_merged = df_merged.drop(columns=['Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw', 'Overround'], errors='ignore')\n",
        "                print(\"✓ Normalized odds created\")\n",
        "\n",
        "                norm_odds_cols = ['Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm']\n",
        "                for col in norm_odds_cols:\n",
        "                    if col in df_merged.columns and df_merged[col].isnull().any():\n",
        "                        median_val_odds = df_merged[col].median()\n",
        "                        if pd.isna(median_val_odds):\n",
        "                            median_val_odds = 1/3\n",
        "                        df_merged[col] = df_merged[col].fillna(median_val_odds)\n",
        "\n",
        "            except Exception as e_odds_main:\n",
        "                print(f\"✗ Odds error: {e_odds_main}\")\n",
        "        else:\n",
        "            print(\"✗ Odds columns not found\")\n",
        "\n",
        "        print(\"=\"*100)\n",
        "        print(\"✅ DATA INTEGRATION COMPLETE\\n\")\n",
        "        print(f\"Final dataset: {len(df_merged):,} × {len(df_merged.columns)}\")\n",
        "\n",
        "        merged_file = os.path.join(self.base_path, \"merged_final_complete.csv\")\n",
        "        df_merged.to_csv(merged_file, index=False)\n",
        "        print(f\"✅ Saved: merged_final_complete.csv\\n\")\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# ✅ LAG FEATURES GENERATOR\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def create_lag_features(df, feature_cols, windows=[3, 5], home_away_split=True):\n",
        "    \"\"\"\n",
        "    Create lag features from in-game statistics (FIXED VERSION)\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with MatchDate, HomeTeam, AwayTeam, and feature columns\n",
        "        feature_cols: List of feature names to create lags for\n",
        "        windows: List of window sizes (e.g., [3, 5] for last 3 and 5 matches)\n",
        "        home_away_split: If True, separate home/away performance\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with lag features added, list of created feature names\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"🔄 CREATING LAG FEATURES (Leakage-Free Historical Averages)\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    # ✅ FIX 1: Ensure sorted by date and reset index\n",
        "    df = df.sort_values('MatchDate').reset_index(drop=True).copy()\n",
        "\n",
        "    lag_features_created = []\n",
        "\n",
        "    for feat in feature_cols:\n",
        "        if feat not in df.columns:\n",
        "            print(f\"  ⚠️  Skipping {feat} (not in dataset)\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  Processing: {feat}\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════\n",
        "        # HOME TEAM LAG FEATURES\n",
        "        # ═══════════════════════════════════════════════════════════════\n",
        "        for window in windows:\n",
        "            if home_away_split:\n",
        "                # Home performance at home\n",
        "                col_name = f'Home_{feat}_AtHome_Last{window}_Avg'\n",
        "\n",
        "                # ✅ FIX 2: Use transform() instead of apply() to avoid MultiIndex\n",
        "                df[col_name] = df.groupby('HomeTeam')[feat].transform(\n",
        "                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                )\n",
        "                lag_features_created.append(col_name)\n",
        "\n",
        "                # Home performance overall (fallback)\n",
        "                col_name_overall = f'Home_{feat}_Last{window}_Avg'\n",
        "                df[col_name_overall] = df.groupby('HomeTeam')[feat].transform(\n",
        "                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                )\n",
        "                lag_features_created.append(col_name_overall)\n",
        "            else:\n",
        "                # Simple rolling average\n",
        "                col_name = f'Home_{feat}_Last{window}_Avg'\n",
        "                df[col_name] = df.groupby('HomeTeam')[feat].transform(\n",
        "                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                )\n",
        "                lag_features_created.append(col_name)\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════\n",
        "        # AWAY TEAM LAG FEATURES\n",
        "        # ═══════════════════════════════════════════════════════════════\n",
        "        for window in windows:\n",
        "            if home_away_split:\n",
        "                # Determine away feature name\n",
        "                if feat.startswith('Home'):\n",
        "                    away_feat = feat.replace('Home', 'Away')\n",
        "                elif feat.startswith('Away'):\n",
        "                    away_feat = feat\n",
        "                else:\n",
        "                    away_feat = f'Away{feat}'\n",
        "\n",
        "                # Away performance at away\n",
        "                if away_feat in df.columns:\n",
        "                    col_name = f'Away_{feat}_AtAway_Last{window}_Avg'\n",
        "                    df[col_name] = df.groupby('AwayTeam')[away_feat].transform(\n",
        "                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                    )\n",
        "                    lag_features_created.append(col_name)\n",
        "\n",
        "                # Away performance overall\n",
        "                if away_feat in df.columns:\n",
        "                    col_name_overall = f'Away_{feat}_Last{window}_Avg'\n",
        "                    df[col_name_overall] = df.groupby('AwayTeam')[away_feat].transform(\n",
        "                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                    )\n",
        "                    lag_features_created.append(col_name_overall)\n",
        "            else:\n",
        "                # Simple rolling average for away team\n",
        "                if feat.startswith('Home'):\n",
        "                    away_feat = feat.replace('Home', 'Away')\n",
        "                else:\n",
        "                    away_feat = f'Away{feat}'\n",
        "\n",
        "                if away_feat in df.columns:\n",
        "                    col_name = f'Away_{feat}_Last{window}_Avg'\n",
        "                    df[col_name] = df.groupby('AwayTeam')[away_feat].transform(\n",
        "                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "                    )\n",
        "                    lag_features_created.append(col_name)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # FILL NaN VALUES (ilk maçlar için yeterli data yok)\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    print(f\"\\n  Handling NaN values (early season matches)...\")\n",
        "\n",
        "    for col in lag_features_created:\n",
        "        if col in df.columns:\n",
        "            # Strategy 1: Forward fill (use last known value)\n",
        "            df[col] = df[col].fillna(method='ffill')\n",
        "\n",
        "            # Strategy 2: Fill remaining with league average\n",
        "            if df[col].isna().any():\n",
        "                league_avg = df[col].mean()\n",
        "                df[col] = df[col].fillna(league_avg)\n",
        "\n",
        "            # Strategy 3: Fill any remaining with 0\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # SUMMARY\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    print(f\"\\n  ✅ Created {len(lag_features_created)} lag features\")\n",
        "    print(f\"  📊 Original features: {len(feature_cols)}\")\n",
        "    print(f\"  📊 Lag features: {len(lag_features_created)}\")\n",
        "    print(f\"  📊 Windows used: {windows}\")\n",
        "    print(f\"  📊 Home/Away split: {home_away_split}\\n\")\n",
        "\n",
        "    # Sample output\n",
        "    if lag_features_created:\n",
        "        print(\"  Sample lag features created:\")\n",
        "        for feat in lag_features_created[:5]:\n",
        "            non_null = df[feat].notna().sum()\n",
        "            mean_val = df[feat].mean()\n",
        "            print(f\"     • {feat:<50s} (mean: {mean_val:6.2f}, coverage: {non_null/len(df)*100:5.1f}%)\")\n",
        "        if len(lag_features_created) > 5:\n",
        "            print(f\"     ... and {len(lag_features_created)-5} more\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    return df, lag_features_created\n",
        "\n",
        "# FEATURE PREPARATION\n",
        "def prepare_features(df, n_features=None):\n",
        "    \"\"\"Prepare features for modeling\"\"\"\n",
        "\n",
        "    print(\"\\n2️⃣ FEATURE PREPARATION...\\n\")\n",
        "\n",
        "    all_cols = df.columns.tolist()\n",
        "\n",
        "    target_related = ['FTResult', 'FTHome', 'FTAway', 'HTHome', 'HTAway', 'HTResult']\n",
        "    non_numeric_like = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    temp_cols = ['HomeTeam_TM', 'AwayTeam_TM', 'Season', 'YearMonth']\n",
        "    original_teams = ['HomeTeam', 'AwayTeam', 'Division', 'MatchTime']\n",
        "    date_cols = ['MatchDate']\n",
        "    id_cols = [col for col in df.columns if 'club_id' in col]\n",
        "\n",
        "    cols_to_drop = set(target_related + non_numeric_like + temp_cols + original_teams + date_cols + id_cols)\n",
        "\n",
        "    numeric_features = [col for col in df.select_dtypes(include=[np.number]).columns if col not in cols_to_drop]\n",
        "\n",
        "    print(f\"  Features: {len(numeric_features)}\")\n",
        "\n",
        "    X = df[numeric_features].copy()\n",
        "    y = df['FTResult'].copy()\n",
        "\n",
        "    original_y_index = y.index\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_transformed = le.fit_transform(y)\n",
        "    y = pd.Series(y_transformed, index=original_y_index, name='FTResult_Encoded')\n",
        "\n",
        "    print(f\"  Target encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "    if X.isnull().sum().sum() > 0:\n",
        "        print(\"  Filling NaNs...\")\n",
        "        X = X.fillna(X.median())\n",
        "\n",
        "    if np.isinf(X.values).any() or X.isnull().values.any():\n",
        "        print(\"  Fixing inf/NaN values...\")\n",
        "        bad_cols = X.columns[X.isnull().any()].tolist() + X.columns[np.isinf(X.values).any(axis=0)].tolist()\n",
        "        for col in bad_cols:\n",
        "            X[col] = X[col].fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    selected_features = numeric_features\n",
        "\n",
        "    if n_features and n_features > 0 and n_features < X.shape[1]:\n",
        "        print(f\"  Feature selection (Top {n_features})...\")\n",
        "        try:\n",
        "            selector = SelectKBest(mutual_info_classif, k=n_features)\n",
        "            X_new = selector.fit_transform(X, y)\n",
        "            selected_features = np.array(numeric_features)[selector.get_support()].tolist()\n",
        "            X = pd.DataFrame(X_new, columns=selected_features, index=X.index)\n",
        "        except Exception as e:\n",
        "            print(f\"  Selection error: {e}\")\n",
        "            selected_features = numeric_features\n",
        "            X = pd.DataFrame(X, columns=selected_features, index=X.index)\n",
        "    else:\n",
        "        X = pd.DataFrame(X, columns=selected_features, index=X.index)\n",
        "\n",
        "    return X, y, selected_features\n",
        "\n",
        "# LTCN MODEL\n",
        "class LTCN(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"Liquid Time-Constant Network\"\"\"\n",
        "\n",
        "    def __init__(self, T=25, alpha=0.8, beta=0.2, method=\"inverse\",\n",
        "                 function=\"sigmoid\", ridge_alpha=1e-3, random_state=None):\n",
        "        self.T = T\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.method = method\n",
        "        self.function = function\n",
        "        self.ridge_alpha = ridge_alpha\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.W1 = None\n",
        "        self.W2 = None\n",
        "        self.model_ltcn = None\n",
        "        self.classes_ = None\n",
        "        self.n_classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.atleast_2d(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        Y = np.eye(self.n_classes_)[np.searchsorted(self.classes_, y)]\n",
        "\n",
        "        if self.W1 is None:\n",
        "            try:\n",
        "                _, _, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "                W = Vt.T\n",
        "                W_max = np.max(np.abs(W))\n",
        "                self.W1 = W / (W_max + 1e-10)\n",
        "            except (np.linalg.LinAlgError, ValueError):\n",
        "                if self.random_state is not None:\n",
        "                    np.random.seed(self.random_state)\n",
        "                self.W1 = np.random.randn(X.shape[1], X.shape[1]) * 0.1\n",
        "\n",
        "        H = self._compute_chain(X)\n",
        "\n",
        "        try:\n",
        "            if self.method == \"inverse\":\n",
        "                X_aug = np.c_[H, np.ones((H.shape[0], 1))]\n",
        "                self.W2 = np.linalg.pinv(X_aug) @ Y\n",
        "            else:\n",
        "                self.model_ltcn = Ridge(alpha=self.ridge_alpha).fit(H, Y)\n",
        "        except Exception as e:\n",
        "            self.model_ltcn = Ridge(alpha=self.ridge_alpha).fit(H, Y)\n",
        "            self.method = \"ridge\"\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.atleast_2d(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        H = self._compute_chain(X)\n",
        "\n",
        "        if self.method == \"inverse\" and self.W2 is not None:\n",
        "            X_aug = np.c_[H, np.ones((H.shape[0], 1))]\n",
        "            raw = X_aug @ self.W2\n",
        "        elif self.model_ltcn is not None:\n",
        "            raw = self.model_ltcn.predict(H)\n",
        "        else:\n",
        "            raise RuntimeError(\"Model not fitted\")\n",
        "\n",
        "        pred_indices = np.argmax(raw, axis=1)\n",
        "        return self.classes_[pred_indices]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.atleast_2d(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "\n",
        "        H = self._compute_chain(X)\n",
        "\n",
        "        if self.method == \"inverse\" and self.W2 is not None:\n",
        "            X_aug = np.c_[H, np.ones((H.shape[0], 1))]\n",
        "            raw = X_aug @ self.W2\n",
        "        elif self.model_ltcn is not None:\n",
        "            raw = self.model_ltcn.predict(H)\n",
        "        else:\n",
        "            raise RuntimeError(\"Model not fitted\")\n",
        "\n",
        "        raw_max = raw.max(axis=1, keepdims=True)\n",
        "        exps = np.exp(np.clip(raw - raw_max, -500, 500))\n",
        "        proba = exps / exps.sum(axis=1, keepdims=True)\n",
        "\n",
        "        return proba\n",
        "\n",
        "    def _compute_chain(self, A):\n",
        "        A0 = A.copy()\n",
        "        H = A.copy()\n",
        "\n",
        "        for t in range(self.T):\n",
        "            pre_activation = A @ self.W1\n",
        "\n",
        "            if self.function == \"sigmoid\":\n",
        "                Z = 1.0 / (1.0 + np.exp(-np.clip(pre_activation, -500, 500)))\n",
        "            else:\n",
        "                Z = np.tanh(pre_activation)\n",
        "\n",
        "            if Z.ndim > 2:\n",
        "                Z = np.squeeze(Z, axis=-1)\n",
        "\n",
        "            A = self.alpha * Z + self.beta * A0\n",
        "            H = np.c_[H, A]\n",
        "\n",
        "        return H\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def ranked_probability_score(y_true, y_pred_proba):\n",
        "    if y_pred_proba.ndim == 1:\n",
        "        y_pred_proba = np.column_stack([1 - y_pred_proba, y_pred_proba])\n",
        "    K = y_pred_proba.shape[1]\n",
        "    y_true_one_hot = np.eye(K)[y_true]\n",
        "    rps_scores = []\n",
        "    for i in range(len(y_true)):\n",
        "        cum_pred = np.cumsum(y_pred_proba[i])\n",
        "        cum_true = np.cumsum(y_true_one_hot[i])\n",
        "        rps_scores.append(np.sum((cum_pred - cum_true) ** 2))\n",
        "    return np.mean(rps_scores)\n",
        "\n",
        "# ============================================================================\n",
        "# DIRECTORY HELPER\n",
        "# ============================================================================\n",
        "def ensure_directory_exists(path):\n",
        "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
        "    if path and not os.path.exists(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def expected_calibration_error(y_true, y_pred_proba, n_bins=10):\n",
        "    if y_pred_proba.ndim == 1:\n",
        "        y_pred_proba = np.column_stack([1 - y_pred_proba, y_pred_proba])\n",
        "\n",
        "    pred_confidence = np.max(y_pred_proba, axis=1)\n",
        "    pred_label = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    ece = 0\n",
        "    for i in range(n_bins):\n",
        "        mask = (pred_confidence >= bin_edges[i]) & (pred_confidence < bin_edges[i + 1])\n",
        "        if np.sum(mask) > 0:\n",
        "            acc = np.mean(pred_label[mask] == y_true[mask])\n",
        "            conf = np.mean(pred_confidence[mask])\n",
        "            ece += np.abs(acc - conf) * np.sum(mask) / len(y_true)\n",
        "    return ece\n",
        "# ============================================================================\n",
        "# ✅ YENİ EKLEME: SMART FEATURE SELECTION\n",
        "# ============================================================================\n",
        "def smart_feature_selection(X, y, mi_threshold=0.001, corr_threshold=0.85,\n",
        "                            graphics_dir=None, data_dir=None):\n",
        "    \"\"\"\n",
        "    Hybrid feature selection: MI + Correlation based removal\n",
        "\n",
        "    Args:\n",
        "        X: Training features (DataFrame)\n",
        "        y: Target variable\n",
        "        mi_threshold: Minimum MI score to keep (default: 0.001)\n",
        "        corr_threshold: Maximum correlation allowed (default: 0.85)\n",
        "        graphics_dir: Path to save visualizations\n",
        "        data_dir: Path to save analysis results\n",
        "\n",
        "    Returns:\n",
        "        X_clean: Cleaned feature set\n",
        "        all_drops: List of dropped features\n",
        "        mi_df: MI scores dataframe\n",
        "        report: Dictionary with analysis results\n",
        "    \"\"\"\n",
        "    from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"🔬 SMART FEATURE SELECTION (MI + CORRELATION)\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    initial_features = X.shape[1]\n",
        "    print(f\"  📊 Initial features: {initial_features}\")\n",
        "    print(f\"  🎯 MI Threshold: {mi_threshold}\")\n",
        "    print(f\"  🎯 Correlation Threshold: {corr_threshold}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 1: MI CALCULATION\n",
        "    # ========================================================================\n",
        "    print(\"[STEP 1] 📈 Calculating Mutual Information scores...\")\n",
        "    mi_scores = mutual_info_classif(X, y, random_state=42, n_neighbors=3)\n",
        "    mi_df = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'mi_score': mi_scores\n",
        "    }).sort_values('mi_score', ascending=False)\n",
        "\n",
        "    print(f\"  ✅ MI scores calculated\")\n",
        "    print(f\"     Mean MI: {mi_scores.mean():.6f}\")\n",
        "    print(f\"     Median MI: {np.median(mi_scores):.6f}\")\n",
        "    print(f\"     Max MI: {mi_scores.max():.6f}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 2: LOW MI DROP\n",
        "    # ========================================================================\n",
        "    print(f\"[STEP 2] 🗑️  Identifying low-MI features (< {mi_threshold})...\")\n",
        "    low_mi_features = mi_df[mi_df['mi_score'] < mi_threshold]['feature'].tolist()\n",
        "\n",
        "    print(f\"  ⚠️  Low MI features found: {len(low_mi_features)}\")\n",
        "    if low_mi_features:\n",
        "        print(\"     Features to drop:\")\n",
        "        for i, feat in enumerate(low_mi_features[:10], 1):\n",
        "            mi_val = mi_df[mi_df['feature'] == feat]['mi_score'].iloc[0]\n",
        "            print(f\"       {i:2d}. {feat:40s} (MI = {mi_val:.6f})\")\n",
        "        if len(low_mi_features) > 10:\n",
        "            print(f\"       ... and {len(low_mi_features)-10} more\\n\")\n",
        "    else:\n",
        "        print(\"  ✅ All features have sufficient MI scores\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 3: CORRELATION MATRIX (on remaining features)\n",
        "    # ========================================================================\n",
        "    print(f\"[STEP 3] 🔗 Computing correlation matrix...\")\n",
        "    X_temp = X.drop(columns=low_mi_features) if low_mi_features else X.copy()\n",
        "    corr_matrix = X_temp.corr().abs()\n",
        "\n",
        "    # Visualization\n",
        "    if graphics_dir:\n",
        "        fig, ax = plt.subplots(figsize=(20, 16))\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        sns.heatmap(\n",
        "            corr_matrix,\n",
        "            mask=mask,\n",
        "            annot=False,\n",
        "            cmap='RdYlGn_r',\n",
        "            vmin=0, vmax=1,\n",
        "            center=0.5,\n",
        "            square=True,\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={\"shrink\": 0.8}\n",
        "        )\n",
        "        ax.set_title(f'Feature Correlation Matrix (After Low-MI Drop)\\n{X_temp.shape[1]} features',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        corr_path = os.path.join(graphics_dir, '00_correlation_matrix_after_mi.png')\n",
        "        plt.savefig(corr_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  ✅ Correlation heatmap saved: {os.path.basename(corr_path)}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 4: HIGH CORRELATION DROP (keep higher MI)\n",
        "    # ========================================================================\n",
        "    print(f\"[STEP 4] ⚖️  Identifying high-correlation pairs (> {corr_threshold})...\")\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    features_to_drop_corr = []\n",
        "    mi_dict = dict(zip(mi_df['feature'], mi_df['mi_score']))\n",
        "    processed_pairs = set()\n",
        "    high_corr_details = []\n",
        "\n",
        "    for column in upper.columns:\n",
        "        high_corr_features = upper.index[upper[column] > corr_threshold].tolist()\n",
        "        for corr_feat in high_corr_features:\n",
        "            pair_key = tuple(sorted([column, corr_feat]))\n",
        "            if pair_key in processed_pairs:\n",
        "                continue\n",
        "            processed_pairs.add(pair_key)\n",
        "\n",
        "            mi_col = mi_dict.get(column, 0)\n",
        "            mi_corr = mi_dict.get(corr_feat, 0)\n",
        "            corr_value = corr_matrix.loc[column, corr_feat]\n",
        "\n",
        "            if mi_col < mi_corr:\n",
        "                if column not in features_to_drop_corr:\n",
        "                    features_to_drop_corr.append(column)\n",
        "                    high_corr_details.append({\n",
        "                        'Dropped': column,\n",
        "                        'Kept': corr_feat,\n",
        "                        'Correlation': corr_value,\n",
        "                        'Dropped_MI': mi_col,\n",
        "                        'Kept_MI': mi_corr\n",
        "                    })\n",
        "            else:\n",
        "                if corr_feat not in features_to_drop_corr:\n",
        "                    features_to_drop_corr.append(corr_feat)\n",
        "                    high_corr_details.append({\n",
        "                        'Dropped': corr_feat,\n",
        "                        'Kept': column,\n",
        "                        'Correlation': corr_value,\n",
        "                        'Dropped_MI': mi_corr,\n",
        "                        'Kept_MI': mi_col\n",
        "                    })\n",
        "\n",
        "    print(f\"  ⚠️  High-correlation pairs found: {len(high_corr_details)}\")\n",
        "    if high_corr_details:\n",
        "        print(f\"\\n  {'='*90}\")\n",
        "        print(f\"  {'DROPPED FEATURE':<35} {'KEPT FEATURE':<35} {'|r|':>8} {'Strategy':<10}\")\n",
        "        print(f\"  {'='*90}\")\n",
        "        for detail in high_corr_details[:15]:  # Top 15\n",
        "            print(f\"  {detail['Dropped']:<35} {detail['Kept']:<35} \"\n",
        "                  f\"{detail['Correlation']:>8.4f} Keep Higher MI\")\n",
        "        if len(high_corr_details) > 15:\n",
        "            print(f\"  ... and {len(high_corr_details)-15} more pairs\")\n",
        "        print(f\"  {'='*90}\\n\")\n",
        "    else:\n",
        "        print(\"  ✅ No high-correlation pairs found\\n\")\n",
        "\n",
        "    # Save high-corr details\n",
        "    if high_corr_details and data_dir:\n",
        "        high_corr_df = pd.DataFrame(high_corr_details)\n",
        "        high_corr_path = os.path.join(data_dir, 'high_correlation_dropped.csv')\n",
        "        high_corr_df.to_csv(high_corr_path, index=False)\n",
        "        print(f\"  ✅ High-correlation analysis saved: {os.path.basename(high_corr_path)}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 5: FINAL DROP\n",
        "    # ========================================================================\n",
        "    print(f\"[STEP 5] ✂️  Applying feature removal...\")\n",
        "    all_drops = list(set(low_mi_features + features_to_drop_corr))\n",
        "    X_clean = X.drop(columns=all_drops)\n",
        "\n",
        "    print(f\"\\n  📊 SUMMARY:\")\n",
        "    print(f\"  {'='*80}\")\n",
        "    print(f\"     Initial features:              {initial_features}\")\n",
        "    print(f\"     Dropped (Low MI):              {len(low_mi_features)}\")\n",
        "    print(f\"     Dropped (High Correlation):    {len(features_to_drop_corr)}\")\n",
        "    print(f\"     Dropped (Total):               {len(all_drops)}\")\n",
        "    print(f\"     Remaining features:            {X_clean.shape[1]}\")\n",
        "    print(f\"     Reduction:                     {(1 - X_clean.shape[1]/initial_features)*100:.1f}%\")\n",
        "    print(f\"  {'='*80}\\n\")\n",
        "\n",
        "    # Report dictionary\n",
        "    report = {\n",
        "        'initial_features': initial_features,\n",
        "        'final_features': X_clean.shape[1],\n",
        "        'dropped_low_mi': len(low_mi_features),\n",
        "        'dropped_high_corr': len(features_to_drop_corr),\n",
        "        'total_dropped': len(all_drops),\n",
        "        'reduction_pct': (1 - X_clean.shape[1]/initial_features)*100\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Smart Feature Selection Complete!\\n\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    return X_clean, all_drops, mi_df, report\n",
        "\n",
        "def add_numeric_values_to_bars(ax, bars, format_str='.2f', color='black', fontsize=12):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:{format_str}}',\n",
        "                ha='center', va='bottom', fontsize=fontsize,\n",
        "                fontweight='bold', color=color)\n",
        "\n",
        "# XAI FUNCTION\n",
        "def get_feature_importance(model, X, y, model_name, feature_names, config=None):\n",
        "    \"\"\"XAI analysis with 6 methods\"\"\"\n",
        "    if config is None:\n",
        "        config = {}\n",
        "\n",
        "    if hasattr(X, 'values'):\n",
        "        X_np = X.values\n",
        "    else:\n",
        "        X_np = X\n",
        "\n",
        "    importance_dict = {}\n",
        "    timing_dict = {}\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ EKLEME: XAI CONFIG VE TIMING SETUP\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    import time\n",
        "\n",
        "    xai_config = config.get(\"xai\", {})\n",
        "    PFI_N_REPEATS = xai_config.get(\"pfi_n_repeats\", 5)\n",
        "    XAI_SEED = xai_config.get(\"random_state\", 42)\n",
        "    timing_dict = {}  # Her metodun süresini sakla\n",
        "\n",
        "    print(f\"\\n  [XAI CONFIG] PFI repeats: {PFI_N_REPEATS}, Seed: {XAI_SEED}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # 1. PFI (GÜNCELLEME: n_repeats düşürüldü, timing eklendi)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(f\"  [PFI] Computing...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if hasattr(model, 'predict'):\n",
        "            perm_importance = permutation_importance(\n",
        "                model, X_np, y,\n",
        "                n_repeats=PFI_N_REPEATS,  # ← 25'ten düşürüldü\n",
        "                random_state=XAI_SEED,    # ← Config'ten seed\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            mean_imp = perm_importance.importances_mean\n",
        "            mean_imp[mean_imp < 0] = 0\n",
        "\n",
        "            if mean_imp.sum() > 0:\n",
        "                importance_dict['PFI'] = mean_imp / np.sum(mean_imp)\n",
        "            else:\n",
        "                importance_dict['PFI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            timing_dict['PFI'] = elapsed\n",
        "            print(f\"  ✓ PFI completed in {elapsed:.2f}s\")\n",
        "\n",
        "    except Exception as e_pfi:\n",
        "        print(f\"  ✗ PFI failed: {e_pfi}\")\n",
        "        importance_dict['PFI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "        timing_dict['PFI'] = 0.0\n",
        "\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # 2. PMI (GÜNCELLEME: timing eklendi, seed eklendi)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(f\"    [PMI] Computing...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        n_samples = X_np.shape[0]\n",
        "        n_neighbors_adaptive = min(5, (n_samples // 100))\n",
        "        n_neighbors_final = max(3, n_neighbors_adaptive)\n",
        "\n",
        "        mi = mutual_info_classif(\n",
        "            X_np,\n",
        "            y,\n",
        "            random_state=XAI_SEED,  # ← Seed eklendi\n",
        "            n_neighbors=n_neighbors_final,\n",
        "            discrete_features=False\n",
        "        )\n",
        "\n",
        "        if mi.sum() < 1e-6:\n",
        "            importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "        else:\n",
        "            mi = np.clip(mi, 0, None)\n",
        "            if mi.sum() < 1e-6:\n",
        "                importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "            else:\n",
        "                importance_dict['PMI'] = mi / mi.sum()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        timing_dict['PMI'] = elapsed\n",
        "        print(f\"       ✓ PMI completed in {elapsed:.2f}s\")\n",
        "\n",
        "    except Exception as e_pmi:\n",
        "        print(f\"       ✗ PMI failed: {e_pmi}\")\n",
        "        importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "        timing_dict['PMI'] = 0.0\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # 3. SOFI (GÜNCELLEME: timing eklendi, GA seed eklendi)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(f\"    [SOFI] Computing...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not HAS_PYGAD:\n",
        "            print(f\"    [SOFI] pygad not available, using fallback sensitivity method\")\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # FALLBACK: Sensitivity-based method\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            sensitivity_scores = np.zeros(X_np.shape[1])\n",
        "\n",
        "            try:\n",
        "                base_proba = model.predict_proba(X_np)\n",
        "                use_proba = True\n",
        "            except:\n",
        "                base_pred = model.predict(X_np).astype(float)\n",
        "                use_proba = False\n",
        "\n",
        "            # Set seed for shuffling\n",
        "            np.random.seed(XAI_SEED)\n",
        "\n",
        "            for i in range(X_np.shape[1]):\n",
        "                X_perturbed = X_np.copy()\n",
        "                np.random.shuffle(X_perturbed[:, i])\n",
        "\n",
        "                if use_proba:\n",
        "                    try:\n",
        "                        perturbed_proba = model.predict_proba(X_perturbed)\n",
        "                        sensitivity_scores[i] = np.mean(np.sum(np.abs(base_proba - perturbed_proba), axis=1))\n",
        "                    except:\n",
        "                        sensitivity_scores[i] = 0.0\n",
        "                else:\n",
        "                    perturbed_pred = model.predict(X_perturbed).astype(float)\n",
        "                    sensitivity_scores[i] = np.mean(np.abs(base_pred - perturbed_pred))\n",
        "\n",
        "            if sensitivity_scores.sum() > 0:\n",
        "                importance_dict['SOFI'] = sensitivity_scores / np.sum(sensitivity_scores)\n",
        "            else:\n",
        "                importance_dict['SOFI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "\n",
        "        else:\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ GA-BASED SOFI (FULL VERSION WITH SEED)\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            print(f\"    [SOFI] Using GA-based optimization (pygad)\")\n",
        "\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            # Global variables for pygad fitness function\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            global GA_MODEL, GA_X, GA_Y, GA_BASE_F1\n",
        "\n",
        "            def feature_flip_sofi(model, X_num, y, permutation):\n",
        "                \"\"\"\n",
        "                Replace each feature with its mean sequentially and compute F1 after each flip.\n",
        "                \"\"\"\n",
        "                from sklearn.metrics import f1_score\n",
        "                scores = []\n",
        "                X_copy = X_num.copy()\n",
        "\n",
        "                for idx in permutation:\n",
        "                    X_copy[:, idx] = X_copy[:, idx].mean()\n",
        "                    preds = model.predict(X_copy)\n",
        "                    scores.append(f1_score(y, preds, average=\"weighted\"))\n",
        "\n",
        "                return scores\n",
        "\n",
        "            def fit_value_sofi(model, X_num, y, permutation, base_f1):\n",
        "                \"\"\"\n",
        "                SOFI objective = area under g(pi)-curve + 0.1 * penalty for positive jumps.\n",
        "                \"\"\"\n",
        "                n = len(permutation)\n",
        "                weights = np.arange(1.0, 0.0, -1 / (n + 1))\n",
        "                g_vals = feature_flip_sofi(model, X_num, y, permutation)\n",
        "                g_vals = [base_f1] + g_vals\n",
        "                area = np.sum(weights * g_vals)\n",
        "                penalty = 0.1 * np.sum(np.maximum(np.diff(g_vals), 0))\n",
        "                return area + penalty\n",
        "\n",
        "            def sofi_fitness(ga_instance, solution, solution_idx):\n",
        "                \"\"\"\n",
        "                Fitness = 1 / (SOFI objective + epsilon), so GA maximizes fitness.\n",
        "                \"\"\"\n",
        "                return 1.0 / (fit_value_sofi(GA_MODEL, GA_X, GA_Y, solution, GA_BASE_F1) + 1e-6)\n",
        "\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            # Set global variables\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            GA_MODEL = model\n",
        "            GA_X = X_np\n",
        "            GA_Y = y\n",
        "\n",
        "            from sklearn.metrics import f1_score\n",
        "            GA_BASE_F1 = f1_score(y, model.predict(X_np), average=\"weighted\")\n",
        "\n",
        "            n_features = X_np.shape[1]\n",
        "\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            # Get SOFI config from CONFIG\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            sofi_config = config.get(\"sofi\", {})\n",
        "            n_generations = sofi_config.get(\"n_generations\", 25)\n",
        "            sol_per_pop = sofi_config.get(\"sol_per_pop\", 50)\n",
        "            mutation_prob = sofi_config.get(\"mutation_probability\", 0.05)\n",
        "\n",
        "            # ✅ USE XAI_SEED INSTEAD OF HARDCODED SEED\n",
        "            ga_random_seed = XAI_SEED\n",
        "\n",
        "            num_parents = max(2, sol_per_pop // 2)\n",
        "\n",
        "            print(f\"       GA params: gen={n_generations}, pop={sol_per_pop}, mut={mutation_prob}, seed={ga_random_seed}\")\n",
        "\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            # Create GA instance\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            ga_kwargs = {\n",
        "                \"num_generations\":       n_generations,\n",
        "                \"sol_per_pop\":           sol_per_pop,\n",
        "                \"num_parents_mating\":    num_parents,\n",
        "                \"fitness_func\":          sofi_fitness,\n",
        "                \"num_genes\":             n_features,\n",
        "                \"gene_space\":            list(range(n_features)),\n",
        "                \"allow_duplicate_genes\": False,\n",
        "                \"mutation_probability\":  mutation_prob,\n",
        "                \"gene_type\":             int,\n",
        "                \"random_seed\":           ga_random_seed,  # ← CONFIG'TEN SEED\n",
        "                \"suppress_warnings\":     True\n",
        "            }\n",
        "\n",
        "            ga_instance = GA(**ga_kwargs)\n",
        "            ga_instance.run()\n",
        "\n",
        "            best_solution, _, _ = ga_instance.best_solution()\n",
        "\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            # Convert permutation to importance scores\n",
        "            # ────────────────────────────────────────────────────────────────\n",
        "            sofi_importance = np.zeros(n_features)\n",
        "            for rank, feature_idx in enumerate(best_solution):\n",
        "                sofi_importance[feature_idx] = n_features - rank\n",
        "\n",
        "            # Normalize\n",
        "            if sofi_importance.sum() > 0:\n",
        "                importance_dict['SOFI'] = sofi_importance / np.sum(sofi_importance)\n",
        "            else:\n",
        "                importance_dict['SOFI'] = np.ones(n_features) / n_features\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # Timing\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        elapsed = time.time() - start_time\n",
        "        timing_dict['SOFI'] = elapsed\n",
        "        print(f\"       ✓ SOFI completed in {elapsed:.2f}s\")\n",
        "\n",
        "    except Exception as e_sofi:\n",
        "        print(f\"    [SOFI ERROR] {str(e_sofi)[:100]}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        importance_dict['SOFI'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "        timing_dict['SOFI'] = 0.0\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # 4. SHAP - ENHANCED WITH COMPREHENSIVE DEBUG & VALIDATION (FULL FIX)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if HAS_SHAP and model_name not in ['svm', 'ada']:\n",
        "        try:\n",
        "            print(f\"    [SHAP] Computing feature importance...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ CONFIG'TEN PARAMETRELERİ AL\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            xai_config = config.get(\"xai\", {})\n",
        "            n_shap_samples = xai_config.get(\"n_samples\", 100)  # ← 250'den düşürüldü\n",
        "            n_background = xai_config.get(\"n_shap_background\", 50)  # ← 200'den düşürüldü\n",
        "            n_jobs_shap = xai_config.get(\"n_jobs_shap\", 16)\n",
        "\n",
        "            # Convert to numpy\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                X_np_shap = X.values\n",
        "            else:\n",
        "                X_np_shap = X_np\n",
        "\n",
        "            # ✅ DEBUG 1: Feature count validation\n",
        "            n_features_expected = len(feature_names)\n",
        "            print(f\"       [DEBUG] Expected features: {n_features_expected}\")\n",
        "            print(f\"       [DEBUG] X shape: {X_np_shap.shape}\")\n",
        "            print(f\"       [DEBUG] Samples: {n_shap_samples}, Background: {n_background}\")\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # (A) Select background samples\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            if X_np_shap.shape[0] > n_background:\n",
        "                np.random.seed(XAI_SEED)  # ← SEED KULLAN\n",
        "                idx = np.random.choice(X_np_shap.shape[0], n_background, replace=False)\n",
        "                background = X_np_shap[idx]\n",
        "            else:\n",
        "                background = X_np_shap\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # (B) Select evaluation samples\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            if X_np_shap.shape[0] > n_shap_samples:\n",
        "                np.random.seed(XAI_SEED)  # ← SEED KULLAN\n",
        "                idx_eval = np.random.choice(X_np_shap.shape[0], n_shap_samples, replace=False)\n",
        "                X_eval = X_np_shap[idx_eval]\n",
        "            else:\n",
        "                X_eval = X_np_shap\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ MODEL-SPECIFIC EXPLAINER SELECTION (ENHANCED & FIXED)\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "            if model_name == 'lr':\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # LOGISTIC REGRESSION: LinearExplainer\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                print(f\"       Using LinearExplainer (LR-specific)\")\n",
        "\n",
        "                explainer = shap.LinearExplainer(model, background)\n",
        "                shap_values = explainer.shap_values(X_eval)\n",
        "\n",
        "                print(f\"       [DEBUG] SHAP output type: {type(shap_values)}\")\n",
        "\n",
        "                if isinstance(shap_values, list):\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ MULTICLASS LIST HANDLING\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    print(f\"       [DEBUG] SHAP is multiclass list, length: {len(shap_values)}\")\n",
        "\n",
        "                    # Stack all classes\n",
        "                    try:\n",
        "                        shap_values_all_classes = np.array(shap_values)\n",
        "                        print(f\"       [DEBUG] Stacked shape: {shap_values_all_classes.shape}\")\n",
        "                    except Exception as e_stack:\n",
        "                        print(f\"       [ERROR] Stacking failed: {e_stack}\")\n",
        "                        raise\n",
        "\n",
        "                    # Aggregate correctly based on shape\n",
        "                    if shap_values_all_classes.shape[0] == 3:\n",
        "                        # Shape: (3, 100, 83) - classes first\n",
        "                        print(f\"       [INFO] Aggregating: (classes, samples, features)\")\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes), axis=(0, 1))\n",
        "\n",
        "                    elif shap_values_all_classes.shape[2] == 3:\n",
        "                        # Shape: (100, 83, 3) - classes last\n",
        "                        print(f\"       [INFO] Aggregating: (samples, features, classes)\")\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes), axis=(0, 2))\n",
        "\n",
        "                    else:\n",
        "                        # Fallback\n",
        "                        print(f\"       [WARNING] Using fallback aggregation\")\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes),\n",
        "                                               axis=tuple(range(len(shap_values_all_classes.shape)-1)))\n",
        "\n",
        "                    print(f\"       [DEBUG] After aggregation shape: {mean_abs_shap.shape}\")\n",
        "\n",
        "                    # Shape validation\n",
        "                    if mean_abs_shap.ndim != 1:\n",
        "                        if mean_abs_shap.size == n_features_expected:\n",
        "                            mean_abs_shap = mean_abs_shap.flatten()\n",
        "                            print(f\"       [FIX] Flattened to: {mean_abs_shap.shape}\")\n",
        "                        else:\n",
        "                            raise ValueError(f\"Cannot fix shape: {mean_abs_shap.shape}\")\n",
        "\n",
        "                    if mean_abs_shap.shape[0] != n_features_expected:\n",
        "                        raise ValueError(\n",
        "                            f\"SHAP shape mismatch!\\n\"\n",
        "                            f\"  Got:      {mean_abs_shap.shape[0]} features\\n\"\n",
        "                            f\"  Expected: {n_features_expected} features\"\n",
        "                        )\n",
        "\n",
        "                elif len(shap_values.shape) == 3:\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ 3D ARRAY HANDLING\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    print(f\"       [DEBUG] SHAP 3D array shape: {shap_values.shape}\")\n",
        "\n",
        "                    if shap_values.shape[0] == X_eval.shape[0]:\n",
        "                        # (n_samples, n_features, n_classes)\n",
        "                        print(f\"       [INFO] Shape format: (samples, features, classes)\")\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 2))\n",
        "                    else:\n",
        "                        # (n_classes, n_samples, n_features)\n",
        "                        print(f\"       [INFO] Shape format: (classes, samples, features)\")\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 1))\n",
        "\n",
        "                else:\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ 2D ARRAY (Binary or single-class)\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    print(f\"       [DEBUG] SHAP 2D array shape: {shap_values.shape}\")\n",
        "                    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "\n",
        "            elif model_name in ['xgboost', 'lightgbm', 'rf', 'gb', 'catboost']:\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # TREE-BASED MODELS: TreeExplainer (very fast)\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                print(f\"       Using TreeExplainer (Tree-based)\")\n",
        "\n",
        "                try:\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                    shap_values = explainer.shap_values(X_eval)\n",
        "                except Exception as e_tree:\n",
        "                    print(f\"       [WARNING] TreeExplainer failed: {e_tree}\")\n",
        "                    print(f\"       Falling back to KernelExplainer...\")\n",
        "\n",
        "                    def pred_proba(x_array):\n",
        "                        return model.predict_proba(x_array)\n",
        "\n",
        "                    explainer = shap.KernelExplainer(pred_proba, background)\n",
        "                    shap_values = explainer.shap_values(X_eval, nsamples=100)\n",
        "\n",
        "                print(f\"       [DEBUG] SHAP output type: {type(shap_values)}\")\n",
        "\n",
        "                # Multiclass output handling\n",
        "                if isinstance(shap_values, list):\n",
        "                    print(f\"       [DEBUG] Multiclass list, using ALL classes\")\n",
        "                    shap_values_all = np.array(shap_values)\n",
        "                    print(f\"       [DEBUG] Stacked shape: {shap_values_all.shape}\")\n",
        "                    mean_abs_shap = np.mean(np.abs(shap_values_all), axis=(0, 1))\n",
        "\n",
        "                elif len(shap_values.shape) == 3:\n",
        "                    print(f\"       [DEBUG] 3D array shape: {shap_values.shape}\")\n",
        "\n",
        "                    if shap_values.shape[0] == X_eval.shape[0]:\n",
        "                        # (n_samples, n_features, n_classes)\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 2))\n",
        "                    else:\n",
        "                        # (n_classes, n_samples, n_features)\n",
        "                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 1))\n",
        "\n",
        "                else:\n",
        "                    print(f\"       [DEBUG] 2D array shape: {shap_values.shape}\")\n",
        "                    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "\n",
        "            elif model_name == 'ltcn':\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # LTCN: KernelExplainer (model-agnostic, parallel)\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                print(f\"       Using KernelExplainer (LTCN, parallel)\")\n",
        "\n",
        "                def _compute_shap_for_instance(explainer, x_instance, nsamples):\n",
        "                    \"\"\"Compute KernelSHAP for a single instance\"\"\"\n",
        "                    try:\n",
        "                        shap_vals = explainer.shap_values(x_instance, nsamples=nsamples)\n",
        "                        return np.abs(np.array(shap_vals).reshape(-1))\n",
        "                    except Exception as e_inst:\n",
        "                        print(f\"       [WARNING] Instance SHAP failed: {e_inst}\")\n",
        "                        return np.zeros(n_features_expected)\n",
        "\n",
        "                def pred_positive(x_array):\n",
        "                    \"\"\"Predict probability for Draw class (class 1)\"\"\"\n",
        "                    try:\n",
        "                        if hasattr(model, 'predict_proba'):\n",
        "                            proba = model.predict_proba(x_array)\n",
        "                            if proba.shape[1] == 3:\n",
        "                                return proba[:, 1]  # Draw class\n",
        "                            elif proba.shape[1] == 2:\n",
        "                                return proba[:, 1]  # Binary positive\n",
        "                            else:\n",
        "                                return proba.ravel()\n",
        "                        else:\n",
        "                            return model.predict(x_array)\n",
        "                    except Exception as e_pred:\n",
        "                        print(f\"       [ERROR] Prediction failed: {e_pred}\")\n",
        "                        return np.zeros(x_array.shape[0])\n",
        "\n",
        "                explainer = shap.KernelExplainer(pred_positive, background, link=\"identity\")\n",
        "\n",
        "                print(f\"       Background: {background.shape[0]}, Eval: {X_eval.shape[0]}, \" +\n",
        "                      f\"nsamples: {n_shap_samples}, n_jobs: {n_jobs_shap}\")\n",
        "\n",
        "                # Parallel computation\n",
        "                from joblib import Parallel, delayed\n",
        "\n",
        "                try:\n",
        "                    results = Parallel(n_jobs=n_jobs_shap, verbose=0)(\n",
        "                        delayed(_compute_shap_for_instance)(explainer, X_eval[i:i+1], n_shap_samples)\n",
        "                        for i in range(X_eval.shape[0])\n",
        "                    )\n",
        "\n",
        "                    shap_matrix = np.vstack(results)\n",
        "                    mean_abs_shap = np.mean(shap_matrix, axis=0)\n",
        "\n",
        "                except Exception as e_parallel:\n",
        "                    print(f\"       [ERROR] Parallel SHAP failed: {e_parallel}\")\n",
        "                    print(f\"       Using uniform fallback...\")\n",
        "                    mean_abs_shap = np.ones(n_features_expected) / n_features_expected\n",
        "\n",
        "            else:\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # OTHER MODELS: KernelExplainer (fallback)\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                print(f\"       Using KernelExplainer (generic)\")\n",
        "\n",
        "                def pred_positive(x_array):\n",
        "                    \"\"\"Predict probability for Draw class\"\"\"\n",
        "                    proba = model.predict_proba(x_array)\n",
        "                    if proba.shape[1] == 3:\n",
        "                        return proba[:, 1]\n",
        "                    return proba[:, 1]\n",
        "\n",
        "                explainer = shap.KernelExplainer(pred_positive, background, link=\"identity\")\n",
        "                shap_values = explainer.shap_values(X_eval, nsamples=n_shap_samples)\n",
        "                mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ COMPREHENSIVE DEBUG OUTPUT\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            print(f\"       [DEBUG] SHAP importance statistics:\")\n",
        "            print(f\"         Shape:  {mean_abs_shap.shape}\")\n",
        "            print(f\"         Min:    {mean_abs_shap.min():.6f}\")\n",
        "            print(f\"         Max:    {mean_abs_shap.max():.6f}\")\n",
        "            print(f\"         Mean:   {mean_abs_shap.mean():.6f}\")\n",
        "            print(f\"         Median: {np.median(mean_abs_shap):.6f}\")\n",
        "            print(f\"         Sum:    {mean_abs_shap.sum():.6f}\")\n",
        "\n",
        "            # ✅ Top 5 features BEFORE normalization\n",
        "            top_5_idx = np.argsort(mean_abs_shap)[-5:][::-1]\n",
        "            print(f\"       [DEBUG] Top 5 features (raw SHAP values):\")\n",
        "            for rank, idx in enumerate(top_5_idx, 1):\n",
        "                feat_name = feature_names[int(idx)]\n",
        "                feat_value = mean_abs_shap[int(idx)]\n",
        "                print(f\"         #{rank}: {feat_name:40s} = {feat_value:.6f}\")\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ NORMALIZE WITH VALIDATION\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            if mean_abs_shap.sum() > 0:\n",
        "                importance_dict['SHAP'] = mean_abs_shap / np.sum(mean_abs_shap)\n",
        "\n",
        "                # Post-normalization verification\n",
        "                normalized_sum = importance_dict['SHAP'].sum()\n",
        "                print(f\"       [DEBUG] After normalization:\")\n",
        "                print(f\"         Sum: {normalized_sum:.10f} (should be ≈ 1.0)\")\n",
        "\n",
        "                if abs(normalized_sum - 1.0) > 0.001:\n",
        "                    print(f\"       [WARNING] Normalization sum != 1.0 (got {normalized_sum:.6f})\")\n",
        "                    importance_dict['SHAP'] = importance_dict['SHAP'] / normalized_sum\n",
        "                    print(f\"       [FIX] Re-normalized to sum = {importance_dict['SHAP'].sum():.10f}\")\n",
        "            else:\n",
        "                print(f\"       [WARNING] SHAP sum = 0! Using uniform fallback.\")\n",
        "                importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # Timing\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            elapsed = time.time() - start_time\n",
        "            timing_dict['SHAP'] = elapsed\n",
        "            print(f\"       ✓ SHAP completed in {elapsed:.2f}s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    [SHAP ERROR] {model_name}: {str(e)[:150]}\")\n",
        "            print(f\"    [SHAP ERROR] Full traceback:\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "            timing_dict['SHAP'] = 0.0\n",
        "            print(f\"    [SHAP ERROR] Using uniform fallback\")\n",
        "\n",
        "    else:\n",
        "        # SHAP not available or model excluded\n",
        "        if model_name in ['svm', 'ada']:\n",
        "            print(f\"    [SHAP] Skipped for {model_name} (not supported)\")\n",
        "        else:\n",
        "            print(f\"    [SHAP] Not available (library not installed)\")\n",
        "\n",
        "        importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)\n",
        "        timing_dict['SHAP'] = 0.0\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # 5. Model-based importance (UNCHANGED)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    model_based_scores = np.ones(len(feature_names))\n",
        "    try:\n",
        "        if hasattr(model, 'coef_'):\n",
        "            scores = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
        "            if scores.sum() > 0:\n",
        "                model_based_scores = scores / np.sum(scores)\n",
        "        elif hasattr(model, 'feature_importances_'):\n",
        "            fi = model.feature_importances_\n",
        "            if fi.sum() > 0:\n",
        "                model_based_scores = fi / np.sum(fi)\n",
        "        elif model_name == 'ltcn' and hasattr(model, 'W2') and model.W2 is not None:\n",
        "            try:\n",
        "                W2_F = model.W2\n",
        "                T_steps = model.T\n",
        "                n_rows_w2f, n_classes_out = W2_F.shape\n",
        "                n_feat_check = X_np.shape[1]\n",
        "\n",
        "                d_features = -1\n",
        "                if (n_rows_w2f - 1) % (T_steps + 1) == 0:\n",
        "                    d_features = (n_rows_w2f - 1) // (T_steps + 1)\n",
        "\n",
        "                if d_features == n_feat_check:\n",
        "                    W2_no_bias = W2_F[:-1, :]\n",
        "                    W2_reshaped = W2_no_bias.reshape(T_steps + 1, d_features, n_classes_out)\n",
        "                    scores = np.sum(np.abs(W2_reshaped), axis=(0, 2))\n",
        "                else:\n",
        "                    scores = np.abs(model.W2[:n_feat_check, :]).mean(axis=1)\n",
        "\n",
        "                if scores.sum() > 0:\n",
        "                    model_based_scores = scores / np.sum(scores)\n",
        "                else:\n",
        "                    model_based_scores = np.ones(n_feat_check) / n_feat_check\n",
        "\n",
        "            except:\n",
        "                n_feat = X_np.shape[1]\n",
        "                if model.W2 is not None and model.W2.shape[0] >= n_feat:\n",
        "                    scores = np.abs(model.W2[:n_feat, :]).mean(axis=1)\n",
        "                    if scores.sum() > 0:\n",
        "                        model_based_scores = scores / np.sum(scores)\n",
        "                    else:\n",
        "                        model_based_scores = np.ones(n_feat) / n_feat\n",
        "                else:\n",
        "                    model_based_scores = np.ones(n_feat) / n_feat\n",
        "        else:\n",
        "            model_based_scores = np.random.rand(len(feature_names)) / len(feature_names)\n",
        "\n",
        "    except:\n",
        "        model_based_scores = np.random.rand(len(feature_names)) / len(feature_names)\n",
        "\n",
        "    importance_dict['LTCN'] = model_based_scores\n",
        "    importance_dict['XGBoost'] = model_based_scores\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ BURAYA EKLE! (Model-based importance'tan sonra, return'den önce)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    total_time = sum(timing_dict.values())\n",
        "\n",
        "    print(f\"\\n  [XAI TIMING SUMMARY]\")\n",
        "    print(f\"  {'='*60}\")\n",
        "    for method, elapsed in sorted(timing_dict.items(), key=lambda x: x[1], reverse=True):\n",
        "        pct = (elapsed / total_time * 100) if total_time > 0 else 0\n",
        "        print(f\"    {method:10s}: {elapsed:6.2f}s ({pct:5.1f}%)\")\n",
        "    print(f\"  {'='*60}\")\n",
        "    print(f\"    TOTAL:      {total_time:6.2f}s\\n\")\n",
        "\n",
        "    return importance_dict\n",
        "\n",
        "# TRAIN MODELS\n",
        "def multiclass_brier_score(y_true_one_hot, y_prob, n_classes_local=2):\n",
        "    if y_true_one_hot is None:\n",
        "        return np.nan\n",
        "    if not isinstance(y_prob, np.ndarray):\n",
        "        y_prob = np.array(y_prob)\n",
        "\n",
        "    if y_prob.ndim == 1:\n",
        "        if n_classes_local == 2:\n",
        "            y_prob = np.vstack([1 - y_prob, y_prob]).T\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    if y_prob.shape[1] != n_classes_local:\n",
        "        if y_prob.shape[1] == 1 and n_classes_local == 2:\n",
        "            y_prob = np.hstack([1 - y_prob, y_prob])\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    y_prob = np.nan_to_num(y_prob)\n",
        "\n",
        "    total_brier = 0\n",
        "    for i in range(n_classes_local):\n",
        "        y_true_class_i = y_true_one_hot[:, i]\n",
        "        y_prob_class_i = y_prob[:, i]\n",
        "        y_prob_class_i = np.clip(y_prob_class_i, 0, 1)\n",
        "        total_brier += brier_score_loss(y_true_class_i, y_prob_class_i)\n",
        "\n",
        "    return total_brier / n_classes_local if n_classes_local > 0 else np.nan\n",
        "\n",
        "def _safe_proba_v13(P, K=2):\n",
        "    if not isinstance(P, np.ndarray):\n",
        "        P = np.array(P)\n",
        "    if P.ndim == 1:\n",
        "        if K == 2:\n",
        "            P = np.column_stack([1 - P, P])\n",
        "        else:\n",
        "            temp_P = np.zeros((len(P), K))\n",
        "            temp_P[:, 0] = P\n",
        "            P = temp_P\n",
        "    if P.shape[1] < K:\n",
        "        P_new = np.zeros((P.shape[0], K))\n",
        "        P_new[:, :P.shape[1]] = P\n",
        "        P = P_new\n",
        "    P = np.clip(P, 1e-15, 1 - 1e-15)\n",
        "    P = P / P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# ✅ EARLY STOPPING İÇİN VALİDATİON SPLIT FONKSİYONU 18.11.25 tarihinde eklendi. gayemiz overfitting engellemek agac tipi modeller adina\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def create_validation_split(X, y, test_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Create validation split for early stopping\n",
        "\n",
        "    Args:\n",
        "        X: Training features\n",
        "        y: Training labels\n",
        "        test_size: Validation set size (default: 15%)\n",
        "        random_state: Random seed\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_val, y_train, y_val\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"  [VAL SPLIT] Train: {len(X_train):,} | Val: {len(X_val):,}\")\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "# TRAIN MODELS\n",
        "def train_models(X_train, X_test, y_train, y_test,\n",
        "                 X_val=None, y_val=None,  # ✅ YENİ PARAMETRELER!\n",
        "                 features=None, seed=42,\n",
        "                 CONFIG=None, OUT_DIR=\"./outputs\", N_CLASSES=2):\n",
        "    \"\"\"\n",
        "    Model training with Optuna\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features (SMOTE'd if enabled)\n",
        "        X_test: Test features\n",
        "        y_train: Training labels (SMOTE'd if enabled)\n",
        "        y_test: Test labels\n",
        "        X_val: Validation features (NEVER SMOTE'd, optional)\n",
        "        y_val: Validation labels (NEVER SMOTE'd, optional)\n",
        "        features: Feature names\n",
        "        seed: Random seed\n",
        "        CONFIG: Configuration dict\n",
        "        OUT_DIR: Output directory\n",
        "        N_CLASSES: Number of classes\n",
        "\n",
        "    Returns:\n",
        "        results: Model performance metrics\n",
        "        models_dict: Trained model objects\n",
        "        xai_results: XAI analysis results\n",
        "        validation_data: Tuple of (X_val, y_val) for ablation\n",
        "    \"\"\"\n",
        "\n",
        "    if CONFIG is None:\n",
        "        CONFIG = {}\n",
        "\n",
        "    print(\"\\n5️⃣ TRAINING MODELS...\\n\")\n",
        "    results = {}\n",
        "    models_dict = {}\n",
        "    xai_results = {}\n",
        "    best_params_dict = {}\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # ONE-HOT ENCODING\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        y_train_one_hot = label_binarize(y_train, classes=np.unique(y_train))\n",
        "        y_test_one_hot = label_binarize(y_test, classes=np.unique(y_train))\n",
        "        n_classes_local = y_train_one_hot.shape[1]\n",
        "        print(f\"  One-hot encoded ({n_classes_local} classes)\")\n",
        "    except ValueError as e:\n",
        "        print(f\"  Binarize error: {e}\")\n",
        "        y_train_one_hot = None\n",
        "        y_test_one_hot = None\n",
        "        n_classes_local = N_CLASSES\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # CV & OPTUNA CONFIG\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    n_splits = CONFIG.get('cv', {}).get('folds', 5)\n",
        "    cv_splitter = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    n_trials = CONFIG.get('optuna', {}).get('n_trials', 10)\n",
        "    timeout = CONFIG.get('optuna', {}).get('timeout', 1000)\n",
        "\n",
        "    all_models_in_config = CONFIG.get('models', [])\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ: VALIDATION SET KONTROLÜ (PARAMETRE OLARAK GELDİ)\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    early_stop_config = CONFIG.get('early_stopping', {})\n",
        "    USE_EARLY_STOPPING = early_stop_config.get('enabled', False)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"🔍 VALIDATION SET STATUS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    if USE_EARLY_STOPPING:\n",
        "        if X_val is None or y_val is None:\n",
        "            print(\"⚠️  WARNING: Early stopping enabled but no validation set provided!\")\n",
        "            print(\"   Validation should be created in ADIM 3.7 (before SMOTE)\")\n",
        "            print(\"   Falling back to no early stopping for this run\\n\")\n",
        "            USE_EARLY_STOPPING = False\n",
        "            X_train_for_models = X_train\n",
        "            y_train_for_models = y_train\n",
        "        else:\n",
        "            print(f\"✅ Validation set received from main execution\")\n",
        "            print(f\"   • Samples: {len(X_val):,}\")\n",
        "            print(f\"   • Features: {X_val.shape[1]}\")\n",
        "            print(f\"   • Source: PRE-SMOTE split (ADIM 3.7)\")\n",
        "            print(f\"   • Contains ZERO synthetic samples ✅\")\n",
        "\n",
        "            # Verify class distribution\n",
        "            val_counts = y_val.value_counts().sort_index()\n",
        "            print(f\"\\n   📊 Validation class distribution:\")\n",
        "            print(f\"      • Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"      • Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"      • Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)\")\n",
        "\n",
        "            print(f\"\\n   🎯 Will be used for:\")\n",
        "            print(f\"      • Early stopping (XGBoost, LightGBM, CatBoost)\")\n",
        "            print(f\"      • Ablation analysis (G28, G29)\")\n",
        "            print(f\"      • XAI verification\\n\")\n",
        "\n",
        "            # Use full SMOTE'd train data (validation already removed in ADIM 3.7)\n",
        "            X_train_for_models = X_train\n",
        "            y_train_for_models = y_train\n",
        "\n",
        "            print(f\"   📊 Training data (post-SMOTE):\")\n",
        "            print(f\"      • Samples: {len(X_train_for_models):,}\")\n",
        "            print(f\"      • May contain synthetic samples (correct!)\\n\")\n",
        "    else:\n",
        "        print(\"ℹ️  Early stopping DISABLED\")\n",
        "        print(\"   • All training data will be used\")\n",
        "        print(\"   • No validation split needed\")\n",
        "        print(\"   • Ablation will use test set (less ideal)\\n\")\n",
        "\n",
        "        X_train_for_models = X_train\n",
        "        y_train_for_models = y_train\n",
        "        X_val = None\n",
        "        y_val = None\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ: SMOTE CONFIG VE PIPELINE HAZIRLIĞI\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🔬 CROSS-VALIDATION STRATEGY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    class_balancing_config = CONFIG.get(\"class_balancing\", {})\n",
        "    USE_SMOTE_IN_CV = class_balancing_config.get(\"use_smote\", False)\n",
        "\n",
        "    if USE_SMOTE_IN_CV:\n",
        "        print(\"  ✅ SMOTE will be applied PER FOLD (ImbPipeline)\")\n",
        "        print(\"  ✅ Validation folds will be SYNTHETIC-FREE\")\n",
        "        print(\"  ✅ This prevents overfitting to synthetic samples\\n\")\n",
        "\n",
        "        SMOTE_STRATEGY = class_balancing_config.get(\"strategy\", \"auto\")\n",
        "        K_NEIGHBORS = class_balancing_config.get(\"k_neighbors\", 5)\n",
        "\n",
        "        print(f\"  📊 SMOTE Strategy: {SMOTE_STRATEGY}\")\n",
        "        print(f\"  📊 K-Neighbors: {K_NEIGHBORS}\\n\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════\n",
        "        # TARGETED STRATEGY İÇİN SAMPLING_STRATEGY HESAPLA\n",
        "        # ═══════════════════════════════════════════════════════════════════\n",
        "        if SMOTE_STRATEGY == \"targeted\":\n",
        "            y_counts = y_train.value_counts().sort_index()\n",
        "            current_away = y_counts.get(0, 0)\n",
        "            current_draw = y_counts.get(1, 0)\n",
        "            current_home = y_counts.get(2, 0)\n",
        "\n",
        "            avg_other = int((current_away + current_home) / 2)\n",
        "            target_draw_ratio = class_balancing_config.get(\"target_draw_ratio\", 1.2)\n",
        "            target_draw = int(avg_other * target_draw_ratio)\n",
        "            target_draw = max(target_draw, current_draw)\n",
        "\n",
        "            SAMPLING_STRATEGY_DICT = {\n",
        "                0: current_away,\n",
        "                1: target_draw,\n",
        "                2: current_home\n",
        "            }\n",
        "\n",
        "            print(f\"  🎯 Targeted Sampling Strategy:\")\n",
        "            print(f\"     • Away Win (0): {current_away:,} → {SAMPLING_STRATEGY_DICT[0]:,} (no change)\")\n",
        "            print(f\"     • Draw (1):     {current_draw:,} → {SAMPLING_STRATEGY_DICT[1]:,} (+{target_draw-current_draw:,})\")\n",
        "            print(f\"     • Home Win (2): {current_home:,} → {SAMPLING_STRATEGY_DICT[2]:,} (no change)\")\n",
        "        else:\n",
        "            SAMPLING_STRATEGY_DICT = 'auto'\n",
        "            print(f\"  📊 Using 'auto' strategy (balance to majority class)\")\n",
        "    else:\n",
        "        print(\"  ℹ️  SMOTE DISABLED in CV\")\n",
        "        print(\"  ℹ️  Models will train on original class distribution\")\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # MODEL TRAINING LOOP\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    for model_name in all_models_in_config:\n",
        "\n",
        "        start_model_time = time.time()\n",
        "        model = None\n",
        "        results_key = f\"{model_name.upper()} (Optuna)\"\n",
        "        optimization_failed = False\n",
        "\n",
        "        if CONFIG.get('optuna', {}).get('n_trials', 0) > 0:\n",
        "            print(f\"\\nOptimizing {model_name.upper()}...\")\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ MODEL-SPECIFIC VALİDATİON KULLANIMI (Early Stopping için)\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            needs_validation = model_name in ['xgboost', 'lightgbm', 'catboost']\n",
        "\n",
        "            if needs_validation and USE_EARLY_STOPPING and X_val is not None:\n",
        "                # Bu modeller early stopping kullanacak (Optuna içinde)\n",
        "                X_val_model = X_val\n",
        "                y_val_model = y_val\n",
        "                print(f\"  [EARLY STOP] Using PRE-SMOTE validation set: {len(X_val_model):,} samples\")\n",
        "            else:\n",
        "                # Diğer modeller veya early stopping kapalıysa\n",
        "                X_val_model = None\n",
        "                y_val_model = None\n",
        "\n",
        "            def objective(trial):\n",
        "                try:\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ YENİ: SMOTE PIPELINE STEP HAZIRLA (Her trial için)\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    if USE_SMOTE_IN_CV:\n",
        "                        smote_step = SMOTE(\n",
        "                            sampling_strategy=SAMPLING_STRATEGY_DICT,\n",
        "                            random_state=seed,\n",
        "                            k_neighbors=K_NEIGHBORS\n",
        "                        )\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # LOGISTIC REGRESSION\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    if model_name == 'lr':\n",
        "                        C = trial.suggest_float('C', 1e-4, 1e1, log=True)\n",
        "\n",
        "                        base_model = LogisticRegression(\n",
        "                            C=C,\n",
        "                            max_iter=1000,\n",
        "                            random_state=seed,\n",
        "                            multi_class='ovr',\n",
        "                            n_jobs=-1\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline oluştur (SMOTE varsa)\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        # ✅ PRE-SMOTE veri ile CV\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,  # <-- SMOTE uygulanmamış veri!\n",
        "                            y_train,  # <-- Original labels!\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # RANDOM FOREST\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'rf':\n",
        "                        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
        "                        max_depth = trial.suggest_int('max_depth', 5, 12)\n",
        "                        min_samples_split = trial.suggest_int('min_samples_split', 5, 30)\n",
        "\n",
        "                        base_model = RandomForestClassifier(\n",
        "                            n_estimators=n_estimators,\n",
        "                            max_depth=max_depth,\n",
        "                            min_samples_split=min_samples_split,\n",
        "                            random_state=seed,\n",
        "                            n_jobs=-1\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # GRADIENT BOOSTING\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'gb':\n",
        "                        max_iter = trial.suggest_int('max_iter', 50, 500)\n",
        "                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)\n",
        "                        max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 10, 50)\n",
        "\n",
        "                        base_model = HistGradientBoostingClassifier(\n",
        "                            max_iter=max_iter,\n",
        "                            learning_rate=learning_rate,\n",
        "                            max_leaf_nodes=max_leaf_nodes,\n",
        "                            random_state=seed\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════════════════════\n",
        "                    # ✅ XGBOOST - HYBRİD APPROACH\n",
        "                    # Optuna CV: Pipeline ile (SMOTE per fold, early stopping YOK)\n",
        "                    # Final Model: Validation set ile (Early stopping VAR)\n",
        "                    # ════════════════════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'xgboost' and HAS_XGB:\n",
        "                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
        "                        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "                        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
        "                        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
        "                        min_child_weight = trial.suggest_int('min_child_weight', 1, 7)\n",
        "                        gamma = trial.suggest_float('gamma', 0, 0.5)\n",
        "                        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
        "                        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
        "\n",
        "                        # ✅ Optuna CV için early stopping OLMADAN\n",
        "                        base_model = xgb.XGBClassifier(\n",
        "                            n_estimators=300,  # CV için daha az (hız için)\n",
        "                            learning_rate=learning_rate,\n",
        "                            max_depth=max_depth,\n",
        "                            subsample=subsample,\n",
        "                            colsample_bytree=colsample_bytree,\n",
        "                            min_child_weight=min_child_weight,\n",
        "                            gamma=gamma,\n",
        "                            reg_alpha=reg_alpha,\n",
        "                            reg_lambda=reg_lambda,\n",
        "                            random_state=seed,\n",
        "                            eval_metric='mlogloss',\n",
        "                            objective='multi:softprob',\n",
        "                            n_jobs=-1,\n",
        "                            verbosity=0\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline ile CV (early stopping yok, ama SMOTE sızıntısı yok!)\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ LIGHTGBM - HYBRİD APPROACH\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'lightgbm' and HAS_LGB:\n",
        "                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
        "                        num_leaves = trial.suggest_int('num_leaves', 20, 100)\n",
        "                        max_depth = trial.suggest_int('max_depth', 3, 12)\n",
        "                        min_child_samples = trial.suggest_int('min_child_samples', 10, 50)\n",
        "                        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
        "                        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
        "                        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
        "                        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
        "\n",
        "                        base_model = lgb.LGBMClassifier(\n",
        "                            n_estimators=300,  # CV için\n",
        "                            learning_rate=learning_rate,\n",
        "                            num_leaves=num_leaves,\n",
        "                            max_depth=max_depth,\n",
        "                            min_child_samples=min_child_samples,\n",
        "                            subsample=subsample,\n",
        "                            colsample_bytree=colsample_bytree,\n",
        "                            reg_alpha=reg_alpha,\n",
        "                            reg_lambda=reg_lambda,\n",
        "                            random_state=seed,\n",
        "                            objective='multiclass',\n",
        "                            num_class=n_classes_local,\n",
        "                            n_jobs=-1,\n",
        "                            verbose=-1\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ✅ CATBOOST - HYBRİD APPROACH\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'catboost' and HAS_CB:\n",
        "                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
        "                        depth = trial.suggest_int('depth', 4, 10)\n",
        "                        l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1, 10)\n",
        "                        border_count = trial.suggest_int('border_count', 32, 255)\n",
        "                        bagging_temperature = trial.suggest_float('bagging_temperature', 0, 1)\n",
        "\n",
        "                        base_model = cb.CatBoostClassifier(\n",
        "                            iterations=300,  # CV için\n",
        "                            learning_rate=learning_rate,\n",
        "                            depth=depth,\n",
        "                            l2_leaf_reg=l2_leaf_reg,\n",
        "                            border_count=border_count,\n",
        "                            bagging_temperature=bagging_temperature,\n",
        "                            random_state=seed,\n",
        "                            loss_function='MultiClass',\n",
        "                            classes_count=n_classes_local,\n",
        "                            verbose=False,\n",
        "                            thread_count=-1\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # SVM\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'svm':\n",
        "                        C = trial.suggest_float('C', 0.1, 10, log=True)\n",
        "                        gamma = trial.suggest_float('gamma', 0.01, 1, log=True)\n",
        "\n",
        "                        base_model = SVC(\n",
        "                            C=C,\n",
        "                            gamma=gamma,\n",
        "                            kernel='rbf',\n",
        "                            probability=True,\n",
        "                            random_state=seed\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # ADABOOST\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'ada':\n",
        "                        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "                        learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0, log=True)\n",
        "\n",
        "                        base_est = DecisionTreeClassifier(max_depth=1, random_state=seed)\n",
        "                        base_model = AdaBoostClassifier(\n",
        "                            estimator=base_est,\n",
        "                            n_estimators=n_estimators,\n",
        "                            learning_rate=learning_rate,\n",
        "                            random_state=seed,\n",
        "                            algorithm='SAMME'\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    # LTCN\n",
        "                    # ════════════════════════════════════════════════════════\n",
        "                    elif model_name == 'ltcn':\n",
        "                        T = trial.suggest_int('T', 10, 50)\n",
        "                        alpha = trial.suggest_float('alpha', 0.1, 0.9)\n",
        "                        beta = trial.suggest_float('beta', 0.1, 0.9)\n",
        "                        ridge_alpha = trial.suggest_float('ridge_alpha', 1e-5, 1e-1, log=True)\n",
        "\n",
        "                        base_model = LTCN(\n",
        "                            T=T,\n",
        "                            alpha=alpha,\n",
        "                            beta=beta,\n",
        "                            ridge_alpha=ridge_alpha,\n",
        "                            random_state=seed\n",
        "                        )\n",
        "\n",
        "                        # ✅ Pipeline\n",
        "                        if USE_SMOTE_IN_CV:\n",
        "                            trial_model = ImbPipeline([\n",
        "                                ('smote', smote_step),\n",
        "                                ('clf', base_model)\n",
        "                            ])\n",
        "                        else:\n",
        "                            trial_model = base_model\n",
        "\n",
        "                        score = cross_val_score(\n",
        "                            trial_model,\n",
        "                            X_train,\n",
        "                            y_train,\n",
        "                            cv=cv_splitter,\n",
        "                            scoring='f1_weighted',\n",
        "                            n_jobs=-1\n",
        "                        ).mean()\n",
        "                        return score\n",
        "\n",
        "                    else:\n",
        "                        return 0.0\n",
        "\n",
        "                except Exception as e_trial:\n",
        "                    print(f\"  ⚠️ Trial failed: {str(e_trial)[:50]}\")\n",
        "                    return 0.0\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # OPTUNA OPTIMIZATION\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            try:\n",
        "                study = optuna.create_study(\n",
        "                    direction='maximize',\n",
        "                    pruner=MedianPruner(n_warmup_steps=3),\n",
        "                    sampler=TPESampler(seed=seed)\n",
        "                )\n",
        "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "                study.optimize(objective, n_trials=n_trials, timeout=timeout, n_jobs=1, show_progress_bar=True)\n",
        "                optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "\n",
        "                best_params = study.best_trial.params\n",
        "                best_params_dict[model_name] = best_params\n",
        "                print(f\"  ✓ Best CV F1: {study.best_trial.value:.4f}\")\n",
        "\n",
        "                if USE_SMOTE_IN_CV:\n",
        "                    print(f\"  ℹ️  (CV with ImbPipeline - validation folds are synthetic-free)\")\n",
        "\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # ✅ FİNAL MODEL TRAİNİNG (SMOTE + Early Stopping İLE)\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                print(f\"  [FINAL] Training final model with best params...\")\n",
        "\n",
        "                # ═══════════════════════════════════════════════════════════\n",
        "                # SMOTE'u tüm train setine uygula (final model için)\n",
        "                # ═══════════════════════════════════════════════════════════\n",
        "                if USE_SMOTE_IN_CV:\n",
        "                    print(f\"  [SMOTE] Applying SMOTE to full train set...\")\n",
        "                    smote_final = SMOTE(\n",
        "                        sampling_strategy=SAMPLING_STRATEGY_DICT,\n",
        "                        random_state=seed,\n",
        "                        k_neighbors=K_NEIGHBORS\n",
        "                    )\n",
        "                    X_train_smote_final, y_train_smote_final = smote_final.fit_resample(X_train, y_train)\n",
        "                    print(f\"  [SMOTE] {len(X_train):,} → {len(X_train_smote_final):,} samples\")\n",
        "                else:\n",
        "                    X_train_smote_final = X_train\n",
        "                    y_train_smote_final = y_train\n",
        "\n",
        "                # ═══════════════════════════════════════════════════════════\n",
        "                # MODEL-SPECIFIC FINAL TRAINING\n",
        "                # ═══════════════════════════════════════════════════════════\n",
        "\n",
        "                if model_name == 'lr':\n",
        "                    model = LogisticRegression(\n",
        "                        max_iter=1000,\n",
        "                        random_state=seed,\n",
        "                        multi_class='ovr',\n",
        "                        n_jobs=-1,\n",
        "                        **best_params\n",
        "                    )\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'rf':\n",
        "                    model = RandomForestClassifier(\n",
        "                        random_state=seed,\n",
        "                        n_jobs=-1,\n",
        "                        **best_params\n",
        "                    )\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'gb':\n",
        "                    model = HistGradientBoostingClassifier(\n",
        "                        random_state=seed,\n",
        "                        **best_params\n",
        "                    )\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'xgboost' and HAS_XGB:\n",
        "                    import xgboost\n",
        "                    xgb_version = tuple(map(int, xgboost.__version__.split('.')[:2]))\n",
        "                    early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)\n",
        "\n",
        "                    # XGBoost 2.0+\n",
        "                    if xgb_version >= (2, 0):\n",
        "                        model = xgb.XGBClassifier(\n",
        "                            n_estimators=500,  # Final için daha fazla\n",
        "                            random_state=seed,\n",
        "                            eval_metric='mlogloss',\n",
        "                            objective='multi:softprob',\n",
        "                            n_jobs=-1,\n",
        "                            verbosity=0,\n",
        "                            early_stopping_rounds=early_stop_rounds,\n",
        "                            **best_params\n",
        "                        )\n",
        "\n",
        "                        if X_val is not None:\n",
        "                            model.fit(\n",
        "                                X_train_smote_final, y_train_smote_final,\n",
        "                                eval_set=[(X_val, y_val)],  # ✅ PRE-SMOTE validation!\n",
        "                                verbose=False\n",
        "                            )\n",
        "                        else:\n",
        "                            model.fit(X_train_smote_final, y_train_smote_final, verbose=False)\n",
        "\n",
        "                    # XGBoost 1.x\n",
        "                    else:\n",
        "                        model = xgb.XGBClassifier(\n",
        "                            n_estimators=500,\n",
        "                            random_state=seed,\n",
        "                            eval_metric='mlogloss',\n",
        "                            objective='multi:softprob',\n",
        "                            n_jobs=-1,\n",
        "                            verbosity=0,\n",
        "                            **best_params\n",
        "                        )\n",
        "\n",
        "                        if X_val is not None:\n",
        "                            model.fit(\n",
        "                                X_train_smote_final, y_train_smote_final,\n",
        "                                eval_set=[(X_val, y_val)],\n",
        "                                early_stopping_rounds=early_stop_rounds,\n",
        "                                verbose=False\n",
        "                            )\n",
        "                        else:\n",
        "                            model.fit(X_train_smote_final, y_train_smote_final, verbose=False)\n",
        "\n",
        "                    # Best iteration\n",
        "                    try:\n",
        "                        if hasattr(model, 'best_iteration'):\n",
        "                            print(f\"  ✓ Best iteration: {model.best_iteration}\")\n",
        "                        elif hasattr(model, 'get_booster'):\n",
        "                            print(f\"  ✓ Best iteration: {model.get_booster().best_iteration}\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                elif model_name == 'lightgbm' and HAS_LGB:\n",
        "                    early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)\n",
        "\n",
        "                    model = lgb.LGBMClassifier(\n",
        "                        n_estimators=500,\n",
        "                        random_state=seed,\n",
        "                        objective='multiclass',\n",
        "                        num_class=n_classes_local,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=-1,\n",
        "                        **best_params\n",
        "                    )\n",
        "\n",
        "                    if X_val is not None:\n",
        "                        model.fit(\n",
        "                            X_train_smote_final, y_train_smote_final,\n",
        "                            eval_set=[(X_val, y_val)],  # ✅ PRE-SMOTE!\n",
        "                            eval_metric='multi_logloss',\n",
        "                            callbacks=[\n",
        "                                lgb.early_stopping(stopping_rounds=early_stop_rounds, verbose=False),\n",
        "                                lgb.log_evaluation(period=0)\n",
        "                            ]\n",
        "                        )\n",
        "                        print(f\"  ✓ Best iteration: {model.best_iteration_} (out of 500)\")\n",
        "                    else:\n",
        "                        model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'catboost' and HAS_CB:\n",
        "                    early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)\n",
        "\n",
        "                    model = cb.CatBoostClassifier(\n",
        "                        iterations=500,\n",
        "                        random_state=seed,\n",
        "                        loss_function='MultiClass',\n",
        "                        classes_count=n_classes_local,\n",
        "                        verbose=False,\n",
        "                        thread_count=-1,\n",
        "                        early_stopping_rounds=early_stop_rounds,\n",
        "                        **best_params\n",
        "                    )\n",
        "\n",
        "                    if X_val is not None:\n",
        "                        model.fit(\n",
        "                            X_train_smote_final, y_train_smote_final,\n",
        "                            eval_set=(X_val, y_val),  # ✅ PRE-SMOTE!\n",
        "                            verbose=False,\n",
        "                            plot=False\n",
        "                        )\n",
        "                        print(f\"  ✓ Best iteration: {model.get_best_iteration()} (out of 500)\")\n",
        "                    else:\n",
        "                        model.fit(X_train_smote_final, y_train_smote_final, verbose=False, plot=False)\n",
        "\n",
        "                elif model_name == 'svm':\n",
        "                    model = SVC(\n",
        "                        kernel='rbf',\n",
        "                        probability=True,\n",
        "                        random_state=seed,\n",
        "                        **best_params\n",
        "                    )\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'ada':\n",
        "                    base_est = DecisionTreeClassifier(max_depth=1, random_state=seed)\n",
        "                    model = AdaBoostClassifier(\n",
        "                        estimator=base_est,\n",
        "                        random_state=seed,\n",
        "                        algorithm='SAMME',\n",
        "                        **best_params\n",
        "                    )\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                elif model_name == 'ltcn':\n",
        "                    model = LTCN(random_state=seed, **best_params)\n",
        "                    model.fit(X_train_smote_final, y_train_smote_final)\n",
        "\n",
        "                models_dict[model_name] = model\n",
        "\n",
        "                print(f\"  ✅ Final model trained successfully\")\n",
        "\n",
        "            except Exception as e_opt:\n",
        "                print(f\"❌ Optimization error ({model_name}): {e_opt}\")\n",
        "                best_params_dict[model_name] = \"Optimization Failed\"\n",
        "                optimization_failed = True\n",
        "                continue\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # METRICS CALCULATION\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        if model is not None:\n",
        "            try:\n",
        "                y_train_pred = model.predict(X_train)\n",
        "                y_test_pred = model.predict(X_test)\n",
        "                y_train_proba = _safe_proba_v13(model.predict_proba(X_train), n_classes_local)\n",
        "                y_test_proba = _safe_proba_v13(model.predict_proba(X_test), n_classes_local)\n",
        "\n",
        "                results[results_key] = {\n",
        "                    'train_acc': accuracy_score(y_train, y_train_pred),\n",
        "                    'test_acc': accuracy_score(y_test, y_test_pred),\n",
        "                    'train_f1': f1_score(y_train, y_train_pred, average='weighted'),\n",
        "                    'test_f1': f1_score(y_test, y_test_pred, average='weighted'),\n",
        "                    'train_kappa': cohen_kappa_score(y_train, y_train_pred),\n",
        "                    'test_kappa': cohen_kappa_score(y_test, y_test_pred),\n",
        "                    'train_auc': roc_auc_score(y_train, y_train_proba, multi_class='ovr'),\n",
        "                    'test_auc': roc_auc_score(y_test, y_test_proba, multi_class='ovr'),\n",
        "                    'train_loss': log_loss(y_train, y_train_proba),\n",
        "                    'test_loss': log_loss(y_test, y_test_proba),\n",
        "                    'train_brier': multiclass_brier_score(y_train_one_hot, y_train_proba, n_classes_local),\n",
        "                    'test_brier': multiclass_brier_score(y_test_one_hot, y_test_proba, n_classes_local),\n",
        "                    'test_rps': ranked_probability_score(y_test, y_test_proba),\n",
        "                    'test_ece': expected_calibration_error(y_test, y_test_proba),\n",
        "                }\n",
        "\n",
        "                # Per-class metrics\n",
        "                try:\n",
        "                    from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "                    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "                    cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "                    train_per_class_acc = cm_train.diagonal() / cm_train.sum(axis=1)\n",
        "                    test_per_class_acc = cm_test.diagonal() / cm_test.sum(axis=1)\n",
        "\n",
        "                    precision_train, recall_train, f1_train_pc, _ = precision_recall_fscore_support(\n",
        "                        y_train, y_train_pred, average=None, zero_division=0\n",
        "                    )\n",
        "                    precision_test, recall_test, f1_test_pc, support_test = precision_recall_fscore_support(\n",
        "                        y_test, y_test_pred, average=None, zero_division=0\n",
        "                    )\n",
        "\n",
        "                    class_names = ['Away Win', 'Draw', 'Home Win']\n",
        "\n",
        "                    results[results_key]['per_class_metrics_train'] = {\n",
        "                        'accuracy': {class_names[i]: float(train_per_class_acc[i]) for i in range(len(class_names))},\n",
        "                        'precision': {class_names[i]: float(precision_train[i]) for i in range(len(class_names))},\n",
        "                        'recall': {class_names[i]: float(recall_train[i]) for i in range(len(class_names))},\n",
        "                        'f1': {class_names[i]: float(f1_train_pc[i]) for i in range(len(class_names))},\n",
        "                        'support': {class_names[i]: int(cm_train[i].sum()) for i in range(len(class_names))}\n",
        "                    }\n",
        "\n",
        "                    results[results_key]['per_class_metrics'] = {\n",
        "                        'accuracy': {class_names[i]: float(test_per_class_acc[i]) for i in range(len(class_names))},\n",
        "                        'precision': {class_names[i]: float(precision_test[i]) for i in range(len(class_names))},\n",
        "                        'recall': {class_names[i]: float(recall_test[i]) for i in range(len(class_names))},\n",
        "                        'f1': {class_names[i]: float(f1_test_pc[i]) for i in range(len(class_names))},\n",
        "                        'support': {class_names[i]: int(support_test[i]) for i in range(len(class_names))}\n",
        "                    }\n",
        "\n",
        "                    print(f\"  ✓ Per-class metrics calculated\")\n",
        "\n",
        "                except Exception as e_per_class:\n",
        "                    print(f\"  ⚠️ Per-class metrics error: {e_per_class}\")\n",
        "\n",
        "            except Exception as e_metrics:\n",
        "                print(f\"  ❌ Metrics calculation error: {e_metrics}\")\n",
        "\n",
        "        end_model_time = time.time()\n",
        "        print(f\"  Time: {(end_model_time - start_model_time):.2f}s\")\n",
        "        gc.collect()\n",
        "\n",
        "    # XAI Analysis section (rest of the function remains the same)\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"🎯 XAI ANALYSIS - BEST MODEL SELECTION\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    valid_models_f1 = {}\n",
        "    for model_key in results.keys():\n",
        "        f1_value = results[model_key].get('test_f1', np.nan)\n",
        "        if pd.notna(f1_value):\n",
        "            valid_models_f1[model_key] = f1_value\n",
        "\n",
        "    if not valid_models_f1:\n",
        "        print(\"❌ No valid models found! Skipping XAI analysis.\\n\")\n",
        "        print(\"=\"*100 + \"\\n\")\n",
        "    else:\n",
        "        sorted_models = sorted(\n",
        "            valid_models_f1.items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        print(\"📊 Model Rankings (Top 5):\\n\")\n",
        "        for rank, (model_name, f1_value) in enumerate(sorted_models[:5], 1):\n",
        "            emoji = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else \"  \"\n",
        "            print(f\"  {emoji} #{rank}: {model_name:25s} → F1 = {f1_value:.6f}\")\n",
        "\n",
        "        best_model_key = sorted_models[0][0]\n",
        "        best_f1_score = sorted_models[0][1]\n",
        "        best_model_short = best_model_key.split(' ')[0].lower()\n",
        "\n",
        "        print(f\"\\n🏆 SELECTED BEST MODEL: {best_model_key}\")\n",
        "        print(f\"📊 Test F1-Score: {best_f1_score:.6f}\")\n",
        "        print(f\"🔬 Running Explainable AI Analysis...\\n\")\n",
        "\n",
        "        if len(sorted_models) > 1:\n",
        "            second_best_f1 = sorted_models[1][1]\n",
        "            f1_diff = best_f1_score - second_best_f1\n",
        "\n",
        "            if f1_diff < 0.001:\n",
        "                print(f\"⚠️  WARNING: Very close to 2nd place ({sorted_models[1][0]})\")\n",
        "                print(f\"   Difference: {f1_diff:.6f} ({f1_diff*100:.3f}%)\\n\")\n",
        "\n",
        "        print(\"   XAI Methods:\")\n",
        "        print(\"   ├─ Permutation Feature Importance (PFI)\")\n",
        "        print(\"   ├─ Predictive Mutual Information (PMI)\")\n",
        "        print(\"   ├─ Sensitivity-based Feature Importance (SOFI)\")\n",
        "        print(\"   ├─ SHAP Values (if applicable)\")\n",
        "        print(\"   ├─ Model-based Importance\")\n",
        "        print(\"   └─ Aggregated Analysis\\n\")\n",
        "\n",
        "        if best_model_short in models_dict:\n",
        "            try:\n",
        "                start_xai_time = time.time()\n",
        "\n",
        "                xai_results[best_model_short] = get_feature_importance(\n",
        "                    model=models_dict[best_model_short],\n",
        "                    X=X_train,\n",
        "                    y=y_train,\n",
        "                    model_name=best_model_short,\n",
        "                    feature_names=features,\n",
        "                    config=CONFIG\n",
        "                )\n",
        "\n",
        "                end_xai_time = time.time()\n",
        "                xai_duration = end_xai_time - start_xai_time\n",
        "\n",
        "                print(f\"\\n   ✅ XAI Analysis Completed Successfully!\")\n",
        "                print(f\"   ⏱️  Duration: {xai_duration:.2f}s ({xai_duration/60:.2f} minutes)\")\n",
        "                print(f\"   📋 Methods Analyzed: {len(xai_results[best_model_short])}\")\n",
        "\n",
        "                print(\"\\n   Method Results:\")\n",
        "                for method_name, importance_scores in xai_results[best_model_short].items():\n",
        "                    if importance_scores is not None and len(importance_scores) > 0:\n",
        "                        top_3_idx = np.argsort(importance_scores)[-3:][::-1]\n",
        "                        top_3_idx = np.atleast_1d(top_3_idx).flatten()\n",
        "                        top_3_features = [features[int(i)] for i in top_3_idx]\n",
        "                        print(f\"   ├─ {method_name:8s}: {', '.join(top_3_features[:2])}...\")\n",
        "\n",
        "            except Exception as e_xai:\n",
        "                print(f\"\\n   ❌ XAI Analysis Failed: {e_xai}\")\n",
        "                print(f\"   Continuing without XAI results...\\n\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(f\"   ⚠️  Warning: Best model '{best_model_short}' not found in models_dict\")\n",
        "            print(f\"   Available models: {list(models_dict.keys())}\\n\")\n",
        "\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # PER-CLASS PERFORMANCE ANALYSIS (UNCHANGED - Keep as is)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"📊 PER-CLASS PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys()\n",
        "                           if pd.notna(results[m].get('test_f1'))}\n",
        "\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            metrics = results[best_model]\n",
        "\n",
        "            print(f\"🏆 BEST MODEL: {best_model}\\n\")\n",
        "\n",
        "            if 'per_class_metrics' in metrics:\n",
        "                class_names = ['Away Win', 'Draw', 'Home Win']\n",
        "                emojis = ['✈️', '🤝', '🏠']\n",
        "\n",
        "                print(\"  Class-wise Performance:\\n\")\n",
        "                print(\"  \" + \"-\"*80)\n",
        "\n",
        "                for emoji, class_name in zip(emojis, class_names):\n",
        "                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)\n",
        "                    prec = metrics['per_class_metrics']['precision'].get(class_name, 0)\n",
        "                    rec = metrics['per_class_metrics']['recall'].get(class_name, 0)\n",
        "                    f1 = metrics['per_class_metrics']['f1'].get(class_name, 0)\n",
        "                    sup = metrics['per_class_metrics']['support'].get(class_name, 0)\n",
        "\n",
        "                    print(f\"  {emoji} {class_name:10s}:\")\n",
        "                    print(f\"     • Accuracy:  {acc:.4f} ({acc*100:5.2f}%)\")\n",
        "                    print(f\"     • Precision: {prec:.4f}\")\n",
        "                    print(f\"     • Recall:    {rec:.4f}\")\n",
        "                    print(f\"     • F1-Score:  {f1:.4f}\")\n",
        "                    print(f\"     • Support:   {sup:,} samples\")\n",
        "                    print()\n",
        "\n",
        "                print(\"  \" + \"-\"*80)\n",
        "\n",
        "                overall_acc = metrics.get('test_acc', 0)\n",
        "                overall_f1 = metrics.get('test_f1', 0)\n",
        "\n",
        "                print(f\"\\n  📈 Overall Performance:\")\n",
        "                print(f\"     • Weighted Accuracy: {overall_acc:.4f} ({overall_acc*100:.2f}%)\")\n",
        "                print(f\"     • Weighted F1-Score: {overall_f1:.4f}\")\n",
        "\n",
        "                acc_dict = metrics['per_class_metrics']['accuracy']\n",
        "                best_class = max(acc_dict, key=acc_dict.get)\n",
        "                worst_class = min(acc_dict, key=acc_dict.get)\n",
        "\n",
        "                print(f\"\\n  🎯 Insights:\")\n",
        "                print(f\"     • Best Predicted:  {best_class} ({acc_dict[best_class]*100:.2f}%)\")\n",
        "                print(f\"     • Worst Predicted: {worst_class} ({acc_dict[worst_class]*100:.2f}%)\")\n",
        "                print(f\"     • Class Imbalance: {max(acc_dict.values()) - min(acc_dict.values()):.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Per-class analysis error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # HYPERPARAMETER KAYDETME (UNCHANGED - Keep as is)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        params_path = os.path.join(OUT_DIR, \"best_hyperparameters.json\")\n",
        "        safe_params_dict = {}\n",
        "        for k, v in best_params_dict.items():\n",
        "            if isinstance(v, dict):\n",
        "                safe_params_dict[k] = {\n",
        "                    p_k: (int(p_v) if isinstance(p_v, np.integer)\n",
        "                          else float(p_v) if isinstance(p_v, np.floating)\n",
        "                          else p_v)\n",
        "                    for p_k, p_v in v.items()\n",
        "                }\n",
        "            else:\n",
        "                safe_params_dict[k] = v\n",
        "\n",
        "        with open(params_path, 'w') as f:\n",
        "            filtered_params = {k: v for k, v in safe_params_dict.items()\n",
        "                             if v != \"Optimization Failed\"}\n",
        "            json.dump(filtered_params, f, indent=2)\n",
        "        print(f\"\\n✅ Hyperparameters saved: {params_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Could not save hyperparameters: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ: RETURN PREPARATION & VALIDATION\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"📦 PREPARING RETURN VALUES\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(f\"[RETURN INFO] Function will return 4 values:\")\n",
        "    print(f\"  1. results:       Performance metrics for {len(results)} models\")\n",
        "    print(f\"  2. models_dict:   {len(models_dict)} trained model objects\")\n",
        "    print(f\"  3. xai_results:   {len(xai_results)} XAI analyses\")\n",
        "    print(f\"  4. validation_data: Tuple of (X_val, y_val)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ VALIDATION DATA PREPARATION\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if X_val is not None and y_val is not None:\n",
        "        # Validation exists (PRE-SMOTE split was done in ADIM 3.7)\n",
        "        print(f\"\\n  ✅ Validation set prepared:\")\n",
        "        print(f\"     • X_val shape: {X_val.shape}\")\n",
        "        print(f\"     • y_val length: {len(y_val)}\")\n",
        "        print(f\"     • Source: PRE-SMOTE split (ADIM 3.7)\")\n",
        "        print(f\"     • Contains: 100% REAL data (zero synthetic samples)\")\n",
        "\n",
        "        # Verify class distribution\n",
        "        try:\n",
        "            val_counts = y_val.value_counts().sort_index()\n",
        "            print(f\"\\n     📊 Validation class distribution:\")\n",
        "            print(f\"        • Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"        • Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"        • Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)\")\n",
        "        except Exception as e_dist:\n",
        "            print(f\"        ⚠️  Could not compute distribution: {e_dist}\")\n",
        "\n",
        "        print(f\"\\n     🎯 Usage:\")\n",
        "        print(f\"        • Ablation analysis (G28: XAI comparison)\")\n",
        "        print(f\"        • Ablation analysis (G29: Cumulative importance)\")\n",
        "        print(f\"        • XAI method verification\")\n",
        "        print(f\"        • Early stopping (already used during training)\")\n",
        "\n",
        "        validation_data = (X_val, y_val)\n",
        "\n",
        "    else:\n",
        "        # No validation (early stopping disabled)\n",
        "        print(f\"\\n  ⚠️  Validation set: NOT AVAILABLE\")\n",
        "        print(f\"     • Reason: Early stopping disabled in CONFIG\")\n",
        "        print(f\"     • Impact: Ablation will use test set (less ideal)\")\n",
        "        print(f\"     • Recommendation: Enable early stopping for better ablation\")\n",
        "\n",
        "        validation_data = (None, None)\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ SANITY CHECKS\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    print(f\"\\n[SANITY CHECKS]\")\n",
        "\n",
        "    # Check 1: Results not empty\n",
        "    if not results:\n",
        "        print(f\"  ⚠️  WARNING: results dictionary is EMPTY!\")\n",
        "        print(f\"     No models were successfully trained\")\n",
        "    else:\n",
        "        print(f\"  ✅ Results: {len(results)} models trained\")\n",
        "\n",
        "    # Check 2: Models dict matches results\n",
        "    if len(models_dict) != len(results):\n",
        "        print(f\"  ⚠️  WARNING: Mismatch between models_dict and results\")\n",
        "        print(f\"     models_dict: {len(models_dict)}, results: {len(results)}\")\n",
        "    else:\n",
        "        print(f\"  ✅ Models dict: Consistent with results\")\n",
        "\n",
        "    # Check 3: XAI results (may be empty initially)\n",
        "    if not xai_results:\n",
        "        print(f\"  ℹ️  XAI results: Empty (will be populated after best model selection)\")\n",
        "    else:\n",
        "        print(f\"  ✅ XAI results: {len(xai_results)} analyses\")\n",
        "\n",
        "    # Check 4: Validation data consistency\n",
        "    if validation_data[0] is not None:\n",
        "        if validation_data[1] is None:\n",
        "            print(f\"  ❌ ERROR: X_val exists but y_val is None!\")\n",
        "        elif len(validation_data[0]) != len(validation_data[1]):\n",
        "            print(f\"  ❌ ERROR: X_val and y_val length mismatch!\")\n",
        "            print(f\"     X_val: {len(validation_data[0])}, y_val: {len(validation_data[1])}\")\n",
        "        else:\n",
        "            print(f\"  ✅ Validation data: Consistent\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ FINAL RETURN STATEMENT\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    return results, models_dict, xai_results, validation_data\n",
        "\n",
        "    # ============================================================================\n",
        "    # HYPERPARAMETER KAYDETME\n",
        "    # ============================================================================\n",
        "    try:\n",
        "        params_path = os.path.join(OUT_DIR, \"best_hyperparameters.json\")\n",
        "        safe_params_dict = {}\n",
        "        for k, v in best_params_dict.items():\n",
        "            if isinstance(v, dict):\n",
        "                safe_params_dict[k] = {p_k: (int(p_v) if isinstance(p_v, np.integer) else float(p_v) if isinstance(p_v, np.floating) else p_v) for p_k, p_v in v.items()}\n",
        "            else:\n",
        "                safe_params_dict[k] = v\n",
        "\n",
        "        with open(params_path, 'w') as f:\n",
        "            filtered_params = {k: v for k, v in safe_params_dict.items() if v != \"Optimization Failed\"}\n",
        "            json.dump(filtered_params, f, indent=2)\n",
        "        print(f\"\\n✅ Hyperparameters saved: {params_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Could not save hyperparameters: {e}\")\n",
        "\n",
        "    return results, models_dict, xai_results  # ← RETURN STATEMENT\n",
        "\n",
        "# CREATE TABLES\n",
        "def create_dynamic_tables(results):\n",
        "    \"\"\"Create result tables\"\"\"\n",
        "\n",
        "    print(\"\\n6️⃣ CREATING TABLES...\\n\")\n",
        "    tables = {}\n",
        "    models = list(results.keys())\n",
        "\n",
        "    if not results:\n",
        "        print(\"  No results\")\n",
        "        return tables\n",
        "\n",
        "    # T1: Performance\n",
        "    perf_data = []\n",
        "    for model_key in models:\n",
        "        metrics = results.get(model_key, {})\n",
        "        perf_data.append({\n",
        "            'Model': model_key,\n",
        "            'Train Acc': f\"{metrics.get('train_acc', np.nan):.4f}\",\n",
        "            'Test Acc': f\"{metrics.get('test_acc', np.nan):.4f}\",\n",
        "            'Train F1': f\"{metrics.get('train_f1', np.nan):.4f}\",\n",
        "            'Test F1': f\"{metrics.get('test_f1', np.nan):.4f}\",\n",
        "        })\n",
        "    tables['T1_Model_Performance'] = pd.DataFrame(perf_data)\n",
        "    print(\"  ✓ T1\")\n",
        "\n",
        "    # T3: AUC\n",
        "    auc_data = []\n",
        "    for model_key in models:\n",
        "        metrics = results.get(model_key, {})\n",
        "        auc_data.append({\n",
        "            'Model': model_key,\n",
        "            'Train AUC': f\"{metrics.get('train_auc', np.nan):.4f}\",\n",
        "            'Test AUC': f\"{metrics.get('test_auc', np.nan):.4f}\",\n",
        "            'ECE (Test)': f\"{metrics.get('test_ece', np.nan):.4f}\",\n",
        "            'Kappa (Test)': f\"{metrics.get('test_kappa', np.nan):.4f}\",\n",
        "        })\n",
        "    tables['T3_AUC_Calibration'] = pd.DataFrame(auc_data)\n",
        "    print(\"  ✓ T3\")\n",
        "\n",
        "    # T4: Generalization\n",
        "    cons_data = []\n",
        "    for model_key in models:\n",
        "        metrics = results.get(model_key, {})\n",
        "        train_f1 = metrics.get('train_f1', np.nan)\n",
        "        test_f1 = metrics.get('test_f1', np.nan)\n",
        "        gap = abs(train_f1 - test_f1) if pd.notna(train_f1) and pd.notna(test_f1) else np.nan\n",
        "        generalization = 'N/A'\n",
        "        if pd.notna(gap):\n",
        "            generalization = 'Good' if gap < 0.1 else 'Fair' if gap < 0.2 else 'Poor'\n",
        "\n",
        "        cons_data.append({\n",
        "            'Model': model_key,\n",
        "            'Train-Test Gap (F1)': f\"{gap:.4f}\",\n",
        "            'Generalization': generalization,\n",
        "        })\n",
        "    tables['T4_Generalization'] = pd.DataFrame(cons_data)\n",
        "    print(\"  ✓ T4\")\n",
        "\n",
        "    # T7: Kappa\n",
        "    tables['T7_Kappa_Analysis'] = pd.DataFrame({\n",
        "        'Model': models,\n",
        "        'Train Kappa': [f\"{results.get(m, {}).get('train_kappa', np.nan):.4f}\" for m in models],\n",
        "        'Test Kappa': [f\"{results.get(m, {}).get('test_kappa', np.nan):.4f}\" for m in models],\n",
        "    })\n",
        "    print(\"  ✓ T7\")\n",
        "\n",
        "    # T8: Summary\n",
        "    try:\n",
        "        mean_test_acc = np.nanmean([results.get(m, {}).get('test_acc') for m in models])\n",
        "        mean_test_f1 = np.nanmean([results.get(m, {}).get('test_f1') for m in models])\n",
        "        mean_test_auc = np.nanmean([results.get(m, {}).get('test_auc') for m in models])\n",
        "        mean_test_loss = np.nanmean([results.get(m, {}).get('test_loss') for m in models])\n",
        "\n",
        "        tables['T8_Summary_Stats'] = pd.DataFrame({\n",
        "            'Metric': ['Mean Test Acc', 'Mean Test F1', 'Mean Test AUC', 'Mean Test Loss'],\n",
        "            'Value': [\n",
        "                f\"{mean_test_acc:.4f}\", f\"{mean_test_f1:.4f}\",\n",
        "                f\"{mean_test_auc:.4f}\", f\"{mean_test_loss:.4f}\",\n",
        "            ],\n",
        "        })\n",
        "        print(\"  ✓ T8\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # T9: Best Model\n",
        "    try:\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            metrics = results[best_model]\n",
        "\n",
        "            comparable_data = {\n",
        "                'Metric': ['Accuracy', 'F1-Score', 'AUC-ROC', 'Cohen\\'s Kappa'],\n",
        "                'Value': [\n",
        "                    f\"{metrics.get('test_acc', np.nan):.4f}\",\n",
        "                    f\"{metrics.get('test_f1', np.nan):.4f}\",\n",
        "                    f\"{metrics.get('test_auc', np.nan):.4f}\",\n",
        "                    f\"{metrics.get('test_kappa', np.nan):.4f}\",\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            tables['T9_Best_Model_Performance'] = pd.DataFrame(comparable_data)\n",
        "            print(f\"  ✓ T9 ({best_model})\")\n",
        "    except:\n",
        "        pass\n",
        "    # T10: Per-Class Performance Metrics\n",
        "    try:\n",
        "        per_class_rows = []\n",
        "        class_names = ['Home Win', 'Draw', 'Away Win']\n",
        "\n",
        "        for model_key in models:\n",
        "            metrics = results.get(model_key, {})\n",
        "            if 'per_class_metrics' in metrics:\n",
        "                for class_name in class_names:\n",
        "                    per_class_rows.append({\n",
        "                        'Model': model_key,\n",
        "                        'Class': class_name,\n",
        "                        'Precision': f\"{metrics['per_class_metrics']['precision'].get(class_name, 0):.4f}\",\n",
        "                        'Recall': f\"{metrics['per_class_metrics']['recall'].get(class_name, 0):.4f}\",\n",
        "                        'F1-Score': f\"{metrics['per_class_metrics']['f1'].get(class_name, 0):.4f}\",\n",
        "                        'Support': metrics['per_class_metrics']['support'].get(class_name, 0)\n",
        "                    })\n",
        "\n",
        "        if per_class_rows:\n",
        "            tables['T10_Per_Class_Metrics'] = pd.DataFrame(per_class_rows)\n",
        "            print(\"  ✓ T10\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ T10: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ Created {len(tables)} tables\\n\")\n",
        "    return tables\n",
        "\n",
        "    # ✅ YENİ TABLO: T11 - Per-Class Accuracy Comparison\n",
        "    try:\n",
        "        per_class_acc_rows = []\n",
        "        class_names = ['Away Win', 'Draw', 'Home Win']\n",
        "\n",
        "        for model_key in models:\n",
        "            metrics = results.get(model_key, {})\n",
        "\n",
        "            # Test per-class accuracy\n",
        "            if 'per_class_metrics' in metrics and 'accuracy' in metrics['per_class_metrics']:\n",
        "                for class_name in class_names:\n",
        "                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)\n",
        "                    per_class_acc_rows.append({\n",
        "                        'Model': model_key,\n",
        "                        'Class': class_name,\n",
        "                        'Accuracy': f\"{acc:.4f}\",\n",
        "                        'Accuracy %': f\"{acc*100:.2f}%\"\n",
        "                    })\n",
        "\n",
        "        if per_class_acc_rows:\n",
        "            tables['T11_Per_Class_Accuracy'] = pd.DataFrame(per_class_acc_rows)\n",
        "            print(\"  ✓ T11 (Per-Class Accuracy)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ T11: {e}\")\n",
        "\n",
        "    # ✅ YENİ TABLO: T12 - Best Model Detailed Breakdown\n",
        "    try:\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            metrics = results[best_model]\n",
        "\n",
        "            breakdown_rows = []\n",
        "\n",
        "            if 'per_class_metrics' in metrics:\n",
        "                class_names = ['Away Win', 'Draw', 'Home Win']\n",
        "\n",
        "                for class_name in class_names:\n",
        "                    breakdown_rows.append({\n",
        "                        'Class': class_name,\n",
        "                        'Accuracy': f\"{metrics['per_class_metrics']['accuracy'].get(class_name, 0):.4f}\",\n",
        "                        'Precision': f\"{metrics['per_class_metrics']['precision'].get(class_name, 0):.4f}\",\n",
        "                        'Recall': f\"{metrics['per_class_metrics']['recall'].get(class_name, 0):.4f}\",\n",
        "                        'F1-Score': f\"{metrics['per_class_metrics']['f1'].get(class_name, 0):.4f}\",\n",
        "                        'Support': metrics['per_class_metrics']['support'].get(class_name, 0)\n",
        "                    })\n",
        "\n",
        "            if breakdown_rows:\n",
        "                tables['T12_Best_Model_Breakdown'] = pd.DataFrame(breakdown_rows)\n",
        "                print(f\"  ✓ T12 (Best Model: {best_model})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ T12: {e}\")\n",
        "\n",
        "def create_feature_importance_comparison_v19_ABLATION(\n",
        "    models_dict, xai_results, X_train, X_test, y_train, y_test,\n",
        "    X_val, y_val,\n",
        "    features, output_dir, results_dict\n",
        "):\n",
        "    print(\"\\n[G28-v19] Creating XAI Method Comparison (Ablation-Based)...\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 1: EN İYİ MODELİ BUL\n",
        "    # ========================================================================\n",
        "    valid_models_f1 = {\n",
        "        m: results_dict[m].get('test_f1', -1)\n",
        "        for m in results_dict.keys()\n",
        "        if pd.notna(results_dict[m].get('test_f1'))\n",
        "    }\n",
        "\n",
        "    if not valid_models_f1:\n",
        "        print(\"  ⚠️ No valid models found\")\n",
        "        return None\n",
        "\n",
        "    best_model_key = max(valid_models_f1, key=valid_models_f1.get)\n",
        "    best_model_short = best_model_key.split(' ')[0].lower()\n",
        "    best_f1_score = valid_models_f1[best_model_key]\n",
        "\n",
        "    print(f\"  Best Model: {best_model_key} (F1: {best_f1_score:.4f})\")\n",
        "\n",
        "    # ✅ YENİ: Validation set kontrolü\n",
        "    if X_val is None or y_val is None:\n",
        "        print(f\"  ⚠️  WARNING: No validation set provided!\")\n",
        "        print(f\"     Falling back to test set (less ideal for ablation)\")\n",
        "        X_ablation = X_test\n",
        "        y_ablation = y_test\n",
        "        ablation_set_name = \"Test Set\"\n",
        "    else:\n",
        "        print(f\"  ✅ Using validation set for ablation: {len(X_val):,} samples\")\n",
        "        X_ablation = X_val\n",
        "        y_ablation = y_val\n",
        "        ablation_set_name = \"Validation Set\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 2: XAI SONUÇLARINI AL\n",
        "    # ========================================================================\n",
        "    if best_model_short not in xai_results:\n",
        "        print(f\"  ⚠️ No XAI results for {best_model_key}\")\n",
        "        return None\n",
        "\n",
        "    xai_data = xai_results[best_model_short]\n",
        "    model = models_dict[best_model_short]\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ EKLEME: SMOOTHING HELPER FUNCTION\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # create_feature_importance_comparison_v19_ABLATION() fonksiyonundan ÖNCE\n",
        "    # Satır ~1820 civarına ekle\n",
        "    def smooth_ablation_curve(scores, window=3, method='moving_average'):\n",
        "        \"\"\"\n",
        "        Smooth ablation curve to reduce noise and jumps\n",
        "\n",
        "        Args:\n",
        "            scores: List of normalized F1 scores\n",
        "            window: Smoothing window size (3 or 5 recommended)\n",
        "            method: 'moving_average' or 'savgol'\n",
        "\n",
        "        Returns:\n",
        "            Smoothed scores (numpy array)\n",
        "        \"\"\"\n",
        "        scores_array = np.array(scores)\n",
        "\n",
        "        if method == 'moving_average':\n",
        "            # ✅ OPTION A: Simple Moving Average (RECOMMENDED)\n",
        "            from scipy.ndimage import uniform_filter1d\n",
        "            smoothed = uniform_filter1d(scores_array, size=window, mode='nearest')\n",
        "\n",
        "        elif method == 'savgol':\n",
        "            # ✅ OPTION B: Savitzky-Golay filter (more sophisticated)\n",
        "            from scipy.signal import savgol_filter\n",
        "            # Window must be odd and >= polyorder+2\n",
        "            window = window if window % 2 == 1 else window + 1\n",
        "            smoothed = savgol_filter(scores_array, window_length=window,\n",
        "                                    polyorder=2, mode='nearest')\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown smoothing method: {method}\")\n",
        "\n",
        "        # ✅ CRITICAL: Ensure monotonicity (optional but recommended)\n",
        "        # Force curve to never increase\n",
        "        for i in range(1, len(smoothed)):\n",
        "            if smoothed[i] > smoothed[i-1]:\n",
        "                smoothed[i] = smoothed[i-1]  # ← Clip jumps\n",
        "\n",
        "        # ✅ Clip to valid range [0, 1]\n",
        "        smoothed = np.clip(smoothed, 0.0, 1.0)\n",
        "\n",
        "        return smoothed\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 3: ABLATION-BASED CURVE COMPUTATION (İÇ FONKSİYON - GÜNCELLENDİ)\n",
        "    # ========================================================================\n",
        "    # Bu fonksiyon, arkadaşınızın/danışmanınızın önerdiği tüm düzeltmeleri içerir:\n",
        "    # 1. Baseline F1, X_test üzerinden hesaplanır (X_train yerine).\n",
        "    # 2. 'zero' ablation metodunu destekler.\n",
        "    # 3. Monotonicity (zıplama) kontrolü 'force_strict_monotonicity' (Tüm zıplamaları engelle)\n",
        "    #    ve 'monotonicity_threshold' (esnek eşik) parametreleri eklendi.\n",
        "\n",
        "    def create_feature_importance_comparison_v19_ABLATION(\n",
        "        models_dict, xai_results, X_train, X_test, y_train, y_test,\n",
        "        X_val, y_val,\n",
        "        features, output_dir, results_dict\n",
        "    ):\n",
        "        print(\"\\n[G28-v19] Creating XAI Method Comparison (Ablation-Based - Academic Style)...\")\n",
        "\n",
        "        # ========================================================================\n",
        "        # STEP 1: EN İYİ MODELİ BUL\n",
        "        # ========================================================================\n",
        "        valid_models_f1 = {\n",
        "            m: results_dict[m].get('test_f1', -1)\n",
        "            for m in results_dict.keys()\n",
        "            if pd.notna(results_dict[m].get('test_f1'))\n",
        "        }\n",
        "\n",
        "        if not valid_models_f1:\n",
        "            print(\"  ⚠️ No valid models found\")\n",
        "            return None\n",
        "\n",
        "        best_model_key = max(valid_models_f1, key=valid_models_f1.get)\n",
        "        best_model_short = best_model_key.split(' ')[0].lower()\n",
        "        best_f1_score = valid_models_f1[best_model_key]\n",
        "\n",
        "        print(f\"  Best Model: {best_model_key} (F1: {best_f1_score:.4f})\")\n",
        "\n",
        "        # Validation set kontrolü\n",
        "        if X_val is None or y_val is None:\n",
        "            X_ablation = X_test\n",
        "            y_ablation = y_test\n",
        "            ablation_set_name = \"Test Set\"\n",
        "        else:\n",
        "            X_ablation = X_val\n",
        "            y_ablation = y_val\n",
        "            ablation_set_name = \"Validation Set\"\n",
        "\n",
        "        # ========================================================================\n",
        "        # STEP 2: XAI SONUÇLARINI AL\n",
        "        # ========================================================================\n",
        "        if best_model_short not in xai_results:\n",
        "            print(f\"  ⚠️ No XAI results for {best_model_key}\")\n",
        "            return None\n",
        "\n",
        "        xai_data = xai_results[best_model_short]\n",
        "        model = models_dict[best_model_short]\n",
        "\n",
        "        # ------------------------------------------------------------------------\n",
        "        # INTERNAL HELPER: Compute Curve\n",
        "        # ------------------------------------------------------------------------\n",
        "        def compute_ablation_curve_normalized(model, X_test, y_test, feature_order,\n",
        "                                              base_score, max_feats):\n",
        "            \"\"\"\n",
        "            Compute normalized ablation curve: g(pi) = F1_ablated / F1_baseline\n",
        "            \"\"\"\n",
        "            from sklearn.metrics import f1_score\n",
        "\n",
        "            if hasattr(X_test, 'values'):\n",
        "                X_work = X_test.values.copy()\n",
        "            else:\n",
        "                X_work = X_test.copy()\n",
        "\n",
        "            scores = [1.0] # Start at 100%\n",
        "\n",
        "            # Limit loop to max_features to save time, but calculate carefully\n",
        "            limit = min(len(feature_order), max_feats)\n",
        "\n",
        "            # Pre-calculate means for replacement\n",
        "            means = np.mean(X_work, axis=0)\n",
        "\n",
        "            for i in range(limit):\n",
        "                idx = feature_order[i]\n",
        "                # Ablate: Replace with mean\n",
        "                X_work[:, idx] = means[idx]\n",
        "\n",
        "                # Predict\n",
        "                y_pred = model.predict(X_work)\n",
        "                current_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "                # Normalize\n",
        "                norm_score = current_f1 / base_score\n",
        "\n",
        "                # Monotonicity Check (Simple clamp)\n",
        "                if norm_score > scores[-1]:\n",
        "                    norm_score = scores[-1]\n",
        "\n",
        "                scores.append(norm_score)\n",
        "\n",
        "            return scores\n",
        "\n",
        "        # ------------------------------------------------------------------------\n",
        "        # STEP 3: COMPUTE CURVES\n",
        "        # ------------------------------------------------------------------------\n",
        "        import time\n",
        "        methods_to_plot = ['PFI', 'SOFI', 'SHAP', 'LTCN'] # Main methods\n",
        "        available_methods = [m for m in methods_to_plot if m in xai_data]\n",
        "\n",
        "        # Config'den max features al, yoksa 20 ile sınırla (görsellik için)\n",
        "        # Arkadaşınızın grafiği 14'te bitiyor, biz de odağı artırmak için 20-25 civarı tutalım.\n",
        "        xai_config = CONFIG.get(\"xai\", {})\n",
        "        MAX_EVAL_FEATURES = xai_config.get(\"ablation_max_features\", 25)\n",
        "\n",
        "        print(f\"  Computing curves (Limit: Top {MAX_EVAL_FEATURES} features)...\")\n",
        "\n",
        "        curves_normalized = {}\n",
        "\n",
        "        # Baseline F1\n",
        "        y_pred_base = model.predict(X_ablation)\n",
        "        base_f1 = f1_score(y_ablation, y_pred_base, average='weighted')\n",
        "\n",
        "        for method in available_methods:\n",
        "            start_t = time.time()\n",
        "            importance = xai_data[method]\n",
        "            # Sort features by importance\n",
        "            feature_order = np.argsort(importance)[::-1]\n",
        "\n",
        "            curve = compute_ablation_curve_normalized(\n",
        "                model, X_ablation, y_ablation, feature_order, base_f1, MAX_EVAL_FEATURES\n",
        "            )\n",
        "            curves_normalized[method] = curve\n",
        "            print(f\"    ✓ {method}: {time.time()-start_t:.2f}s\")\n",
        "\n",
        "        # Random Baseline\n",
        "        import random\n",
        "        random.seed(42) # Reproducible random\n",
        "        random_order = list(range(X_ablation.shape[1]))\n",
        "        random.shuffle(random_order)\n",
        "        curves_normalized['Random'] = compute_ablation_curve_normalized(\n",
        "            model, X_ablation, y_ablation, random_order, base_f1, MAX_EVAL_FEATURES\n",
        "        )\n",
        "\n",
        "        # ========================================================================\n",
        "        # STEP 4: PLOTTING (ACADEMIC STYLE MATCHING)\n",
        "        # ========================================================================\n",
        "        print(f\"\\n  Generating Academic Style Plot...\")\n",
        "\n",
        "        # Arkadaşınızın stil ayarları\n",
        "        plt.rcParams[\"font.family\"] = \"serif\"\n",
        "        # plt.rcParams[\"font.serif\"] = [\"STIXGeneral\"] # Eğer sisteminizde yoksa hata verebilir, o yüzden kapattım\n",
        "        plt.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        # 1. X Ekseni Limiti Belirleme (Arkadaşınızın x_max mantığı)\n",
        "        # Grafiğin nerede biteceğine karar veriyoruz.\n",
        "        x_max = MAX_EVAL_FEATURES\n",
        "        x_all = np.arange(x_max + 1)\n",
        "\n",
        "        # 2. Envelope (Zarf) Hesaplama - En düşük değerlerin altını boyama\n",
        "        envelope = []\n",
        "        for i in x_all:\n",
        "            vals_at_i = []\n",
        "            for scores in curves_normalized.values():\n",
        "                if i < len(scores):\n",
        "                    vals_at_i.append(scores[i])\n",
        "            if not vals_at_i:\n",
        "                envelope.append(0.0)\n",
        "            else:\n",
        "                envelope.append(min(vals_at_i))\n",
        "\n",
        "        # Shading (Gri Alan)\n",
        "        ax.fill_between(\n",
        "            x_all,\n",
        "            0,\n",
        "            envelope,\n",
        "            facecolor=\"lightgray\",\n",
        "            alpha=0.5,\n",
        "            label=\"_nolegend_\"\n",
        "        )\n",
        "\n",
        "        # 3. Renk ve Stil Tanımları\n",
        "        colors = {\n",
        "            'LTCN': '#1A9EC6',   # Blue\n",
        "            'SOFI': '#FF0000',   # Red\n",
        "            'SHAP': '#FFD700',   # Gold\n",
        "            'LIME': '#15BD0C',   # Green\n",
        "            'PFI':  '#8A2BE2',   # Purple\n",
        "            'Random': 'black'    # Black Dashed\n",
        "        }\n",
        "        markers = {\n",
        "            'LTCN': 'o', 'SOFI': 'o', 'SHAP': 'o', 'LIME': 's', 'PFI': 'o', 'Random': None\n",
        "        }\n",
        "        linestyles = {\n",
        "            'Random': '--'\n",
        "        }\n",
        "\n",
        "        # 4. Çizgileri Çizme (Natural Stopping ile)\n",
        "        for label, scores in curves_normalized.items():\n",
        "            # Find natural stopping point (min score index)\n",
        "            # Arkadaşınızın mantığı: Skorun dip yaptığı yerde çizgiyi kes.\n",
        "            min_score = min(scores)\n",
        "            idx_stop = scores.index(min_score)\n",
        "\n",
        "            # Eğer idx_stop çok küçükse (hemen düştüyse) biraz uzat görsel görünsün\n",
        "            idx_stop = max(idx_stop, 3)\n",
        "            # Eğer idx_stop limit dışındaysa sınırla\n",
        "            idx_stop = min(idx_stop, x_max)\n",
        "\n",
        "            x_vals = list(range(idx_stop + 1))\n",
        "            y_vals = scores[:idx_stop + 1]\n",
        "\n",
        "            style = linestyles.get(label, '-')\n",
        "            color = colors.get(label, 'gray')\n",
        "            marker = markers.get(label, 'o')\n",
        "\n",
        "            ax.plot(\n",
        "                x_vals, y_vals,\n",
        "                color=color,\n",
        "                linestyle=style,\n",
        "                marker=marker,\n",
        "                markersize=7,\n",
        "                linewidth=2.5,\n",
        "                label=label,\n",
        "                markevery=1\n",
        "            )\n",
        "\n",
        "        # 5. Eksen ve Başlık Ayarları (Bold, Serif)\n",
        "        ax.set_title(\"Feature Importance Comparison\", fontsize=18, fontweight='bold')\n",
        "        ax.set_xlabel(r\"$\\pi_i$ (Feature Rank)\", fontsize=16, fontweight='bold')\n",
        "        ax.set_ylabel(r\"$g(\\pi)$ (Normalized F1)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Limitler\n",
        "        ax.set_xlim(0, x_max)\n",
        "        ax.set_ylim(-0.02, 1.02)\n",
        "\n",
        "        # Grid ve Legend\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.legend(fontsize=12, loc='upper right', framealpha=0.95, edgecolor='black')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # 6. Kaydetme (Vektörel PDF - Beyaz Arka Plan)\n",
        "        output_path_pdf = os.path.join(output_dir, '28_xai_ablation_comparison_academic.pdf')\n",
        "\n",
        "        # Zorla beyaz arka plan (Stil bozulmasını önler)\n",
        "        fig.set_facecolor('white')\n",
        "        ax.set_facecolor('white')\n",
        "\n",
        "        plt.savefig(\n",
        "            output_path_pdf,\n",
        "            format='pdf',\n",
        "            dpi=300,\n",
        "            bbox_inches='tight',\n",
        "            facecolor='white',\n",
        "            edgecolor='none'\n",
        "        )\n",
        "\n",
        "        # PNG yedeği (Hızlı önizleme için)\n",
        "        output_path_png = output_path_pdf.replace('.pdf', '.png')\n",
        "        plt.savefig(output_path_png, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\n  ✅ Academic Plot Saved: {os.path.basename(output_path_pdf)}\")\n",
        "\n",
        "        # AUC Raporu\n",
        "        print(\"\\n  === Normalized AUC Values (Lower is Better) ===\")\n",
        "        for label, scores in curves_normalized.items():\n",
        "            # Basit trapezoidal alan hesabı\n",
        "            auc = np.trapz(scores, dx=1)\n",
        "            print(f\"  {label:10s}: {auc:.4f}\")\n",
        "\n",
        "        return output_path_pdf\n",
        "\n",
        "# ============================================================================\n",
        "# BURAYA KADAR ⬆️\n",
        "# ============================================================================\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 📐 DUAL FORMAT SAVE FUNCTION\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Satır ~2480'e EKLE (THESIS_COLORS'dan sonra, create_dynamic_graphics'ten önce)\n",
        "\n",
        "def save_figure_dual_format(fig, base_path, dpi=300):\n",
        "    \"\"\"Save figure in both PNG and PDF formats\"\"\"\n",
        "    save_params = {\n",
        "        'bbox_inches': 'tight',\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'none',\n",
        "        'pad_inches': 0.1\n",
        "    }\n",
        "\n",
        "    png_path = f\"{base_path}.png\"\n",
        "    fig.savefig(png_path, dpi=dpi, format='png', **save_params)\n",
        "\n",
        "    pdf_path = f\"{base_path}.pdf\"\n",
        "    pdf_metadata = {\n",
        "        'Title': os.path.basename(base_path),\n",
        "        'Author': 'Dokumus - Tilburg University',\n",
        "        'Subject': 'Football Match Prediction System v18.0',\n",
        "        'Keywords': 'Machine Learning, XAI, Football Prediction',\n",
        "        'Creator': 'Matplotlib',\n",
        "        'Producer': 'Thesis Research'\n",
        "    }\n",
        "\n",
        "    fig.savefig(pdf_path, format='pdf', dpi=300, metadata=pdf_metadata, **save_params)\n",
        "\n",
        "    return png_path, pdf_path\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 🎨 THESIS TITLE HELPER\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def get_thesis_compliant_title(graphic_id, short_title, rq=None):\n",
        "    \"\"\"Generate thesis-compliant title format\"\"\"\n",
        "    title = f\"Figure {graphic_id}: {short_title}\"\n",
        "    if rq:\n",
        "        title = f\"{title}\\n({rq}: Predictive Performance Evaluation)\"\n",
        "    return title\n",
        "\n",
        "# CREATE GRAPHICS FUNCTION\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 📊 CREATE DYNAMIC GRAPHICS FUNCTION (COMPLETE REWRITE)\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# 📊 CREATE DYNAMIC GRAPHICS FUNCTION (COMPLETE REWRITE)\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def create_dynamic_graphics(results, xai_results, output_dir, features, models_dict,\n",
        "                           y_test=None, X_test=None, X_train=None, y_train=None,\n",
        "                           X_val=None, y_val=None):\n",
        "    \"\"\"\n",
        "    Create comprehensive graphics with thesis-compliant styling\n",
        "\n",
        "    All graphics use THESIS_COLORS (blue tones) except G28 (XAI comparison)\n",
        "    All graphics saved in dual format: PNG (preview) + PDF (thesis)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n7️⃣ CREATING GRAPHICS (Thesis-Compliant)...\\n\")\n",
        "    print(\"  📐 Format: Dual (PNG + PDF)\")\n",
        "    print(\"  🎨 Color Palette: Blue tones (G28 exception)\")\n",
        "    print(\"  📊 Resolution: 300 DPI\\n\")\n",
        "\n",
        "    graphics = []  # Will store PDF paths only\n",
        "\n",
        "    if not results:\n",
        "        print(\"  No results\")\n",
        "        return graphics\n",
        "\n",
        "    models = list(results.keys())\n",
        "    x_pos = np.arange(len(models))\n",
        "    width = 0.35\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G1: MODEL ACCURACY COMPARISON\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G1] Creating Model Accuracy Comparison...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        # ✅ THESIS COLORS: Blue tones\n",
        "        train_bars = ax.bar(\n",
        "            x_pos - width/2,\n",
        "            [results[m].get('train_acc', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Train',\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        test_bars = ax.bar(\n",
        "            x_pos + width/2,\n",
        "            [results[m].get('test_acc', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Test',\n",
        "            color=THESIS_COLORS['primary']['light_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)\n",
        "        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G1', 'Model Accuracy Comparison', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.set_ylim([0, 1.1])\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '01_model_accuracy')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G1 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G1: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G2: F1-SCORE COMPARISON\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G2] Creating F1-Score Comparison...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        train_bars = ax.bar(\n",
        "            x_pos - width/2,\n",
        "            [results[m].get('train_f1', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Train',\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        test_bars = ax.bar(\n",
        "            x_pos + width/2,\n",
        "            [results[m].get('test_f1', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Test',\n",
        "            color=THESIS_COLORS['primary']['light_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)\n",
        "        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G2', 'Model F1-Score Comparison', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.set_ylim([0, 1.1])\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '02_model_f1_score')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G2 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G2: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G3: AUC-ROC COMPARISON\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G3] Creating AUC-ROC Comparison...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        train_bars = ax.bar(\n",
        "            x_pos - width/2,\n",
        "            [results[m].get('train_auc', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Train',\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        test_bars = ax.bar(\n",
        "            x_pos + width/2,\n",
        "            [results[m].get('test_auc', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Test',\n",
        "            color=THESIS_COLORS['primary']['light_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)\n",
        "        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('AUC-ROC', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G3', 'Model AUC-ROC Comparison', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.set_ylim([0, 1.1])\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '03_model_auc')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G3 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G3: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G5: COHEN'S KAPPA SCORE\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G5] Creating Cohen's Kappa Score...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        train_bars = ax.bar(\n",
        "            x_pos - width/2,\n",
        "            [results[m].get('train_kappa', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Train',\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        test_bars = ax.bar(\n",
        "            x_pos + width/2,\n",
        "            [results[m].get('test_kappa', np.nan) for m in models],\n",
        "            width,\n",
        "            label='Test',\n",
        "            color=THESIS_COLORS['primary']['light_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)\n",
        "        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel(\"Cohen's Kappa\", fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G5', \"Cohen's Kappa Score\", 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '05_kappa_score')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G5 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G5: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G7: RANKED PROBABILITY SCORE (RPS)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G7] Creating Ranked Probability Score...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        rps_scores = [results[m].get('test_rps', np.nan) for m in models]\n",
        "        bars = ax.bar(\n",
        "            x_pos,\n",
        "            rps_scores,\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('RPS', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G7', 'Ranked Probability Score', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '07_rps_score')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G7 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G7: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G8: EXPECTED CALIBRATION ERROR (ECE)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G8] Creating Expected Calibration Error...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        ece_scores = [results[m].get('test_ece', np.nan) for m in models]\n",
        "        bars = ax.bar(\n",
        "            x_pos,\n",
        "            ece_scores,\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8,\n",
        "            edgecolor='black'\n",
        "        )\n",
        "\n",
        "        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('ECE', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G8', 'Expected Calibration Error', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '08_ece_score')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G8 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G8: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G9: PROBABILISTIC METRICS COMPARISON (Brier, RPS, ECE)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G9] Creating Probabilistic Metrics Comparison...\")\n",
        "        fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "        x_pos_prob = np.arange(len(models))\n",
        "        width_prob = 0.25\n",
        "\n",
        "        brier_scores = [results[m].get('test_brier', np.nan) for m in models]\n",
        "        rps_scores = [results[m].get('test_rps', np.nan) for m in models]\n",
        "        ece_scores = [results[m].get('test_ece', np.nan) for m in models]\n",
        "\n",
        "        # ✅ THESIS COLORS: Blue gradient\n",
        "        bars1 = ax.bar(\n",
        "            x_pos_prob - width_prob, brier_scores, width_prob,\n",
        "            label='Brier Score',\n",
        "            color=THESIS_COLORS['primary']['dark_blue'],\n",
        "            alpha=0.8, edgecolor='black'\n",
        "        )\n",
        "        bars2 = ax.bar(\n",
        "            x_pos_prob, rps_scores, width_prob,\n",
        "            label='RPS',\n",
        "            color=THESIS_COLORS['primary']['medium_blue'],\n",
        "            alpha=0.8, edgecolor='black'\n",
        "        )\n",
        "        bars3 = ax.bar(\n",
        "            x_pos_prob + width_prob, ece_scores, width_prob,\n",
        "            label='ECE',\n",
        "            color=THESIS_COLORS['primary']['light_blue'],\n",
        "            alpha=0.8, edgecolor='black'\n",
        "        )\n",
        "\n",
        "        for bars in [bars1, bars2, bars3]:\n",
        "            add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('Score (Lower is Better)', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G9', 'Probabilistic Metrics Comparison', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos_prob)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "        ax.legend(fontsize=12, loc='upper right')\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '09_probabilistic_metrics_comparison')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G9 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G9: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G11: TRAIN-TEST GENERALIZATION GAP\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G11] Creating Generalization Gap...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        gaps = [abs(results[m].get('train_f1', np.nan) - results[m].get('test_f1', np.nan)) for m in models]\n",
        "\n",
        "        # ✅ THESIS COLORS: Color-coded bars (blue-based)\n",
        "        colors = [\n",
        "            THESIS_COLORS['primary']['sky_blue'] if g < 0.1\n",
        "            else THESIS_COLORS['accent']['highlight'] if g < 0.2\n",
        "            else THESIS_COLORS['accent']['warning']\n",
        "            for g in np.nan_to_num(gaps)\n",
        "        ]\n",
        "\n",
        "        bars = ax.bar(x_pos, np.nan_to_num(gaps), color=colors, alpha=0.8, edgecolor='black')\n",
        "        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)\n",
        "\n",
        "        ax.set_ylabel('Absolute F1 Gap', fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Model', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G11', 'Train-Test Generalization Gap', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "\n",
        "        # Legend\n",
        "        good = mpatches.Patch(color=THESIS_COLORS['primary']['sky_blue'], label='Good (<0.1)')\n",
        "        fair = mpatches.Patch(color=THESIS_COLORS['accent']['highlight'], label='Fair (0.1-0.2)')\n",
        "        poor = mpatches.Patch(color=THESIS_COLORS['accent']['warning'], label='Poor (>0.2)')\n",
        "        ax.legend(handles=[good, fair, poor])\n",
        "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '11_generalization_gap')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G11 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G11: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G12: MODEL RANKING BY TEST F1-SCORE\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G12] Creating Model Ranking...\")\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        f1_scores = [(m, results[m].get('test_f1', 0)) for m in models]\n",
        "        f1_scores_sorted = sorted(f1_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        model_names_sorted = [m[0] for m in f1_scores_sorted]\n",
        "        scores_sorted = [m[1] for m in f1_scores_sorted]\n",
        "\n",
        "        # ✅ THESIS COLORS: Blue gradient (dark to light)\n",
        "        n_models = len(model_names_sorted)\n",
        "        colors_grad = [\n",
        "            THESIS_COLORS['primary']['dark_blue'] if i == 0\n",
        "            else THESIS_COLORS['primary']['medium_blue'] if i < n_models//3\n",
        "            else THESIS_COLORS['primary']['light_blue'] if i < 2*n_models//3\n",
        "            else THESIS_COLORS['primary']['sky_blue']\n",
        "            for i in range(n_models)\n",
        "        ]\n",
        "\n",
        "        y_pos = np.arange(len(model_names_sorted))\n",
        "        bars = ax.barh(y_pos, scores_sorted, color=colors_grad, alpha=0.8, edgecolor='black')\n",
        "\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(model_names_sorted, fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Test F1-Score', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G12', 'Model Ranking by Test F1-Score', 'RQ1'),\n",
        "            fontsize=13, fontweight='bold'\n",
        "        )\n",
        "        ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "        ax.set_xlim([0, 1.0])\n",
        "\n",
        "        # Add ranking numbers and scores\n",
        "        for i, (bar, score) in enumerate(zip(bars, scores_sorted)):\n",
        "            width = bar.get_width()\n",
        "            ax.text(0.01, bar.get_y() + bar.get_height()/2.,\n",
        "                   f'#{i+1}', ha='left', va='center', fontsize=12,\n",
        "                   fontweight='bold', color='white')\n",
        "            ax.text(width - 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "                   f'{score:.4f}', ha='right', va='center', fontsize=12,\n",
        "                   fontweight='bold', color='black')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '12_model_ranking_f1')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G12 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G12: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G13: TEST ACCURACY VS LOSS\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G13] Creating Accuracy vs Loss...\")\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # ✅ THESIS COLORS: Single blue tone with varying sizes\n",
        "        for i, model in enumerate(models):\n",
        "            loss = results[model].get('test_loss', np.nan)\n",
        "            acc = results[model].get('test_acc', np.nan)\n",
        "            if pd.notna(loss) and pd.notna(acc):\n",
        "                ax.scatter(\n",
        "                    loss, acc,\n",
        "                    s=150,\n",
        "                    alpha=0.7,\n",
        "                    label=model,\n",
        "                    color=THESIS_COLORS['primary']['medium_blue'],\n",
        "                    edgecolors='black',\n",
        "                    linewidths=1.5\n",
        "                )\n",
        "                ax.text(loss, acc, model, fontsize=12, ha='center', va='bottom')\n",
        "\n",
        "        ax.set_xlabel('Test Log Loss', fontsize=11, fontweight='bold')\n",
        "        ax.set_ylabel('Test Accuracy', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(\n",
        "            get_thesis_compliant_title('G13', 'Test Accuracy vs Loss', 'RQ1'),\n",
        "            fontsize=12, fontweight='bold'\n",
        "        )\n",
        "        ax.grid(alpha=0.4, linestyle='--')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        base_path = os.path.join(output_dir, '13_accuracy_vs_loss')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G13 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G13: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G14: PERFORMANCE METRICS HEATMAP\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if 14 not in CONFIG.get(\"graphics_to_skip\", []):\n",
        "        try:\n",
        "            print(\"  [G14] Creating Performance Metrics Heatmap...\")\n",
        "            fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "            metrics_to_plot = ['test_acc', 'test_f1', 'test_auc', 'test_kappa',\n",
        "                              'test_brier', 'test_rps', 'test_ece']\n",
        "            metrics_labels = ['Accuracy', 'F1-Score', 'AUC', 'Kappa',\n",
        "                             'Brier', 'RPS', 'ECE']\n",
        "\n",
        "            data_matrix = []\n",
        "            for model in models:\n",
        "                row = []\n",
        "                for metric in metrics_to_plot:\n",
        "                    value = results[model].get(metric, np.nan)\n",
        "                    if metric in ['test_brier', 'test_rps', 'test_ece']:\n",
        "                        value = 1 - value if pd.notna(value) else np.nan\n",
        "                    row.append(value)\n",
        "                data_matrix.append(row)\n",
        "\n",
        "            data_matrix = np.array(data_matrix)\n",
        "\n",
        "            # ✅ THESIS COLORS: Blue-based colormap\n",
        "            from matplotlib.colors import LinearSegmentedColormap\n",
        "            colors_heatmap = [\n",
        "                THESIS_COLORS['accent']['warning'],      # Low (red)\n",
        "                THESIS_COLORS['accent']['highlight'],    # Medium (orange)\n",
        "                THESIS_COLORS['primary']['light_blue'],  # Good (light blue)\n",
        "                THESIS_COLORS['primary']['medium_blue']  # Excellent (blue)\n",
        "            ]\n",
        "            n_bins = 100\n",
        "            cmap = LinearSegmentedColormap.from_list('thesis', colors_heatmap, N=n_bins)\n",
        "\n",
        "            im = ax.imshow(data_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "            ax.set_xticks(np.arange(len(metrics_labels)))\n",
        "            ax.set_yticks(np.arange(len(models)))\n",
        "            ax.set_xticklabels(metrics_labels, fontsize=12, fontweight='bold')\n",
        "            ax.set_yticklabels(models, fontsize=12)\n",
        "\n",
        "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "            for i in range(len(models)):\n",
        "                for j in range(len(metrics_labels)):\n",
        "                    if pd.notna(data_matrix[i, j]):\n",
        "                        text = ax.text(j, i, f'{data_matrix[i, j]:.3f}',\n",
        "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
        "\n",
        "            ax.set_title(\n",
        "                get_thesis_compliant_title('G14', 'Performance Metrics Heatmap (Normalized)', 'RQ1'),\n",
        "                fontsize=13, fontweight='bold', pad=20\n",
        "            )\n",
        "\n",
        "            cbar = plt.colorbar(im, ax=ax)\n",
        "            cbar.set_label('Normalized Score', rotation=270, labelpad=20, fontsize=12, fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            base_path = os.path.join(output_dir, '14_metrics_heatmap')\n",
        "            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "            plt.close(fig)\n",
        "\n",
        "            graphics.append(pdf_path)\n",
        "            print(f\"    ✓ G14 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ G14: {e}\")\n",
        "    else:\n",
        "        print(f\"    ⏸️  G14 - Skipped (CONFIG)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G15: MODEL PREDICTIONS CORRELATION\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if 15 not in CONFIG.get(\"graphics_to_skip\", []):\n",
        "        try:\n",
        "            if y_test is not None and X_test is not None:\n",
        "                print(\"  [G15] Creating Predictions Correlation...\")\n",
        "                fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "                predictions_dict = {}\n",
        "                for model_name in models:\n",
        "                    short_name = model_name.split(' ')[0].lower()\n",
        "                    if short_name in models_dict:\n",
        "                        try:\n",
        "                            y_pred = models_dict[short_name].predict(X_test)\n",
        "                            predictions_dict[model_name] = y_pred.flatten()\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                if len(predictions_dict) > 1:\n",
        "                    pred_df = pd.DataFrame(predictions_dict)\n",
        "                    corr_matrix = pred_df.corr()\n",
        "\n",
        "                    # ✅ THESIS COLORS: Blue-based colormap\n",
        "                    from matplotlib.colors import LinearSegmentedColormap\n",
        "                    colors_corr = [\n",
        "                        THESIS_COLORS['primary']['sky_blue'],    # Low correlation\n",
        "                        THESIS_COLORS['primary']['light_blue'],\n",
        "                        THESIS_COLORS['primary']['medium_blue'],\n",
        "                        THESIS_COLORS['primary']['dark_blue']    # High correlation\n",
        "                    ]\n",
        "                    cmap = LinearSegmentedColormap.from_list('thesis_corr', colors_corr, N=100)\n",
        "\n",
        "                    im = ax.imshow(corr_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "                    ax.set_xticks(np.arange(len(corr_matrix)))\n",
        "                    ax.set_yticks(np.arange(len(corr_matrix)))\n",
        "                    ax.set_xticklabels(corr_matrix.columns, fontsize=12, rotation=45, ha='right')\n",
        "                    ax.set_yticklabels(corr_matrix.index, fontsize=12)\n",
        "\n",
        "                    for i in range(len(corr_matrix)):\n",
        "                        for j in range(len(corr_matrix)):\n",
        "                            text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
        "                                         ha=\"center\", va=\"center\",\n",
        "                                         color=\"white\" if corr_matrix.iloc[i, j] > 0.5 else \"black\",\n",
        "                                         fontsize=12, fontweight='bold')\n",
        "\n",
        "                    ax.set_title(\n",
        "                        get_thesis_compliant_title(\n",
        "                            'G15',\n",
        "                            'Model Predictions Correlation\\n(High correlation = Similar predictions)',\n",
        "                            'RQ1'\n",
        "                        ),\n",
        "                        fontsize=12, fontweight='bold', pad=20\n",
        "                    )\n",
        "\n",
        "                    cbar = plt.colorbar(im, ax=ax)\n",
        "                    cbar.set_label('Correlation', rotation=270, labelpad=20, fontsize=12, fontweight='bold')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    base_path = os.path.join(output_dir, '15_predictions_correlation')\n",
        "                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                    plt.close(fig)\n",
        "\n",
        "                    graphics.append(pdf_path)\n",
        "                    print(f\"    ✓ G15 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ G15: {e}\")\n",
        "    else:\n",
        "        print(f\"    ⏸️  G15 - Skipped (CONFIG)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G16: FEATURE IMPORTANCE AGREEMENT\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        if xai_results:\n",
        "            print(\"  [G16] Creating Feature Importance Agreement...\")\n",
        "            fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "            model_top_features = {}\n",
        "            for model_short in xai_results:\n",
        "                if 'PFI' in xai_results[model_short]:\n",
        "                    importance = xai_results[model_short]['PFI']\n",
        "                    top_20_indices = np.argsort(importance)[-20:]\n",
        "                    model_top_features[model_short] = set(top_20_indices)\n",
        "\n",
        "            if len(model_top_features) > 1:\n",
        "                model_names_list = list(model_top_features.keys())\n",
        "                n_models_fi = len(model_names_list)\n",
        "                similarity_matrix = np.zeros((n_models_fi, n_models_fi))\n",
        "\n",
        "                for i in range(n_models_fi):\n",
        "                    for j in range(n_models_fi):\n",
        "                        set_i = model_top_features[model_names_list[i]]\n",
        "                        set_j = model_top_features[model_names_list[j]]\n",
        "                        jaccard = len(set_i & set_j) / len(set_i | set_j) if len(set_i | set_j) > 0 else 0\n",
        "                        similarity_matrix[i, j] = jaccard\n",
        "\n",
        "                # ✅ THESIS COLORS: Blue-based colormap\n",
        "                from matplotlib.colors import LinearSegmentedColormap\n",
        "                colors_agreement = [\n",
        "                    THESIS_COLORS['primary']['sky_blue'],\n",
        "                    THESIS_COLORS['primary']['light_blue'],\n",
        "                    THESIS_COLORS['primary']['medium_blue'],\n",
        "                    THESIS_COLORS['primary']['dark_blue']\n",
        "                ]\n",
        "                cmap = LinearSegmentedColormap.from_list('thesis_agreement', colors_agreement, N=100)\n",
        "\n",
        "                im = ax.imshow(similarity_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "                ax.set_xticks(np.arange(n_models_fi))\n",
        "                ax.set_yticks(np.arange(n_models_fi))\n",
        "                ax.set_xticklabels(model_names_list, fontsize=12, rotation=45, ha='right')\n",
        "                ax.set_yticklabels(model_names_list, fontsize=12)\n",
        "\n",
        "                for i in range(n_models_fi):\n",
        "                    for j in range(n_models_fi):\n",
        "                        text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
        "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
        "\n",
        "                ax.set_title(\n",
        "                    get_thesis_compliant_title(\n",
        "                        'G16',\n",
        "                        'Feature Importance Agreement (Top 20)\\n(High agreement = Similar feature focus)',\n",
        "                        'RQ2'\n",
        "                    ),\n",
        "                    fontsize=12, fontweight='bold', pad=20\n",
        "                )\n",
        "\n",
        "                cbar = plt.colorbar(im, ax=ax)\n",
        "                cbar.set_label('Jaccard Similarity', rotation=270, labelpad=20, fontsize=12, fontweight='bold')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                base_path = os.path.join(output_dir, '16_feature_importance_agreement')\n",
        "                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                plt.close(fig)\n",
        "\n",
        "                graphics.append(pdf_path)\n",
        "                print(f\"    ✓ G16 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G16: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G17: DISTRIBUTION OF KEY TEST METRICS (BOX PLOTS)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if 17 not in CONFIG.get(\"graphics_to_skip\", []):\n",
        "        try:\n",
        "            print(\"  [G17] Creating Metrics Distribution...\")\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            metrics_to_plot_box = [\n",
        "                ('test_acc', 'Accuracy'),\n",
        "                ('test_f1', 'F1-Score'),\n",
        "                ('test_auc', 'AUC-ROC'),\n",
        "                ('test_kappa', \"Cohen's Kappa\"),\n",
        "                ('test_brier', 'Brier Score'),\n",
        "                ('test_rps', 'RPS')\n",
        "            ]\n",
        "\n",
        "            # ✅ THESIS COLORS: Each box plot uses different blue shade\n",
        "            box_colors = [\n",
        "                THESIS_COLORS['primary']['dark_blue'],\n",
        "                THESIS_COLORS['primary']['medium_blue'],\n",
        "                THESIS_COLORS['primary']['light_blue'],\n",
        "                THESIS_COLORS['primary']['sky_blue'],\n",
        "                THESIS_COLORS['primary']['medium_blue'],\n",
        "                THESIS_COLORS['primary']['light_blue']\n",
        "            ]\n",
        "\n",
        "            for idx, ((metric_key, metric_label), color) in enumerate(zip(metrics_to_plot_box, box_colors)):\n",
        "                ax = axes[idx]\n",
        "\n",
        "                metric_values = [results[m].get(metric_key, np.nan) for m in models]\n",
        "                metric_values = [v for v in metric_values if pd.notna(v)]\n",
        "\n",
        "                if metric_values:\n",
        "                    bp = ax.boxplot([metric_values], vert=True, patch_artist=True,\n",
        "                                   widths=0.5, showmeans=True, meanline=True)\n",
        "\n",
        "                    for patch in bp['boxes']:\n",
        "                        patch.set_facecolor(color)\n",
        "                        patch.set_alpha(0.7)\n",
        "\n",
        "                    y_points = metric_values\n",
        "                    x_points = np.random.normal(1, 0.04, size=len(y_points))\n",
        "                    ax.scatter(x_points, y_points, alpha=0.6, s=80,\n",
        "                              color=THESIS_COLORS['neutral']['dark_gray'],\n",
        "                              edgecolors='white', zorder=3)\n",
        "\n",
        "                    ax.set_ylabel(metric_label, fontsize=11, fontweight='bold')\n",
        "                    ax.set_title(f'{metric_label} Distribution', fontsize=11, fontweight='bold')\n",
        "                    ax.set_xticks([1])\n",
        "                    ax.set_xticklabels(['All Models'])\n",
        "                    ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "\n",
        "                    mean_val = np.mean(metric_values)\n",
        "                    median_val = np.median(metric_values)\n",
        "                    std_val = np.std(metric_values)\n",
        "                    stats_text = f'Mean: {mean_val:.4f}\\nMedian: {median_val:.4f}\\nStd: {std_val:.4f}'\n",
        "                    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes,\n",
        "                           fontsize=12, verticalalignment='top', horizontalalignment='right',\n",
        "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
        "                    ax.axis('off')\n",
        "\n",
        "            plt.suptitle(\n",
        "                get_thesis_compliant_title('G17', 'Distribution of Key Test Metrics', 'RQ1'),\n",
        "                fontsize=14, fontweight='bold'\n",
        "            )\n",
        "            plt.tight_layout()\n",
        "            base_path = os.path.join(output_dir, '17_metrics_distribution_boxplots')\n",
        "            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "            plt.close(fig)\n",
        "\n",
        "            graphics.append(pdf_path)\n",
        "            print(f\"    ✓ G17 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ G17: {e}\")\n",
        "    else:\n",
        "        print(f\"    ⏸️  G17 - Skipped (CONFIG)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G18a: BEST MODEL FEATURE IMPORTANCE (TOP 20) - 6 METHODS\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if \"18a\" not in CONFIG.get(\"graphics_to_skip\", []):\n",
        "        try:\n",
        "            print(\"  [G18a] Creating Best Model Feature Importance (6 methods)...\")\n",
        "            valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "            if valid_models_f1:\n",
        "                best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "                best_model_short = best_model.split(' ')[0].lower()\n",
        "\n",
        "                if best_model_short in xai_results:\n",
        "                    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "                    axes = axes.flatten()\n",
        "\n",
        "                    xai_methods = ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']\n",
        "\n",
        "                    # ✅ THESIS COLORS: Blue gradient for each method\n",
        "                    colors_xai_thesis = [\n",
        "                        THESIS_COLORS['primary']['dark_blue'],\n",
        "                        THESIS_COLORS['primary']['medium_blue'],\n",
        "                        THESIS_COLORS['primary']['light_blue'],\n",
        "                        THESIS_COLORS['primary']['sky_blue'],\n",
        "                        THESIS_COLORS['primary']['medium_blue'],\n",
        "                        THESIS_COLORS['primary']['light_blue']\n",
        "                    ]\n",
        "\n",
        "                    for idx, (method, color) in enumerate(zip(xai_methods, colors_xai_thesis)):\n",
        "                        ax = axes[idx]\n",
        "                        if method in xai_results[best_model_short]:\n",
        "                            importance = xai_results[best_model_short][method]\n",
        "\n",
        "                            top_indices = np.argsort(importance)[-20:][::-1]\n",
        "                            top_features = [features[i] for i in top_indices]\n",
        "                            top_scores = importance[top_indices]\n",
        "\n",
        "                            y_pos = np.arange(len(top_features))\n",
        "                            bars = ax.barh(y_pos, top_scores, color=color, alpha=0.8, edgecolor='black')\n",
        "\n",
        "                            ax.set_yticks(y_pos)\n",
        "                            ax.set_yticklabels(top_features, fontsize=12)\n",
        "                            ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
        "                            ax.set_title(f'{method}', fontsize=11, fontweight='bold')\n",
        "                            ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "                            for bar in bars:\n",
        "                                width = bar.get_width()\n",
        "                                ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "                                       f'{width:.4f}', ha='left', va='center', fontsize=12)\n",
        "                        else:\n",
        "                            ax.text(0.5, 0.5, 'No Data', ha='center', va='center',\n",
        "                                   transform=ax.transAxes, fontsize=12)\n",
        "                            ax.axis('off')\n",
        "\n",
        "                    plt.suptitle(\n",
        "                        get_thesis_compliant_title(\n",
        "                            'G18a',\n",
        "                            f'Best Model ({best_model}) - Feature Importance by Method',\n",
        "                            'RQ2'\n",
        "                        ),\n",
        "                        fontsize=14, fontweight='bold'\n",
        "                    )\n",
        "                    plt.tight_layout()\n",
        "                    base_path = os.path.join(output_dir, '18a_best_model_feature_importance')\n",
        "                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                    plt.close(fig)\n",
        "\n",
        "                    graphics.append(pdf_path)\n",
        "                    print(f\"    ✓ G18a saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ✗ G18a: {e}\")\n",
        "    else:\n",
        "        print(f\"    ⏸️  G18a - Skipped (CONFIG)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G18b: BEST MODEL - AGGREGATED TOP 10 FEATURES\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G18b] Creating Aggregated Top 10 Features...\")\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            best_model_short = best_model.split(' ')[0].lower()\n",
        "\n",
        "            if best_model_short in xai_results:\n",
        "                all_importance = []\n",
        "                n_features_expected = len(features)\n",
        "\n",
        "                for method in ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']:\n",
        "                    if method in xai_results[best_model_short]:\n",
        "                        importance = xai_results[best_model_short][method]\n",
        "\n",
        "                        if importance is None:\n",
        "                            continue\n",
        "\n",
        "                        importance = np.array(importance).flatten()\n",
        "\n",
        "                        if len(importance) != n_features_expected:\n",
        "                            continue\n",
        "\n",
        "                        if np.any(np.isnan(importance)) or np.any(np.isinf(importance)):\n",
        "                            importance = np.nan_to_num(importance, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                        if importance.sum() == 0:\n",
        "                            importance = np.ones(n_features_expected) / n_features_expected\n",
        "\n",
        "                        all_importance.append(importance)\n",
        "\n",
        "                if len(all_importance) >= 2:\n",
        "                    importance_stack = np.vstack(all_importance)\n",
        "                    mean_importance = np.mean(importance_stack, axis=0)\n",
        "\n",
        "                    top_10_indices = np.argsort(mean_importance)[-10:][::-1]\n",
        "                    top_10_features = [features[i] for i in top_10_indices]\n",
        "                    top_10_scores = mean_importance[top_10_indices]\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "                    y_pos = np.arange(len(top_10_features))\n",
        "\n",
        "                    # ✅ THESIS COLORS: Single blue tone\n",
        "                    bars = ax.barh(\n",
        "                        y_pos, top_10_scores,\n",
        "                        color=THESIS_COLORS['primary']['medium_blue'],\n",
        "                        alpha=0.8, edgecolor='black'\n",
        "                    )\n",
        "\n",
        "                    ax.set_yticks(y_pos)\n",
        "                    ax.set_yticklabels(top_10_features, fontsize=11, fontweight='bold')\n",
        "                    ax.set_xlabel('Aggregated Importance Score', fontsize=12, fontweight='bold')\n",
        "                    ax.set_title(\n",
        "                        get_thesis_compliant_title(\n",
        "                            'G18b',\n",
        "                            f'Best Model ({best_model}) - Top 10 Features (Averaged)',\n",
        "                            'RQ2'\n",
        "                        ),\n",
        "                        fontsize=13, fontweight='bold'\n",
        "                    )\n",
        "                    ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "\n",
        "                    for bar in bars:\n",
        "                        width = bar.get_width()\n",
        "                        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "                               f'{width:.4f}', ha='left', va='center',\n",
        "                               fontsize=12, fontweight='bold')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    base_path = os.path.join(output_dir, '18b_best_model_top10_aggregated')\n",
        "                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                    plt.close(fig)\n",
        "\n",
        "                    graphics.append(pdf_path)\n",
        "                    print(f\"    ✓ G18b saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G18b: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G18c: BEST MODEL - SHAP SUMMARY PLOT\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        if HAS_SHAP:\n",
        "            print(\"  [G18c] Creating SHAP Summary Plot...\")\n",
        "            valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "            if valid_models_f1:\n",
        "                best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "                best_model_short = best_model.split(' ')[0].lower()\n",
        "\n",
        "                if best_model_short in models_dict and best_model_short not in ['svm', 'lr', 'ada', 'ltcn']:\n",
        "                    model = models_dict[best_model_short]\n",
        "\n",
        "                    X_shap = X_test.sample(min(100, X_test.shape[0]), random_state=SEED)\n",
        "\n",
        "                    try:\n",
        "                        def predict_func(x_data):\n",
        "                            if hasattr(model, 'predict_proba'):\n",
        "                                return model.predict_proba(x_data)\n",
        "                            else:\n",
        "                                return model.predict(x_data)\n",
        "\n",
        "                        background = X_test.sample(min(50, X_test.shape[0]), random_state=SEED)\n",
        "                        explainer = shap.KernelExplainer(predict_func, background)\n",
        "                        shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "                    except:\n",
        "                        try:\n",
        "                            explainer = shap.TreeExplainer(\n",
        "                                model,\n",
        "                                feature_perturbation='tree_path_dependent'\n",
        "                            )\n",
        "                            shap_values = explainer.shap_values(X_shap)\n",
        "                        except:\n",
        "                            raise Exception(\"Both SHAP explainers failed\")\n",
        "\n",
        "                    if isinstance(shap_values, list):\n",
        "                        shap_values_plot = shap_values[1]\n",
        "                        class_label = \"Draw Class\"\n",
        "                    else:\n",
        "                        shap_values_plot = shap_values\n",
        "                        class_label = \"\"\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "                    # ✅ SHAP uses its own colormap, but we can't override it easily\n",
        "                    # Just ensure high-quality output\n",
        "                    shap.summary_plot(\n",
        "                        shap_values_plot,\n",
        "                        X_shap,\n",
        "                        feature_names=features,\n",
        "                        show=False,\n",
        "                        max_display=20,\n",
        "                        plot_type='dot'\n",
        "                    )\n",
        "\n",
        "                    title_text = get_thesis_compliant_title(\n",
        "                        'G18c',\n",
        "                        f'Best Model ({best_model}) - SHAP Summary Plot',\n",
        "                        'RQ2'\n",
        "                    )\n",
        "                    if class_label:\n",
        "                        title_text += f'\\n({class_label})'\n",
        "\n",
        "                    plt.title(title_text, fontsize=13, fontweight='bold', pad=20)\n",
        "                    plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=11, fontweight='bold')\n",
        "                    plt.ylabel('Features', fontsize=11, fontweight='bold')\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    base_path = os.path.join(output_dir, '18c_best_model_shap_summary')\n",
        "                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                    plt.close()\n",
        "\n",
        "                    graphics.append(pdf_path)\n",
        "                    print(f\"    ✓ G18c saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G18c: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G20a: CLASSIFICATION METRICS RADAR (Threshold-Dependent)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G20a] Creating Classification Metrics Radar...\")\n",
        "\n",
        "        valid_models_f1 = {\n",
        "            m: results[m].get('test_f1', -1)\n",
        "            for m in models\n",
        "            if pd.notna(results[m].get('test_f1'))\n",
        "        }\n",
        "\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            metrics = results[best_model]\n",
        "\n",
        "            # Font setup\n",
        "            try:\n",
        "                import matplotlib\n",
        "                matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            categories = ['Accuracy', 'F1-Score', 'Precision\\n(Macro)', 'Recall\\n(Macro)', 'Kappa\\n(Scaled)']\n",
        "\n",
        "            per_class = metrics.get('per_class_metrics', {})\n",
        "            if per_class and 'precision' in per_class and 'recall' in per_class:\n",
        "                macro_precision = np.mean(list(per_class['precision'].values()))\n",
        "                macro_recall = np.mean(list(per_class['recall'].values()))\n",
        "            else:\n",
        "                macro_precision = metrics.get('test_f1', 0.5) * 0.95\n",
        "                macro_recall = metrics.get('test_f1', 0.5) * 1.05\n",
        "\n",
        "            kappa_raw = metrics.get('test_kappa', 0)\n",
        "            kappa_normalized = (kappa_raw + 1) / 2\n",
        "\n",
        "            values = [\n",
        "                metrics.get('test_acc', 0),\n",
        "                metrics.get('test_f1', 0),\n",
        "                macro_precision,\n",
        "                macro_recall,\n",
        "                kappa_normalized\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "                values_plot = values + values[:1]\n",
        "                angles_plot = angles + angles[:1]\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "                # ✅ THESIS COLORS: Blue for main plot\n",
        "                ax.plot(angles_plot, values_plot, 'o-', linewidth=3,\n",
        "                       color=THESIS_COLORS['primary']['dark_blue'],\n",
        "                       label=best_model, markersize=8, zorder=3)\n",
        "                ax.fill(angles_plot, values_plot, alpha=0.25,\n",
        "                       color=THESIS_COLORS['primary']['dark_blue'], zorder=2)\n",
        "\n",
        "                ax.set_xticks(angles)\n",
        "                ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
        "                ax.set_ylim(0, 1)\n",
        "                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)\n",
        "                ax.grid(True, linestyle='--', linewidth=1.5, alpha=0.6, zorder=1)\n",
        "\n",
        "                # Baseline\n",
        "                baseline_values = [0.6] * len(angles_plot)\n",
        "                ax.plot(angles_plot, baseline_values, '--', linewidth=2,\n",
        "                       color=THESIS_COLORS['accent']['highlight'],\n",
        "                       alpha=0.5, label='Baseline (0.6)', zorder=1)\n",
        "\n",
        "                for angle, value, cat in zip(angles, values, categories):\n",
        "                    text_radius = value + 0.08\n",
        "                    ax.text(angle, text_radius, f'{value:.3f}',\n",
        "                           ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
        "                                    edgecolor='black', alpha=0.9), zorder=4)\n",
        "\n",
        "                plt.title(\n",
        "                    get_thesis_compliant_title(\n",
        "                        'G20a',\n",
        "                        f'Classification Performance ({best_model})\\nThreshold-Dependent Metrics',\n",
        "                        'RQ1'\n",
        "                    ),\n",
        "                    fontsize=12, fontweight='bold', pad=25\n",
        "                )\n",
        "                plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1),\n",
        "                          fontsize=12, framealpha=0.9)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                base_path = os.path.join(output_dir, '20a_classification_radar')\n",
        "                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                graphics.append(pdf_path)\n",
        "                print(f\"    ✓ G20a saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "            except Exception as e_radar:\n",
        "                print(f\"    ⚠️  Radar plot failed, using bar chart fallback\")\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(12, 8))\n",
        "                y_pos = np.arange(len(categories))\n",
        "\n",
        "                bars = ax.barh(y_pos, values,\n",
        "                              color=THESIS_COLORS['primary']['medium_blue'],\n",
        "                              alpha=0.8, edgecolor='black')\n",
        "\n",
        "                ax.set_yticks(y_pos)\n",
        "                ax.set_yticklabels(categories, fontsize=11, fontweight='bold')\n",
        "                ax.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
        "                ax.set_title(\n",
        "                    get_thesis_compliant_title(\n",
        "                        'G20a',\n",
        "                        f'Classification Performance ({best_model})',\n",
        "                        'RQ1'\n",
        "                    ),\n",
        "                    fontsize=14, fontweight='bold', pad=20\n",
        "                )\n",
        "                ax.set_xlim(0, 1)\n",
        "                ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "                ax.invert_yaxis()\n",
        "\n",
        "                for bar, value in zip(bars, values):\n",
        "                    width = bar.get_width()\n",
        "                    ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,\n",
        "                           f'{value:.3f}', ha='left', va='center',\n",
        "                           fontsize=10, fontweight='bold')\n",
        "\n",
        "                ax.axvline(x=0.6,\n",
        "                          color=THESIS_COLORS['accent']['highlight'],\n",
        "                          linestyle='--', linewidth=2,\n",
        "                          alpha=0.5, label='Baseline (0.6)')\n",
        "                ax.legend(fontsize=11)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                base_path = os.path.join(output_dir, '20a_classification_radar')\n",
        "                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                graphics.append(pdf_path)\n",
        "                print(f\"    ✓ G20a saved (BAR FALLBACK, PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e_g20a:\n",
        "        print(f\"    ❌ G20a: {e_g20a}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G20b: PROBABILISTIC METRICS RADAR (Threshold-Independent)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G20b] Creating Probabilistic Metrics Radar...\")\n",
        "\n",
        "        valid_models_f1 = {\n",
        "            m: results[m].get('test_f1', -1)\n",
        "            for m in models\n",
        "            if pd.notna(results[m].get('test_f1'))\n",
        "        }\n",
        "\n",
        "        if valid_models_f1:\n",
        "            best_model = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            metrics = results[best_model]\n",
        "\n",
        "            try:\n",
        "                import matplotlib\n",
        "                matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            categories = ['AUC-ROC', '1-Brier\\nScore', '1-RPS', '1-ECE', '1-Log Loss']\n",
        "\n",
        "            log_loss_raw = metrics.get('test_loss', 1.0)\n",
        "            log_loss_normalized = max(0, min(1, 1 - (log_loss_raw / 2)))\n",
        "\n",
        "            values = [\n",
        "                metrics.get('test_auc', 0),\n",
        "                max(0, 1 - metrics.get('test_brier', 0)),\n",
        "                max(0, 1 - metrics.get('test_rps', 0)),\n",
        "                max(0, 1 - metrics.get('test_ece', 0)),\n",
        "                log_loss_normalized\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "                values_plot = values + values[:1]\n",
        "                angles_plot = angles + angles[:1]\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "                # ✅ THESIS COLORS: Blue for probabilistic metrics\n",
        "                ax.plot(angles_plot, values_plot, 'o-', linewidth=3,\n",
        "                       color=THESIS_COLORS['primary']['dark_blue'],\n",
        "                       label=best_model, markersize=8, zorder=3)\n",
        "                ax.fill(angles_plot, values_plot, alpha=0.25,\n",
        "                       color=THESIS_COLORS['primary']['dark_blue'], zorder=2)\n",
        "\n",
        "                ax.set_xticks(angles)\n",
        "                ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
        "                ax.set_ylim(0, 1)\n",
        "                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)\n",
        "                ax.grid(True, linestyle='--', linewidth=1.5, alpha=0.6, zorder=1)\n",
        "\n",
        "                baseline_values = [0.7] * len(angles_plot)\n",
        "                ax.plot(angles_plot, baseline_values, '--', linewidth=2,\n",
        "                       color=THESIS_COLORS['accent']['highlight'],\n",
        "                       alpha=0.5, label='Good (0.7)', zorder=1)\n",
        "\n",
        "                for angle, value, cat in zip(angles, values, categories):\n",
        "                    text_radius = value + 0.08\n",
        "                    ax.text(angle, text_radius, f'{value:.3f}',\n",
        "                           ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
        "                                    edgecolor='black', alpha=0.9), zorder=4)\n",
        "\n",
        "                plt.title(\n",
        "                    get_thesis_compliant_title(\n",
        "                        'G20b',\n",
        "                        f'Probabilistic Performance ({best_model})\\nThreshold-Independent Metrics',\n",
        "                        'RQ1'\n",
        "                    ),\n",
        "                    fontsize=14, fontweight='bold', pad=25\n",
        "                )\n",
        "                plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1),\n",
        "                          fontsize=11, framealpha=0.9)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                base_path = os.path.join(output_dir, '20b_probabilistic_radar')\n",
        "                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                graphics.append(pdf_path)\n",
        "                print(f\"    ✓ G20b saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "            except Exception as e_radar:\n",
        "                print(f\"    ⚠️  Radar plot failed, using bar chart fallback\")\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(12, 8))\n",
        "                y_pos = np.arange(len(categories))\n",
        "\n",
        "                bars = ax.barh(y_pos, values,\n",
        "                              color=THESIS_COLORS['primary']['medium_blue'],\n",
        "                              alpha=0.8, edgecolor='black')\n",
        "\n",
        "                ax.set_yticks(y_pos)\n",
        "                ax.set_yticklabels(categories, fontsize=11, fontweight='bold')\n",
        "                ax.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
        "                ax.set_title(\n",
        "                    get_thesis_compliant_title(\n",
        "                        'G20b',\n",
        "                        f'Probabilistic Performance ({best_model})',\n",
        "                        'RQ1'\n",
        "                    ),\n",
        "                    fontsize=14, fontweight='bold', pad=20\n",
        "                )\n",
        "                ax.set_xlim(0, 1)\n",
        "                ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
        "                ax.invert_yaxis()\n",
        "\n",
        "                for bar, value in zip(bars, values):\n",
        "                    width = bar.get_width()\n",
        "                    ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,\n",
        "                           f'{value:.3f}', ha='left', va='center',\n",
        "                           fontsize=12, fontweight='bold')\n",
        "\n",
        "                ax.axvline(x=0.7,\n",
        "                          color=THESIS_COLORS['accent']['highlight'],\n",
        "                          linestyle='--', linewidth=2,\n",
        "                          alpha=0.5, label='Good (0.7)')\n",
        "                ax.legend(fontsize=11)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                base_path = os.path.join(output_dir, '20b_probabilistic_radar')\n",
        "                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                graphics.append(pdf_path)\n",
        "                print(f\"    ✓ G20b saved (BAR FALLBACK, PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e_g20b:\n",
        "        print(f\"    ❌ G20b: {e_g20b}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G24: EXECUTIVE SUMMARY\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G24] Creating Executive Summary...\")\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "        gs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.3)\n",
        "\n",
        "        ax1 = fig.add_subplot(gs[0, :])\n",
        "        ax1.axis('off')\n",
        "\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}\n",
        "        avg_test_f1 = np.nanmean([results[m].get('test_f1', np.nan) for m in models])\n",
        "        best_f1_score = max(valid_models_f1.values()) if valid_models_f1 else np.nan\n",
        "        best_model = max(valid_models_f1, key=valid_models_f1.get) if valid_models_f1 else 'N/A'\n",
        "\n",
        "        summary_text = f\"\"\"\n",
        "        --- FOOTBALL PREDICTION SYSTEM v18.0 ---\n",
        "        Models Trained: {len(models)}\n",
        "        Best Model: {best_model} (F1 = {best_f1_score:.4f})\n",
        "        Average Test F1: {avg_test_f1:.4f}\n",
        "        Output Directory: {os.path.basename(output_dir)}\n",
        "        \"\"\"\n",
        "        ax1.text(0.5, 0.5, summary_text.strip(), ha='center', va='center', fontsize=11,\n",
        "                 family='monospace', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        metrics_to_plot = [\n",
        "            ('Test F1', 'test_f1', THESIS_COLORS['primary']['medium_blue']),\n",
        "            ('Test Accuracy', 'test_acc', THESIS_COLORS['primary']['light_blue']),\n",
        "            ('Test AUC', 'test_auc', THESIS_COLORS['primary']['sky_blue'])\n",
        "        ]\n",
        "\n",
        "        for i, (label, key, color) in enumerate(metrics_to_plot):\n",
        "            ax_sub = fig.add_subplot(gs[1, i])\n",
        "            scores = [results[m].get(key, np.nan) for m in models]\n",
        "            bars = ax_sub.bar(models, np.nan_to_num(scores), color=color, alpha=0.8, edgecolor='black')\n",
        "            add_numeric_values_to_bars(ax_sub, bars, format_str='.3f', fontsize=12, color='black')\n",
        "            ax_sub.set_title(label, fontsize=12, fontweight='bold')\n",
        "            ax_sub.set_ylim([0, 1])\n",
        "            ax_sub.tick_params(axis='x', rotation=90, labelsize=8)\n",
        "            ax_sub.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.suptitle(\n",
        "            get_thesis_compliant_title('G24', 'Executive Summary', None),\n",
        "            fontsize=14, fontweight='bold', y=0.99\n",
        "        )\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "        base_path = os.path.join(output_dir, '24_executive_summary')\n",
        "        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        graphics.append(pdf_path)\n",
        "        print(f\"    ✓ G24 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G24: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G25: CONFUSION MATRICES (Per Model)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G25] Creating Confusion Matrices...\")\n",
        "        for idx, model_name in enumerate(models):\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            short_model_name = model_name.split(' ')[0].lower()\n",
        "\n",
        "            if short_model_name in models_dict:\n",
        "                model = models_dict[short_model_name]\n",
        "                if y_test is not None and X_test is not None:\n",
        "                    try:\n",
        "                        y_pred = model.predict(X_test)\n",
        "                        cm = confusion_matrix(y_test, y_pred)\n",
        "                    except:\n",
        "                        cm = np.zeros((3, 3))\n",
        "                else:\n",
        "                    cm = np.zeros((3, 3))\n",
        "            else:\n",
        "                cm = np.zeros((3, 3))\n",
        "\n",
        "            # ✅ THESIS COLORS: Blue colormap\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                        xticklabels=['Away Win', 'Draw', 'Home Win'],\n",
        "                        yticklabels=['Away Win', 'Draw', 'Home Win'],\n",
        "                        annot_kws={\"size\": 14},\n",
        "                        cbar_kws={'label': 'Count'})\n",
        "\n",
        "            ax.set_title(\n",
        "                get_thesis_compliant_title('G25', f'{model_name}', 'RQ1'),\n",
        "                fontsize=14, fontweight='bold'\n",
        "            )\n",
        "            ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "            ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            safe_model_name = model_name.replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')', '')\n",
        "            base_path = os.path.join(output_dir, f'25_{idx+1:02d}_{safe_model_name}_cm')\n",
        "            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "            plt.close(fig)\n",
        "\n",
        "            graphics.append(pdf_path)\n",
        "\n",
        "        print(f\"    ✓ G25 - {len(models)} Confusion Matrices saved (PDF format)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ✗ G25: {e}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G28: XAI METHOD COMPARISON (EXCEPTION - Keep Original Colors)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G28] Creating XAI Method Comparison...\")\n",
        "        print(\"    ⚠️  Exception: Using original multi-color palette\")\n",
        "\n",
        "        g28_expected_path = os.path.join(output_dir, '28_xai_ablation_comparison_v2.png')\n",
        "        g28_already_exists = os.path.exists(g28_expected_path)\n",
        "\n",
        "        if g28_already_exists:\n",
        "            print(f\"    ℹ️  G28 already exists\")\n",
        "\n",
        "            # Convert existing PNG to PDF\n",
        "            g28_pdf_path = g28_expected_path.replace('.png', '.pdf')\n",
        "            if not os.path.exists(g28_pdf_path):\n",
        "                try:\n",
        "                    img = plt.imread(g28_expected_path)\n",
        "                    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "                    ax.imshow(img)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    pdf_metadata = {\n",
        "                        'Title': 'Figure G28: XAI Ablation Comparison',\n",
        "                        'Author': 'Dokumus - Tilburg University',\n",
        "                        'Subject': 'RQ2: Explainability Analysis',\n",
        "                        'Keywords': 'XAI, Ablation, Feature Importance',\n",
        "                        'Creator': 'Matplotlib',\n",
        "                        'Producer': 'Thesis Research'\n",
        "                    }\n",
        "\n",
        "                    plt.savefig(g28_pdf_path, format='pdf', dpi=300,\n",
        "                               bbox_inches='tight', metadata=pdf_metadata)\n",
        "                    plt.close()\n",
        "\n",
        "                    print(f\"    ✓ Converted PNG to PDF\")\n",
        "                except Exception as e_convert:\n",
        "                    print(f\"    ⚠️  Could not convert to PDF: {e_convert}\")\n",
        "                    g28_pdf_path = g28_expected_path\n",
        "\n",
        "            if graphics is not None and g28_pdf_path not in graphics:\n",
        "                graphics.append(g28_pdf_path)\n",
        "                print(f\"    ✓ G28 added to graphics list\")\n",
        "\n",
        "        elif xai_results and X_train is not None and y_train is not None:\n",
        "            try:\n",
        "                print(f\"    🔄 Creating G28...\")\n",
        "\n",
        "                comparison_path = create_feature_importance_comparison_v19_ABLATION(\n",
        "                    models_dict=models_dict,\n",
        "                    xai_results=xai_results,\n",
        "                    X_train=X_train,\n",
        "                    X_test=X_test,\n",
        "                    y_train=y_train,\n",
        "                    y_test=y_test,\n",
        "                    X_val=X_val,\n",
        "                    y_val=y_val,\n",
        "                    features=features,\n",
        "                    output_dir=output_dir,\n",
        "                    results_dict=results\n",
        "                )\n",
        "\n",
        "                if comparison_path and os.path.exists(comparison_path):\n",
        "                    # Convert to PDF if PNG\n",
        "                    if comparison_path.endswith('.png'):\n",
        "                        pdf_path = comparison_path.replace('.png', '.pdf')\n",
        "                        try:\n",
        "                            img = plt.imread(comparison_path)\n",
        "                            fig, ax = plt.subplots(figsize=(12, 10))\n",
        "                            ax.imshow(img)\n",
        "                            ax.axis('off')\n",
        "\n",
        "                            pdf_metadata = {\n",
        "                                'Title': 'Figure G28: XAI Ablation Comparison',\n",
        "                                'Author': 'Dokumus - Tilburg University',\n",
        "                                'Subject': 'RQ2: Explainability Analysis',\n",
        "                                'Keywords': 'XAI, Ablation, Feature Importance',\n",
        "                                'Creator': 'Matplotlib',\n",
        "                                'Producer': 'Thesis Research'\n",
        "                            }\n",
        "\n",
        "                            plt.savefig(pdf_path, format='pdf', dpi=300,\n",
        "                                       bbox_inches='tight', metadata=pdf_metadata)\n",
        "                            plt.close()\n",
        "\n",
        "                            comparison_path = pdf_path\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    if graphics is not None:\n",
        "                        if comparison_path not in graphics:\n",
        "                            graphics.append(comparison_path)\n",
        "                            print(f\"    ✓ G28 created and added (PDF: {os.path.getsize(comparison_path)/1024:.1f} KB)\")\n",
        "\n",
        "            except Exception as e_g28:\n",
        "                print(f\"    ❌ G28 creation error: {str(e_g28)[:100]}\")\n",
        "\n",
        "    except Exception as e_outer:\n",
        "        print(f\"    ❌ G28 outer exception: {e_outer}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G29: CUMULATIVE IMPORTANCE COMPARISON\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if 29 not in CONFIG.get(\"graphics_to_skip\", []):\n",
        "        try:\n",
        "            print(\"  [G29] Creating Cumulative Importance Comparison...\")\n",
        "            if xai_results and X_train is not None:\n",
        "                g29_path = create_cumulative_importance_comparison(\n",
        "                    xai_results=xai_results,\n",
        "                    features=features,\n",
        "                    output_dir=output_dir,\n",
        "                    results_dict=results\n",
        "                )\n",
        "                if g29_path and os.path.exists(g29_path):\n",
        "                    # Convert to PDF if PNG\n",
        "                    if g29_path.endswith('.png'):\n",
        "                        pdf_path = g29_path.replace('.png', '.pdf')\n",
        "                        try:\n",
        "                            img = plt.imread(g29_path)\n",
        "                            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "                            ax.imshow(img)\n",
        "                            ax.axis('off')\n",
        "\n",
        "                            pdf_metadata = {\n",
        "                                'Title': 'Figure G29: Cumulative Importance Comparison',\n",
        "                                'Author': 'Dokumus - Tilburg University',\n",
        "                                'Subject': 'RQ2: Explainability Analysis',\n",
        "                                'Keywords': 'XAI, Cumulative Importance, Feature Ranking',\n",
        "                                'Creator': 'Matplotlib',\n",
        "                                'Producer': 'Thesis Research'\n",
        "                            }\n",
        "\n",
        "                            plt.savefig(pdf_path, format='pdf', dpi=300,\n",
        "                                       bbox_inches='tight', metadata=pdf_metadata)\n",
        "                            plt.close()\n",
        "\n",
        "                            g29_path = pdf_path\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    if graphics is not None:\n",
        "                        graphics.append(g29_path)\n",
        "                    print(f\"    ✓ G29 saved (PDF: {os.path.getsize(g29_path)/1024:.1f} KB)\")\n",
        "\n",
        "        except Exception as e_g29:\n",
        "            print(f\"    ❌ G29: {e_g29}\")\n",
        "    else:\n",
        "        print(f\"    ⏸️  G29 - Skipped (CONFIG)\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # G31: PER-CLASS ACCURACY HEATMAP\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        print(\"  [G31] Creating Per-Class Accuracy Heatmap...\")\n",
        "\n",
        "        class_names = ['Away Win', 'Draw', 'Home Win']\n",
        "        data_matrix = []\n",
        "        model_labels = []\n",
        "\n",
        "        for model_key in models:\n",
        "            metrics = results.get(model_key, {})\n",
        "\n",
        "            if 'per_class_metrics' in metrics and 'accuracy' in metrics['per_class_metrics']:\n",
        "                row = []\n",
        "                for class_name in class_names:\n",
        "                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)\n",
        "                    row.append(acc)\n",
        "\n",
        "                data_matrix.append(row)\n",
        "                model_labels.append(model_key)\n",
        "\n",
        "        if len(data_matrix) > 0:\n",
        "            data_matrix = np.array(data_matrix)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 12))\n",
        "\n",
        "            # ✅ THESIS COLORS: Blue-based colormap\n",
        "            from matplotlib.colors import LinearSegmentedColormap\n",
        "            colors_per_class = [\n",
        "                THESIS_COLORS['accent']['warning'],      # Low (red)\n",
        "                THESIS_COLORS['accent']['highlight'],    # Medium (orange)\n",
        "                THESIS_COLORS['primary']['light_blue'],  # Good (light blue)\n",
        "                THESIS_COLORS['primary']['medium_blue'], # Very good (medium blue)\n",
        "                THESIS_COLORS['primary']['dark_blue']    # Excellent (dark blue)\n",
        "            ]\n",
        "            cmap = LinearSegmentedColormap.from_list('thesis_per_class', colors_per_class, N=100)\n",
        "\n",
        "            im = ax.imshow(data_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "            ax.set_xticks(np.arange(len(class_names)))\n",
        "            ax.set_yticks(np.arange(len(model_labels)))\n",
        "            ax.set_xticklabels(class_names, fontsize=11, fontweight='bold')\n",
        "            ax.set_yticklabels(model_labels, fontsize=12)\n",
        "\n",
        "            plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
        "\n",
        "            for i in range(len(model_labels)):\n",
        "                for j in range(len(class_names)):\n",
        "                    acc = data_matrix[i, j]\n",
        "                    text_color = \"white\" if acc < 0.5 else \"black\"\n",
        "\n",
        "                    text = ax.text(j, i,\n",
        "                                 f'{acc:.3f}\\n({acc*100:.1f}%)',\n",
        "                                 ha=\"center\", va=\"center\",\n",
        "                                 color=text_color,\n",
        "                                 fontsize=12,\n",
        "                                 fontweight='bold')\n",
        "\n",
        "            ax.set_title(\n",
        "                get_thesis_compliant_title(\n",
        "                    'G31',\n",
        "                    'Per-Class Accuracy by Model\\nPrediction Performance per Match Outcome',\n",
        "                    'RQ1'\n",
        "                ),\n",
        "                fontsize=13, fontweight='bold', pad=20\n",
        "            )\n",
        "\n",
        "            ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
        "            ax.set_xlabel('Match Outcome', fontsize=12, fontweight='bold')\n",
        "\n",
        "            cbar = plt.colorbar(im, ax=ax)\n",
        "            cbar.set_label('Accuracy', rotation=270, labelpad=20,\n",
        "                          fontsize=11, fontweight='bold')\n",
        "\n",
        "            ax.set_xticks(np.arange(len(class_names)+1)-.5, minor=True)\n",
        "            ax.set_yticks(np.arange(len(model_labels)+1)-.5, minor=True)\n",
        "            ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=2)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            base_path = os.path.join(output_dir, '31_per_class_accuracy_heatmap')\n",
        "            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            graphics.append(pdf_path)\n",
        "            print(f\"    ✓ G31 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)\")\n",
        "\n",
        "    except Exception as e_g31:\n",
        "        print(f\"    ❌ G31: {e_g31}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # FINAL SUMMARY\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ GRAPHICS CREATION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"  📊 Total graphics created: {len(graphics)}\")\n",
        "    print(f\"  📐 Format: Dual (PNG + PDF)\")\n",
        "    print(f\"  🎨 Color scheme: Thesis-compliant blue tones\")\n",
        "    print(f\"  ⚠️  Exception: G28 (multi-color preserved)\")\n",
        "    print(f\"  📁 Output directory: {output_dir}\")\n",
        "\n",
        "    # File size summary\n",
        "    total_pdf_size = sum(os.path.getsize(p)/1024 for p in graphics if os.path.exists(p))\n",
        "    print(f\"  💾 Total PDF size: {total_pdf_size:.1f} KB ({total_pdf_size/1024:.1f} MB)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return graphics\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# END OF create_dynamic_graphics() FUNCTION\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def create_cumulative_importance_comparison(xai_results, features, output_dir, results_dict):\n",
        "    \"\"\"\n",
        "    G29: Cumulative Feature Importance Comparison\n",
        "    Paper-style comparison of XAI methods - BEST MODEL FOCUSED\n",
        "\n",
        "    Args:\n",
        "        xai_results: XAI analysis results dictionary\n",
        "        features: List of feature names\n",
        "        output_dir: Output directory for saving\n",
        "        results_dict: Model performance results (for finding best model)\n",
        "\n",
        "    Returns:\n",
        "        str: Path to saved graphic or None\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n[G29] Creating Cumulative Importance Comparison...\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # STEP 1: EN İYİ MODELİ BUL (F1-Score'a göre)\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if not xai_results:\n",
        "        print(\"  ⚠️  No XAI results available\")\n",
        "        return None\n",
        "\n",
        "    # Test F1 skorlarına göre en iyi modeli seç\n",
        "    valid_models_f1 = {\n",
        "        m: results_dict[m].get('test_f1', -1)\n",
        "        for m in results_dict.keys()\n",
        "        if pd.notna(results_dict[m].get('test_f1'))\n",
        "    }\n",
        "\n",
        "    if not valid_models_f1:\n",
        "        print(\"  ⚠️  No valid F1 scores found\")\n",
        "        return None\n",
        "\n",
        "    best_model_key = max(valid_models_f1, key=valid_models_f1.get)\n",
        "    best_model_short = best_model_key.split(' ')[0].lower()  # \"XGBOOST (Optuna)\" → \"xgboost\"\n",
        "    best_f1_score = valid_models_f1[best_model_key]\n",
        "\n",
        "    print(f\"  🏆 Best Model: {best_model_key} (F1: {best_f1_score:.4f})\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # STEP 2: EN İYİ MODELİN XAI SONUÇLARINI AL\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    if best_model_short not in xai_results:\n",
        "        print(f\"  ⚠️  No XAI results for {best_model_key}\")\n",
        "        print(f\"  Available models: {list(xai_results.keys())}\")\n",
        "        return None\n",
        "\n",
        "    xai_data = xai_results[best_model_short]\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # STEP 3: MEVCUT METOTLARI KONTROL ET\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    methods_to_plot = ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']\n",
        "    available_methods = []\n",
        "\n",
        "    for method in methods_to_plot:\n",
        "        if method in xai_data and xai_data[method] is not None:\n",
        "            if len(xai_data[method]) > 0:\n",
        "                available_methods.append(method)\n",
        "\n",
        "    if len(available_methods) < 2:\n",
        "        print(f\"  ⚠️  Not enough methods available ({len(available_methods)} found)\")\n",
        "        return None\n",
        "\n",
        "    print(f\"  📊 Methods available: {', '.join(available_methods)}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ YENİ EKLEME: MAX FEATURES TRUNCATION CONFIG\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    xai_config = CONFIG.get(\"xai\", {})\n",
        "    MAX_FEATURES = xai_config.get(\"ablation_max_features\", 30)\n",
        "\n",
        "    print(f\"  📊 Max features for cumulative plot: {MAX_FEATURES}\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # STEP 4: GRAFİK OLUŞTUR\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    try:\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # Renk ve marker ayarları\n",
        "        colors = {\n",
        "            'PFI': '#9b59b6',      # Mor\n",
        "            'PMI': '#1abc9c',      # Turkuaz\n",
        "            'SOFI': '#e74c3c',     # Kırmızı\n",
        "            'SHAP': '#f39c12',     # Turuncu\n",
        "            'LTCN': '#3498db',     # Mavi\n",
        "            'XGBoost': '#2ecc71',  # Yeşil\n",
        "        }\n",
        "        markers = {\n",
        "            'PFI': 'o',\n",
        "            'PMI': 's',\n",
        "            'SOFI': 'D',\n",
        "            'SHAP': '^',\n",
        "            'LTCN': 'v',\n",
        "            'XGBoost': 'p',\n",
        "        }\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 5: HER METOD İÇİN CUMULATIVE IMPORTANCE HESAPLA\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        plotted_methods = []\n",
        "\n",
        "        for method in available_methods:\n",
        "            try:\n",
        "                importance = xai_data[method]\n",
        "\n",
        "                # Feature'ları önem sırasına göre sırala\n",
        "                sorted_indices = np.argsort(importance)[::-1]\n",
        "\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                # ✅ TRUNCATE TO MAX_FEATURES\n",
        "                # ════════════════════════════════════════════════════════════\n",
        "                if MAX_FEATURES and MAX_FEATURES < len(sorted_indices):\n",
        "                    print(f\"      {method}: Using top {MAX_FEATURES} features (was {len(sorted_indices)})\")\n",
        "                    sorted_indices = sorted_indices[:MAX_FEATURES]\n",
        "\n",
        "                sorted_importance = importance[sorted_indices]\n",
        "\n",
        "                # Normalize et (0-1 arası)\n",
        "                if sorted_importance.sum() > 0:\n",
        "                    sorted_importance = sorted_importance / sorted_importance.sum()\n",
        "                else:\n",
        "                    print(f\"  ⚠️  {method}: All zeros, skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Cumulative sum hesapla\n",
        "                cumulative = np.cumsum(sorted_importance)\n",
        "\n",
        "                # Plot\n",
        "                x = np.arange(1, len(cumulative) + 1)\n",
        "                line = ax.plot(\n",
        "                    x,\n",
        "                    cumulative,\n",
        "                    label=method,\n",
        "                    color=colors.get(method, 'gray'),\n",
        "                    marker=markers.get(method, 'o'),\n",
        "                    markersize=7,\n",
        "                    linewidth=2.5,\n",
        "                    markevery=max(1, len(x)//10),  # Her 10 noktada marker\n",
        "                    alpha=0.85\n",
        "                )\n",
        "\n",
        "                plotted_methods.append(method)\n",
        "\n",
        "            except Exception as e_method:\n",
        "                print(f\"  ⚠️  {method} error: {e_method}\")\n",
        "                continue\n",
        "\n",
        "        if len(plotted_methods) < 2:\n",
        "            print(f\"  ❌ Not enough methods plotted ({len(plotted_methods)})\")\n",
        "            plt.close()\n",
        "            return None\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 6: RANDOM BASELINE EKLE\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        n_features = MAX_FEATURES if MAX_FEATURES else len(features)\n",
        "        random_cumulative = np.cumsum(np.ones(n_features) / n_features)\n",
        "        ax.plot(\n",
        "            np.arange(1, n_features + 1),\n",
        "            random_cumulative,\n",
        "            label='Random Baseline',\n",
        "            color='black',\n",
        "            linestyle='--',\n",
        "            linewidth=2.5,\n",
        "            alpha=0.6,\n",
        "            zorder=1\n",
        "        )\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 7: STYLING VE ANNOTATIONS\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # Axes labels\n",
        "        ax.set_xlabel('πᵢ (Feature Rank)', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('g(π) (Cumulative Importance)', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # Title\n",
        "        title_text = f'Graphic 29: Feature Importance Comparison\\n'\n",
        "        title_text += f'Cumulative Distribution by XAI Method ({best_model_key})'\n",
        "        ax.set_title(title_text, fontsize=15, fontweight='bold', pad=20)\n",
        "\n",
        "        # Legend\n",
        "        ax.legend(\n",
        "            loc='lower right',\n",
        "            fontsize=11,\n",
        "            framealpha=0.95,\n",
        "            edgecolor='black',\n",
        "            fancybox=True,\n",
        "            shadow=True\n",
        "        )\n",
        "\n",
        "        # Grid\n",
        "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
        "\n",
        "        # Limits\n",
        "        ax.set_xlim(0, n_features + 1)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 8: THRESHOLD ÇİZGİLERİ EKLE\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # 80% threshold\n",
        "        ax.axhline(y=0.8, color='red', linestyle=':', linewidth=1.8, alpha=0.6, zorder=2)\n",
        "        ax.text(\n",
        "            n_features * 0.75, 0.82, '80% Importance',\n",
        "            fontsize=12, color='red', alpha=0.8, fontweight='bold',\n",
        "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
        "        )\n",
        "\n",
        "        # 90% threshold (opsiyonel)\n",
        "        ax.axhline(y=0.9, color='orange', linestyle=':', linewidth=1.5, alpha=0.5, zorder=2)\n",
        "        ax.text(\n",
        "            n_features * 0.75, 0.92, '90%',\n",
        "            fontsize=12, color='orange', alpha=0.7\n",
        "        )\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 9: MODEL BİLGİSİ KUTUSU EKLE\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        info_text = f'Model: {best_model_key}\\n'\n",
        "        info_text += f'Test F1: {best_f1_score:.4f}\\n'\n",
        "        info_text += f'Features: {n_features}\\n'\n",
        "        info_text += f'Methods: {len(plotted_methods)}'\n",
        "\n",
        "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='black')\n",
        "        ax.text(\n",
        "            0.02, 0.98, info_text,\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=12,\n",
        "            verticalalignment='top',\n",
        "            bbox=props,\n",
        "            family='monospace'\n",
        "        )\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        # STEP 10: KAYDET VE KAPAT\n",
        "        # ════════════════════════════════════════════════════════════════════\n",
        "        plt.tight_layout()\n",
        "\n",
        "        path = os.path.join(output_dir, '29_cumulative_importance_comparison.png')\n",
        "        plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        plt.close(fig)\n",
        "\n",
        "        print(f\"  ✅ G29 saved: {os.path.basename(path)}\")\n",
        "        print(f\"  📊 Plotted methods: {', '.join(plotted_methods)}\\n\")\n",
        "\n",
        "        return path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ G29 creation error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        plt.close('all')\n",
        "        return None\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "# ✅ YENİ FONKSİYON: SAVE TABLES AS PNG (SATIR ~4600 CİVARI EKLE)\n",
        "# ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def save_tables_as_png(tables, output_dir):\n",
        "    \"\"\"\n",
        "    Save tables as PNG images for PDF inclusion\n",
        "\n",
        "    Args:\n",
        "        tables (dict): Dictionary of {table_name: DataFrame}\n",
        "        output_dir (str): Directory to save PNG files\n",
        "\n",
        "    Returns:\n",
        "        list: List of saved PNG file paths\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    print(\"\\n[SAVE] Saving tables as PNG...\\n\")\n",
        "\n",
        "    saved_paths = []\n",
        "\n",
        "    for table_name, df in tables.items():\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            print(f\"  ⚠️  Skipping {table_name} (not a DataFrame)\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Figure oluştur\n",
        "            fig, ax = plt.subplots(figsize=(12, max(6, len(df)*0.4)))\n",
        "            ax.axis('tight')\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Tabloyu çiz\n",
        "            table = ax.table(\n",
        "                cellText=df.values,\n",
        "                colLabels=df.columns,\n",
        "                cellLoc='center',\n",
        "                loc='center',\n",
        "                colWidths=[0.15]*len(df.columns)\n",
        "            )\n",
        "\n",
        "            table.auto_set_font_size(False)\n",
        "            table.set_fontsize(12)\n",
        "            table.scale(1, 1.5)\n",
        "\n",
        "            # Header stilini ayarla\n",
        "            for (i, j), cell in table.get_celld().items():\n",
        "                if i == 0:  # Header row\n",
        "                    cell.set_facecolor('#3498db')\n",
        "                    cell.set_text_props(weight='bold', color='white')\n",
        "                else:\n",
        "                    cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n",
        "\n",
        "            plt.title(table_name.replace('_', ' '),\n",
        "                     fontsize=12, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Kaydet\n",
        "            png_path = os.path.join(output_dir, f\"{table_name}.png\")\n",
        "            plt.savefig(png_path, dpi=300, bbox_inches='tight',\n",
        "                       facecolor='white', edgecolor='none')\n",
        "            plt.close()\n",
        "\n",
        "            saved_paths.append(png_path)\n",
        "            print(f\"  ✓ {table_name} -> PNG\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ {table_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    print(f\"\\n✅ Tables saved to: {output_dir}\")\n",
        "    print(f\"   Total saved: {len(saved_paths)} PNG files\\n\")\n",
        "\n",
        "    return saved_paths\n",
        "\n",
        "# SAVE TABLES\n",
        "def save_tables_as_csv_and_excel(tables, output_dir):\n",
        "    \"\"\"Save tables as CSV and Excel for high-resolution use\"\"\"\n",
        "    print(\"\\n[SAVE] Saving tables as CSV and Excel...\\n\")\n",
        "\n",
        "    # Tek bir Excel dosyası için bir writer oluştur\n",
        "    excel_path = os.path.join(output_dir, \"All_Tables_v18.xlsx\")\n",
        "    try:\n",
        "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "            for table_name, df in tables.items():\n",
        "                if not isinstance(df, pd.DataFrame):\n",
        "                    print(f\"  ⚠️  Skipping {table_name} (not a DataFrame)\")\n",
        "                    continue\n",
        "\n",
        "                # 1. Excel'e kaydet (her tabloyu ayrı bir sekmeye)\n",
        "                try:\n",
        "                    # Excel sekme adları 31 karakteri geçemez\n",
        "                    safe_sheet_name = table_name[:31]\n",
        "                    df.to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
        "                    print(f\"  ✓ {table_name} -> Excel sheet '{safe_sheet_name}'\")\n",
        "                except Exception as e_excel:\n",
        "                    print(f\"  ✗ {table_name} (Excel): {e_excel}\")\n",
        "                # 2. Ayrı CSV dosyaları olarak kaydet\n",
        "                try:\n",
        "                    csv_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
        "                    df.to_csv(csv_path, index=False, encoding='utf-8-sig') # utf-8-sig Türkçe karakterler için\n",
        "                    print(f\"  ✓ {table_name} -> CSV file\")\n",
        "                except Exception as e_csv:\n",
        "                    print(f\"  ✗ {table_name} (CSV): {e_csv}\")\n",
        "\n",
        "        print(f\"\\n✅ All tables saved to: {excel_path}\")\n",
        "\n",
        "    except Exception as e_writer:\n",
        "        # Excel writer hata verirse (kütüphane eksikse vb.) sadece CSV kaydet\n",
        "        print(f\"❌ Excel Writer Error: {e_writer}. Saving as separate CSVs only.\")\n",
        "        for table_name, df in tables.items():\n",
        "            if isinstance(df, pd.DataFrame):\n",
        "                try:\n",
        "                    csv_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
        "                    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "                    print(f\"  ✓ {table_name} -> CSV file\")\n",
        "                except Exception as e_csv:\n",
        "                    print(f\"  ✗ {table_name} (CSV): {e_csv}\")\n",
        "# SAVE RESULTS FUNCTION\n",
        "def save_results_safely(results, output_dir):\n",
        "    \"\"\"Save results to JSON\"\"\"\n",
        "    try:\n",
        "        json_path = os.path.join(output_dir, \"results.json\")\n",
        "\n",
        "        # Convert numpy types to Python types\n",
        "        safe_results = {}\n",
        "        for model_key, metrics in results.items():\n",
        "            if isinstance(metrics, dict):\n",
        "                safe_results[model_key] = {}\n",
        "                for metric_name, metric_value in metrics.items():\n",
        "                    # Per-class metrics'i özel olarak işle\n",
        "                    if metric_name == 'per_class_metrics' and isinstance(metric_value, dict):\n",
        "                        safe_results[model_key][metric_name] = {}\n",
        "                        for class_metric, class_data in metric_value.items():\n",
        "                            if isinstance(class_data, dict):\n",
        "                                safe_results[model_key][metric_name][class_metric] = {\n",
        "                                    k: int(v) if isinstance(v, (np.integer, int)) else float(v)\n",
        "                                    for k, v in class_data.items()\n",
        "                                }\n",
        "                            else:\n",
        "                                safe_results[model_key][metric_name][class_metric] = class_data\n",
        "                    # Normal metrikleri işle\n",
        "                    elif isinstance(metric_value, (np.floating, np.integer)):\n",
        "                        safe_results[model_key][metric_name] = float(metric_value)\n",
        "                    elif isinstance(metric_value, (float, int, str)):\n",
        "                        safe_results[model_key][metric_name] = metric_value\n",
        "                    elif pd.isna(metric_value):\n",
        "                        safe_results[model_key][metric_name] = None\n",
        "                    else:\n",
        "                        safe_results[model_key][metric_name] = str(metric_value)\n",
        "            else:\n",
        "                safe_results[model_key] = str(metrics)\n",
        "\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(safe_results, f, indent=2)\n",
        "\n",
        "        print(f\"✅ Results saved: {json_path}\")\n",
        "        return json_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not save results: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# CREATE PDF\n",
        "# Satır 2827 civarı - TAMAMEN DEĞİŞTİR\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# 📄 CREATE PDF REPORT - TEXT SUMMARY ONLY (VEKTÖREL AKIŞ İÇİN DÜZENLENMİŞ)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# ✅ Bu versiyon SADECE başlık/özet sayfası oluşturur\n",
        "# ✅ Grafik ve tabloların Overleaf'e manuel yüklenmesi beklenir\n",
        "# ✅ Vektörel kalite korunur (rasterize olmaz)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def create_pdf_report(output_dir, graphics, tables):\n",
        "    \"\"\"\n",
        "    Sadece metin özeti ve kapak sayfası içeren basit bir PDF oluşturur.\n",
        "    Grafik ve tabloların Overleaf'te manuel olarak eklenmesi beklenir (Vektörel akış).\n",
        "    Bu, vektörel kalitenin korunmasını garantiler.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        PDF'in kaydedileceği dizin\n",
        "    graphics : list\n",
        "        Grafik dosyalarının listesi (kullanılmaz ama uyumluluk için tutulur)\n",
        "    tables : list\n",
        "        Tablo dosyalarının listesi (kullanılmaz ama uyumluluk için tutulur)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str or None\n",
        "        PDF dosyasının tam yolu (başarılı ise), None (hata varsa)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n8️⃣ CREATING PDF (Text Summary Only)...\\n\")\n",
        "\n",
        "    # ✅ DÜZELTME 1: Çıktı dosyasını Metin Özeti olarak adlandır\n",
        "    pdf_path = os.path.join(output_dir, \"Football_Prediction_v18_TEXT_SUMMARY.pdf\")\n",
        "\n",
        "    try:\n",
        "        # ⚠️ Yalnızca Matplotlib tarafından oluşturulan içeriği topluyoruz\n",
        "        with PdfPages(pdf_path) as pdf:\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # TITLE PAGE (Başlık sayfası içeriği) - KORUNUR\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "            fig = plt.figure(figsize=(8.5, 11))  # US Letter size\n",
        "            ax = fig.add_subplot(111)\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Ana başlık\n",
        "            ax.text(\n",
        "                0.5, 0.85,\n",
        "                'Football Match Prediction System',\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=24,\n",
        "                fontweight='bold',\n",
        "                transform=ax.transAxes\n",
        "            )\n",
        "\n",
        "            # Alt başlık\n",
        "            ax.text(\n",
        "                0.5, 0.75,\n",
        "                'v18.0 - ODDS-PLUS WITH EDA PIPELINE',\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=18,\n",
        "                transform=ax.transAxes\n",
        "            )\n",
        "\n",
        "            # Açıklama\n",
        "            ax.text(\n",
        "                0.5, 0.65,\n",
        "                'EDA Integration + Best Model Focus',\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=12,\n",
        "                style='italic',\n",
        "                transform=ax.transAxes\n",
        "            )\n",
        "\n",
        "            # Tarih\n",
        "            ax.text(\n",
        "                0.5, 0.45,\n",
        "                f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=12,\n",
        "                transform=ax.transAxes\n",
        "            )\n",
        "\n",
        "            # Özellikler listesi\n",
        "            features_text = \"\"\"✅ Advanced Fuzzy Matching\n",
        "✅ Data Integration (Matches + ELO + Transfermarkt)\n",
        "✅ EDA Pipeline Integration\n",
        "✅ 8 ML Models (Including LTCN)\n",
        "✅ 6 XAI Methods (PFI, PMI, SOFI, SHAP, LTCN, XGBoost)\n",
        "✅ Confusion Matrices\n",
        "✅ Professional Graphics (VECTOR FORMAT)\n",
        "✅ Comprehensive Ablation Analysis\"\"\"\n",
        "\n",
        "            ax.text(\n",
        "                0.5, 0.20,\n",
        "                features_text,\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=12,\n",
        "                transform=ax.transAxes,\n",
        "                family='monospace'\n",
        "            )\n",
        "\n",
        "            # Uyarı metni\n",
        "            ax.text(\n",
        "                0.5, 0.05,\n",
        "                '⚠️ Graphics and tables available as separate vector PDF files',\n",
        "                ha='center',\n",
        "                va='top',\n",
        "                fontsize=12,\n",
        "                style='italic',\n",
        "                color='red',\n",
        "                transform=ax.transAxes\n",
        "            )\n",
        "\n",
        "            # Sayfayı kaydet\n",
        "            pdf.savefig(fig, bbox_inches='tight')\n",
        "            plt.close(fig)\n",
        "\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # ✅ DÜZELTME 2: GRAFİK VE TABLO EKLEME MANTIKLARI KALDIRILDI!\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "            # Grafik ve tabloların ayrı PDF dosyaları olarak Overleaf'e\n",
        "            # yüklenmesi beklenir. Bu sayede vektörel kalite korunur.\n",
        "            # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "        # BAŞARI RAPORU\n",
        "        # ════════════════════════════════════════════════════════════════\n",
        "\n",
        "        file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB cinsinden\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"✅ PDF TEXT SUMMARY CREATED SUCCESSFULLY\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"   📄 Path: {pdf_path}\")\n",
        "        print(f\"   💾 Size: {file_size:.2f} MB\")\n",
        "        print(f\"   📊 Pages: 1 (Title page only)\")\n",
        "        print(f\"   🎨 Format: Vector (Matplotlib)\")\n",
        "        print(f\"\\n   📦 ADDITIONAL FILES (for Overleaf):\")\n",
        "        print(f\"      • {len(graphics)} graphics (separate vector PDFs)\")\n",
        "        print(f\"      • {len(tables)} tables (separate PNG files)\")\n",
        "        print(f\"\\n   ⚠️  NEXT STEP:\")\n",
        "        print(f\"      Upload all vector PDF graphics to Overleaf for final document\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return pdf_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"❌ PDF CREATION ERROR\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"   Error: {e}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Detaylı hata ayıklama bilgisi\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return None\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ ENHANCED: Graphics list validation + auto-recovery\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "    if graphics is None:\n",
        "        print(\"  ⚠️  Warning: Graphics list is None - attempting recovery...\")\n",
        "        graphics = []\n",
        "\n",
        "        # ✅ RECOVERY: Scan graphics directory\n",
        "        graphics_dir = os.path.join(output_dir, 'graphics')\n",
        "        if os.path.exists(graphics_dir):\n",
        "            found_files = sorted([\n",
        "                os.path.join(graphics_dir, f)\n",
        "                for f in os.listdir(graphics_dir)\n",
        "                if f.endswith('.png') and not f.startswith('.')\n",
        "            ])\n",
        "\n",
        "            if found_files:\n",
        "                graphics = found_files\n",
        "                print(f\"  ✅ Recovered {len(graphics)} graphics from directory\")\n",
        "\n",
        "                # Debug: Show what we found\n",
        "                print(f\"\\n  📋 Graphics to be added to PDF:\")\n",
        "                for idx, path in enumerate(graphics, 1):\n",
        "                    filename = os.path.basename(path)\n",
        "                    size_kb = os.path.getsize(path) / 1024\n",
        "                    print(f\"     {idx:2d}. {filename:50s} ({size_kb:6.1f} KB)\")\n",
        "                print()\n",
        "            else:\n",
        "                print(f\"  ❌ No graphics found in {graphics_dir}\")\n",
        "        else:\n",
        "            print(f\"  ❌ Graphics directory not found: {graphics_dir}\")\n",
        "\n",
        "    if tables is None:\n",
        "        tables = {}\n",
        "        print(\"  ⚠️  Warning: Tables dictionary is None, using empty dict\")\n",
        "\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    # ✅ VALIDATION: Check if G28 is in the list\n",
        "    # ════════════════════════════════════════════════════════════════════════\n",
        "    g28_files = [g for g in graphics if '28_xai' in os.path.basename(g)]\n",
        "\n",
        "    if g28_files:\n",
        "        print(f\"  ✅ G28 found in graphics list: {len(g28_files)} file(s)\")\n",
        "        for g28 in g28_files:\n",
        "            print(f\"     • {os.path.basename(g28)}\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  WARNING: G28 not in graphics list!\")\n",
        "        print(f\"     PDF will be missing XAI comparison graphic\")\n",
        "\n",
        "        # ✅ LAST-CHANCE RECOVERY: Check if file exists but not in list\n",
        "        graphics_dir = os.path.join(output_dir, 'graphics')\n",
        "        g28_pattern = os.path.join(graphics_dir, '28_xai*.png')\n",
        "\n",
        "        import glob\n",
        "        g28_existing = glob.glob(g28_pattern)\n",
        "\n",
        "        if g28_existing:\n",
        "            print(f\"  🔄 G28 exists but not in list - adding now...\")\n",
        "            for g28 in g28_existing:\n",
        "                if g28 not in graphics:\n",
        "                    graphics.append(g28)\n",
        "                    print(f\"     ✅ Added: {os.path.basename(g28)}\")\n",
        "        else:\n",
        "            print(f\"     ❌ G28 file not found in directory either\")\n",
        "\n",
        "    print()  # Blank line\n",
        "\n",
        "    pdf_path = os.path.join(output_dir, \"Football_Prediction_v18_COMPLETE.pdf\")\n",
        "    try:\n",
        "        with PdfPages(pdf_path) as pdf:\n",
        "            # Title page\n",
        "            fig = plt.figure(figsize=(8.5, 11))\n",
        "            ax = fig.add_subplot(111)\n",
        "            ax.axis('off')\n",
        "\n",
        "            ax.text(0.5, 0.85, 'Football Match Prediction System',\n",
        "                    ha='center', va='top', fontsize=24, fontweight='bold',\n",
        "                    transform=ax.transAxes)\n",
        "            ax.text(0.5, 0.75, 'v18.0 - ODDS-PLUS WITH EDA PIPELINE',\n",
        "                    ha='center', va='top', fontsize=18,\n",
        "                    transform=ax.transAxes)\n",
        "            ax.text(0.5, 0.65, 'EDA Integration + Best Model Focus',\n",
        "                    ha='center', va='top', fontsize=12, style='italic',\n",
        "                    transform=ax.transAxes)\n",
        "\n",
        "            ax.text(0.5, 0.45, f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
        "                    ha='center', va='top', fontsize=12,\n",
        "                    transform=ax.transAxes)\n",
        "\n",
        "            features_text = \"\"\"✅ Advanced Fuzzy Matching\n",
        "✅ Data Integration (Matches + ELO + Transfermarkt)\n",
        "✅ EDA Pipeline\n",
        "✅ 8 ML Models (Including LTCN)\n",
        "✅ XAI Analysis\n",
        "✅ Confusion Matrices\n",
        "✅ Professional Graphics\"\"\"\n",
        "\n",
        "            ax.text(0.5, 0.20, features_text,\n",
        "                    ha='center', va='top', fontsize=12,\n",
        "                    transform=ax.transAxes, family='monospace')\n",
        "\n",
        "            pdf.savefig(fig, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # ============================================================================\n",
        "            # FIX 2: Graphics ekleme - güvenli kontrol\n",
        "            # ============================================================================\n",
        "            print(\"  Adding graphics...\")\n",
        "            if graphics and len(graphics) > 0:  # ← Eklenen kontrol\n",
        "                for graphic_path in graphics:\n",
        "                    if os.path.exists(graphic_path):\n",
        "                        try:\n",
        "                            img = plt.imread(graphic_path)\n",
        "                            fig, ax = plt.subplots(figsize=(11, 8.5))\n",
        "                            ax.imshow(img)\n",
        "                            ax.axis('off')\n",
        "                            pdf.savefig(fig, bbox_inches='tight', orientation='landscape')\n",
        "                            plt.close()\n",
        "                        except Exception as e_img:\n",
        "                            print(f\"    ⚠️  Could not add graphic: {os.path.basename(graphic_path)}\")\n",
        "            else:\n",
        "                print(\"    ⚠️  No graphics to add\")\n",
        "\n",
        "            # ============================================================================\n",
        "            # FIX 3: Tables ekleme - güvenli kontrol\n",
        "            # ============================================================================\n",
        "            print(\"  Adding tables...\")\n",
        "            if os.path.exists(TABLES_DIR):  # ← Eklenen kontrol\n",
        "                table_files = sorted([f for f in os.listdir(TABLES_DIR) if f.endswith('.png')])\n",
        "                if table_files:\n",
        "                    for table_file in table_files:\n",
        "                        table_path = os.path.join(TABLES_DIR, table_file)\n",
        "                        try:\n",
        "                            img = plt.imread(table_path)\n",
        "                            fig, ax = plt.subplots(figsize=(11, 8.5))\n",
        "                            ax.imshow(img)\n",
        "                            ax.axis('off')\n",
        "                            pdf.savefig(fig, bbox_inches='tight', orientation='landscape')\n",
        "                            plt.close()\n",
        "                        except Exception as e_table:\n",
        "                            print(f\"    ⚠️  Could not add table: {table_file}\")\n",
        "                else:\n",
        "                    print(\"    ⚠️  No table files found\")\n",
        "            else:\n",
        "                print(f\"    ⚠️  Tables directory not found: {TABLES_DIR}\")\n",
        "\n",
        "        print(f\"✅ PDF: {pdf_path}\\n\")\n",
        "        return pdf_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PDF error: {e}\\n\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"🚀 FOOTBALL MATCH PREDICTION SYSTEM v18.0 (Single Scenario Mode)\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 1: VERİ YÜKLEME VE BİRLEŞTİRME\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n1️⃣ DATA INTEGRATION...\\n\")\n",
        "        data_integration = DataIntegration()\n",
        "        df = data_integration.merge_data()\n",
        "        print(f\"✅ Data merged: {len(df):,} matches × {len(df.columns)} features\\n\")\n",
        "\n",
        "        if len(df) == 0:\n",
        "            print(\"❌ FATAL ERROR: No data after merge!\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 1.5: LAG FEATURES (GECİKMELİ ÖZELLİKLER) OLUŞTURMA\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n1️⃣.5️⃣ LAG FEATURES CREATION (Leakage-Free)...\\n\")\n",
        "\n",
        "        # Hangi in-game features'ları lag'e çevirelim?\n",
        "        IN_GAME_FEATURES = [\n",
        "            'HomeTarget',    # İsabetli şut\n",
        "            'AwayTarget',\n",
        "            'HomeShot',      # Toplam şut\n",
        "            'AwayShot',\n",
        "            'HomeShots', # <-- EKLENDİ\n",
        "            'AwayShots',\n",
        "            'HomeCorners',   # Korner\n",
        "            'AwayCorners',\n",
        "            'HomeFouls',     # Faul (opsiyonel)\n",
        "            'AwayFouls',\n",
        "            'HomeYellow',\n",
        "            'AwayYellow',\n",
        "            'HomeRed',\n",
        "            'AwayRed',\n",
        "        ]\n",
        "\n",
        "        # Mevcut olanları filtrele\n",
        "        features_to_lag = [f for f in IN_GAME_FEATURES if f in df.columns]\n",
        "\n",
        "        if features_to_lag:\n",
        "            print(f\"  🔄 Converting {len(features_to_lag)} in-game features to lag features:\")\n",
        "            for feat in features_to_lag:\n",
        "                print(f\"     • {feat}\")\n",
        "            print()\n",
        "\n",
        "            # Lag features oluştur\n",
        "            df, lag_feature_names = create_lag_features(\n",
        "                df=df,\n",
        "                feature_cols=features_to_lag,\n",
        "                windows=[3, 5],        # Son 3 ve son 5 maç\n",
        "                home_away_split=True     # Home/away ayrımı yap\n",
        "            )\n",
        "\n",
        "            # ═══════════════════════════════════════════════════════════════\n",
        "            # ORIGINAL IN-GAME FEATURES'I ŞİMDİ KALDIR!\n",
        "            # ═══════════════════════════════════════════════════════════════\n",
        "            print(f\"  🗑️  Removing original in-game features (now replaced with lags)...\")\n",
        "\n",
        "            features_removed = []\n",
        "            for feat in features_to_lag:\n",
        "                if feat in df.columns:\n",
        "                    df = df.drop(columns=[feat])\n",
        "                    features_removed.append(feat)\n",
        "\n",
        "            print(f\"     ✅ Removed {len(features_removed)} original features\")\n",
        "            print(f\"     ✅ Added {len(lag_feature_names)} lag features\")\n",
        "            print(f\"     📊 Net change: +{len(lag_feature_names) - len(features_removed)} features\\n\")\n",
        "\n",
        "            # ✅ YENİ: Kartların kaldırıldığını özellikle vurgula\n",
        "            card_features_removed = [f for f in features_removed if 'Red' in f or 'Yellow' in f]\n",
        "            if card_features_removed:\n",
        "                print(f\"     🔒 LEAKAGE PREVENTED: {len(card_features_removed)} card features removed:\")\n",
        "                for feat in card_features_removed:\n",
        "                    print(f\"        • {feat} → Replaced with lag features\")\n",
        "                print()\n",
        "\n",
        "            # Lag features listesini kaydet\n",
        "            lag_features_path = os.path.join(DATA_DIR, \"lag_features_created.txt\")\n",
        "            with open(lag_features_path, 'w') as f:\n",
        "                 f.write(\"LAG FEATURES CREATED\\n\")\n",
        "                 f.write(\"=\"*80 + \"\\n\\n\")\n",
        "                 f.write(f\"Original features converted: {len(features_to_lag)}\\n\")\n",
        "                 f.write(f\"Lag features created: {len(lag_feature_names)}\\n\")\n",
        "                 f.write(f\"Windows used: [3, 5]\\n\")\n",
        "                 f.write(f\"Home/Away split: True\\n\\n\")\n",
        "                 f.write(\"List of lag features:\\n\")\n",
        "                 f.write(\"-\"*80 + \"\\n\")\n",
        "                 for i, feat in enumerate(lag_feature_names, 1):\n",
        "                     f.write(f\"{i:3d}. {feat}\\n\")\n",
        "            print(f\"     📄 Lag features list saved: {os.path.basename(lag_features_path)}\\n\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  ⚠️  No in-game features found in dataset\")\n",
        "            print(f\"  ℹ️  Dataset may already be clean or features have different names\\n\")\n",
        "\n",
        "        print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 2: PREPARE FEATURES (İLK HAZIRLIK)\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n2️⃣ FEATURE PREPARATION...\\n\")\n",
        "        n_select = CONFIG['feature_toggles'].get('n_features_to_select', None)\n",
        "        use_selection = CONFIG['feature_toggles'].get('use_feature_selection', False)\n",
        "        if not use_selection:\n",
        "            n_select = None\n",
        "\n",
        "        X, y, features = prepare_features(df, n_features=n_select)\n",
        "\n",
        "        if X.empty:\n",
        "            print(\"❌ FATAL ERROR: Empty dataset!\")\n",
        "            sys.exit(1)\n",
        "        print(f\"✅ Features: {X.shape[0]} × {X.shape[1]}\\n\")\n",
        "\n",
        "        print(\"\\n📊 SCENARIO: ODDS-PLUS\\n\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 3: TRAIN/TEST SPLIT\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n3️⃣ TRAIN/TEST SPLIT...\\n\")\n",
        "        try:\n",
        "            CUTOFF_DATE = pd.to_datetime('2022-08-31')\n",
        "            train_indices = df.index[df['MatchDate'] < CUTOFF_DATE]\n",
        "            test_indices = df.index[df['MatchDate'] >= CUTOFF_DATE]\n",
        "\n",
        "            # y_train ve y_test'i burada ayırıyoruz\n",
        "            y_train = y.loc[train_indices] # SMOTE için orijinal y_train\n",
        "            y_test = y.loc[test_indices]\n",
        "\n",
        "            X_train_raw = X.loc[train_indices]\n",
        "            X_test_raw = X.loc[test_indices]\n",
        "\n",
        "            print(f\"  Train: {len(X_train_raw):,} samples\")\n",
        "            print(f\"  Test:  {len(X_test_raw):,} samples\")\n",
        "\n",
        "            if X_train_raw.empty or X_test_raw.empty:\n",
        "                raise ValueError(\"Empty train/test set!\")\n",
        "\n",
        "        except Exception as e_split:\n",
        "            print(f\"❌ Split error: {e_split}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 3.5: EDA-GUIDED FEATURE PREPARATION (SENKRONİZASYON DÜZELTİLDİ)\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n3️⃣.5️⃣ EDA-GUIDED FEATURE PREPARATION...\\n\")\n",
        "\n",
        "        # ============================================================================\n",
        "        # 🚨 EXPERIMENT: \"KRAL ÇIPLAK\" (REMOVE ODDS & DERIVED METRICS)\n",
        "        # Bahis oranlarını ve türevlerini tamamen kaldırarak modeli \"Saf Futbol\" öğrenmeye zorluyoruz.\n",
        "        # ============================================================================\n",
        "        FEATURES_TO_REMOVE_TOTALLY = [\n",
        "            # --- TEMEL BAHİS ORANLARI (Bet365) ---\n",
        "            'OddHome', 'OddDraw', 'OddAway',\n",
        "\n",
        "            # --- TÜRETİLMİŞ OLASILIKLAR (Normalize ve Ham) ---\n",
        "            'Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm',\n",
        "            'Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw',\n",
        "            'Overround',  # <-- DİKKAT: Bunu ekledik (Gizli sızıntı yapabilir)\n",
        "\n",
        "            # --- DİĞER BAHİS ŞİRKETLERİ (MAX ORANLAR) ---\n",
        "            'MaxHome', 'MaxDraw', 'MaxAway',\n",
        "\n",
        "            # --- GOL BAHİSLERİ (ALT/ÜST) ---\n",
        "            'Over25', 'Under25',\n",
        "            'MaxOver25', 'MaxUnder25',\n",
        "\n",
        "            # --- HANDİKAP BİLGİLERİ ---\n",
        "            'HandiSize', 'HandiHome', 'HandiAway',\n",
        "            'AsianHome', 'AsianAway', # Varsa Asya handikapları da silinmeli\n",
        "\n",
        "            # --- ELO PUANLARI (OPSİYONEL - Saf Saha İçi İstersen Aç) ---\n",
        "            # 'HomeElo', 'AwayElo', 'ELO_Diff'\n",
        "        ]\n",
        "\n",
        "        print(f\"⚠️ [EXPERIMENT] Removing {len(FEATURES_TO_REMOVE_TOTALLY)} betting features to force raw stats learning...\")\n",
        "\n",
        "        # Listede olup dataframe'de olmayanlar hata vermesin diye kontrol\n",
        "        # (errors='ignore' parametresi de kullanılabilir ama bu yöntem daha temiz log verir)\n",
        "        cols_to_drop_odds = [c for c in FEATURES_TO_REMOVE_TOTALLY if c in X_train_raw.columns]\n",
        "\n",
        "        if cols_to_drop_odds:\n",
        "            X_train_raw = X_train_raw.drop(columns=cols_to_drop_odds, errors='ignore')\n",
        "            X_test_raw = X_test_raw.drop(columns=cols_to_drop_odds, errors='ignore')\n",
        "\n",
        "            # Validation seti varsa ondan da çıkar (Validation seti Step 3.7'de oluşturuluyor ama garanti olsun)\n",
        "            if 'X_val' in locals() and X_val is not None:\n",
        "                 X_val = X_val.drop(columns=cols_to_drop_odds, errors='ignore')\n",
        "\n",
        "            print(f\"   ✅ Dropped odds columns ({len(cols_to_drop_odds)} features removed).\")\n",
        "        else:\n",
        "            print(\"   ℹ️ Columns already removed or not found.\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # EDA STRATEGY (DIFF-ONLY)\n",
        "        # -----------------------------------------------------------------\n",
        "        eda_config = CONFIG.get(\"eda_integration\", {})\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # ✅ FIX: cols_to_drop_odds tanımlı değilse boş liste olarak tanımla\n",
        "        # (EXPERIMENT bloğu comment out edildiğinde bu değişken tanımsız kalıyor)\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        if 'cols_to_drop_odds' not in dir():\n",
        "            cols_to_drop_odds = []\n",
        "            print(\"  ℹ️  EXPERIMENT disabled - cols_to_drop_odds set to empty list\")\n",
        "\n",
        "        if eda_config.get(\"enabled\", False) and eda_config.get(\"strategy\") == \"diff_only\":\n",
        "            print(\"\\n[EDA-GUIDED] Applying Diff-Only Strategy...\")\n",
        "\n",
        "            if 'features_to_drop_multicollinearity_REVISED' in globals():\n",
        "                # Odds dışındaki diğer drop edilecekleri belirle (zaten silinenleri tekrar deneme)\n",
        "                other_drops = [f for f in features_to_drop_multicollinearity_REVISED\n",
        "                             if f not in cols_to_drop_odds]\n",
        "\n",
        "                print(f\"  → Dropping additional {len(other_drops)} features from EDA list\")\n",
        "\n",
        "                # Hata vermemesi için 'errors=ignore' kullanıyoruz\n",
        "                X_train_raw = X_train_raw.drop(columns=other_drops, errors='ignore')\n",
        "                X_test_raw = X_test_raw.drop(columns=other_drops, errors='ignore')\n",
        "\n",
        "                print(f\"  ✅ After Diff-Only: {X_train_raw.shape[1]} features\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ EDA drop list not loaded\")\n",
        "        else:\n",
        "            print(\"  ⚠️ EDA integration disabled or strategy not 'diff_only'.\")\n",
        "\n",
        "        # ============================================================================\n",
        "        # 🚨 KRİTİK DÜZELTME: SENKRONİZASYON (SYNCHRONIZATION)\n",
        "        # ============================================================================\n",
        "        # Tüm silme işlemleri bitti. Şimdi features_final listesini\n",
        "        # elimizde kalan gerçek sütunlarla güncelliyoruz.\n",
        "        # ============================================================================\n",
        "        features_final = X_train_raw.columns.tolist()\n",
        "        print(f\"\\n✅ Final feature count (Synced): {len(features_final)}\")\n",
        "        print(f\"✅ DataFrame shape: {X_train_raw.shape}\\n\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # MI RECALCULATION (Artık senkronize olduğu için hata vermez)\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n[SUB-STEP 1.6] 🔄 Recalculating MI scores after feature removal...\")\n",
        "        try:\n",
        "            mi_scores_after = mutual_info_classif(\n",
        "                X_train_raw,\n",
        "                y_train,\n",
        "                random_state=SEED,\n",
        "                n_neighbors=3\n",
        "            )\n",
        "\n",
        "            # Artık features_final ve mi_scores_after aynı uzunlukta olmak ZORUNDA\n",
        "            mi_df_after = pd.DataFrame({\n",
        "                'feature': features_final,\n",
        "                'mi_score': mi_scores_after\n",
        "            }).sort_values('mi_score', ascending=False)\n",
        "\n",
        "            # İstatistiksel özet\n",
        "            print(f\"\\n  📊 MI Score Statistics (After Drop):\")\n",
        "            print(f\"     Mean:   {mi_scores_after.mean():.6f}\")\n",
        "            print(f\"     Max:    {mi_scores_after.max():.6f}\")\n",
        "\n",
        "            # Top 10 feature'ları göster\n",
        "            print(f\"\\n  🏆 Top 10 Features by MI (After Drop):\")\n",
        "            print(\"  \" + \"-\" * 60)\n",
        "            for idx, row in mi_df_after.head(10).iterrows():\n",
        "                print(f\"     {row['feature']:40s} → MI = {row['mi_score']:.6f}\")\n",
        "            print(\"  \" + \"-\" * 60)\n",
        "\n",
        "            # Düşük MI olanları silme bloğu (Opsiyonel)\n",
        "            LOW_MI_THRESHOLD = CONFIG.get('smart_feature_selection', {}).get('mi_threshold', 0.001)\n",
        "            low_mi_features = mi_df_after[mi_df_after['mi_score'] < LOW_MI_THRESHOLD]['feature'].tolist()\n",
        "\n",
        "            if low_mi_features:\n",
        "                print(f\"\\n  🗑️ AUTO-DROP: {len(low_mi_features)} features with MI < {LOW_MI_THRESHOLD}\")\n",
        "                X_train_raw = X_train_raw.drop(columns=low_mi_features)\n",
        "                X_test_raw = X_test_raw.drop(columns=low_mi_features)\n",
        "\n",
        "                # Listeyi tekrar güncelle\n",
        "                features_final = [f for f in features_final if f not in low_mi_features]\n",
        "                print(f\"  ✅ Final features after MI drop: {len(features_final)}\")\n",
        "            else:\n",
        "                print(f\"  ✅ No features dropped by MI threshold\")\n",
        "\n",
        "        except Exception as e_mi:\n",
        "            print(f\"  ❌ MI recalculation failed: {e_mi}\")\n",
        "            # Hata olsa bile devam et, features_final yukarıda güncellendiği için kod kırılmaz\n",
        "\n",
        "            # ================================================================\n",
        "            # CSV'ye kaydet (güncel MI skorları)\n",
        "            # ================================================================\n",
        "            mi_output_path = os.path.join(DATA_DIR, 'mi_scores_after_drop.csv')\n",
        "            mi_df_after.to_csv(mi_output_path, index=False)\n",
        "            print(f\"\\n  ✅ MI scores saved: {os.path.basename(mi_output_path)}\")\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # BAŞLANGIÇ: MI Plot Kodları (Tam Hali)\n",
        "            # -----------------------------------------------------------------\n",
        "            print(f\"\\n  📊 Creating MI distribution plot...\")\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "            # Subplot 1: Bar chart (Top 20)\n",
        "            ax1 = axes[0]\n",
        "            top_20 = mi_df_after.head(20)\n",
        "            bars = ax1.barh(range(len(top_20)), top_20['mi_score'], color='steelblue', edgecolor='black')\n",
        "            ax1.set_yticks(range(len(top_20)))\n",
        "            ax1.set_yticklabels(top_20['feature'], fontsize=12)\n",
        "            ax1.set_xlabel('MI Score', fontsize=12, fontweight='bold')\n",
        "            ax1.set_title('Top 20 Features by Mutual Information', fontsize=12, fontweight='bold')\n",
        "            ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "            ax1.invert_yaxis()\n",
        "\n",
        "            # Değerleri bar'ların üzerine ekle\n",
        "            for i, bar in enumerate(bars):\n",
        "                width = bar.get_width()\n",
        "                ax1.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "                         f'{width:.4f}', ha='left', va='center', fontsize=12)\n",
        "\n",
        "            # Subplot 2: Histogram\n",
        "            ax2 = axes[1]\n",
        "            ax2.hist(mi_scores_after, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "            ax2.axvline(mi_scores_after.mean(), color='red', linestyle='--',\n",
        "                        linewidth=2, label=f'Mean = {mi_scores_after.mean():.4f}')\n",
        "            ax2.axvline(np.median(mi_scores_after), color='green', linestyle='--',\n",
        "                        linewidth=2, label=f'Median = {np.median(mi_scores_after):.4f}')\n",
        "            ax2.set_xlabel('MI Score', fontsize=12, fontweight='bold')\n",
        "            ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "            ax2.set_title('Distribution of MI Scores', fontsize=12, fontweight='bold')\n",
        "            ax2.legend(fontsize=12)\n",
        "            ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            mi_plot_path = os.path.join(GRAPHICS_DIR, '30_mi_scores_after_drop.png')\n",
        "            plt.savefig(mi_plot_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"  ✅ Plot saved: {os.path.basename(mi_plot_path)}\\n\")\n",
        "            # -----------------------------------------------------------------\n",
        "            # BİTİŞ: MI Plot Kodları (Tam Hali)\n",
        "            # -----------------------------------------------------------------\n",
        "\n",
        "        except Exception as e_mi:\n",
        "            print(f\"  ❌ MI recalculation failed: {e_mi}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # (Leakage check ve Log transform)\n",
        "        print(\"\\n[SUB-STEP 3] Leakage check...\")\n",
        "        leakage_check = ['FTHome', 'FTAway', 'HTHome', 'HTAway']\n",
        "        leakage_found = [f for f in leakage_check if f in X_train_raw.columns]\n",
        "        if leakage_found:\n",
        "            raise ValueError(f\"❌ LEAKAGE: {leakage_found}\")\n",
        "        print(\"     ✓ No leakage\")\n",
        "\n",
        "        print(\"\\n[SUB-STEP 4] Log transform skewed features...\")\n",
        "        skewed = [f for f in ['HomeTeam_ClubValue', 'AwayTeam_ClubValue']\n",
        "                  if f in X_train_raw.columns]\n",
        "        for feat in skewed:\n",
        "            X_train_raw[f'{feat}_log'] = np.log1p(X_train_raw[feat])\n",
        "            X_test_raw[f'{feat}_log'] = np.log1p(X_test_raw[feat])\n",
        "            X_train_raw = X_train_raw.drop(columns=[feat])\n",
        "            X_test_raw = X_test_raw.drop(columns=[feat])\n",
        "            if feat in features_final:\n",
        "                features_final.remove(feat)\n",
        "                features_final.append(f'{feat}_log')\n",
        "        print(f\"     ✓ Transformed {len(skewed)} features\")\n",
        "        print(f\"     ✓ Final features: {len(features_final)}\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 3.6: CLASS BALANCING (DÜZELTME: PRE-SMOTE SNAPSHOT EKLEME)\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n3️⃣.6️⃣ CLASS BALANCING (Single Scenario Mode)...\\n\")\n",
        "\n",
        "        class_balancing_config = CONFIG.get(\"class_balancing\", {})\n",
        "        USE_SMOTE = class_balancing_config.get(\"use_smote\", False)\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # ✅ YENİ EKLEME: OUTPUT_SUFFIX TANIMLAMA\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        if USE_SMOTE:\n",
        "            output_suffix = \"_WITH_SMOTE\"\n",
        "            print(f\"    📊 Scenario: WITH SMOTE\")\n",
        "            print(f\"       Strategy: {class_balancing_config.get('strategy', 'auto')}\")\n",
        "        else:\n",
        "            output_suffix = \"_NO_SMOTE\"\n",
        "            print(f\"    📊 Scenario: NO SMOTE\")\n",
        "            print(f\"       Models will train on original class distribution\")\n",
        "        print()\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # 🆕 ADIM 3.7: VALIDATION SPLIT (PRE-SMOTE - EARLY STOPPING İÇİN)\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(\"\\n3️⃣.7️⃣ VALIDATION SPLIT (Pre-SMOTE - For Early Stopping)...\\n\")\n",
        "\n",
        "        early_stop_config = CONFIG.get('early_stopping', {})\n",
        "        USE_EARLY_STOPPING = early_stop_config.get('enabled', False)\n",
        "\n",
        "        if USE_EARLY_STOPPING:\n",
        "            print(f\"  🎯 Early Stopping: ENABLED\")\n",
        "            print(f\"     Validation split: {early_stop_config.get('val_split', 0.15)*100:.0f}%\")\n",
        "\n",
        "            # ═══════════════════════════════════════════════════════════════════════\n",
        "            # ✅ TEMPORAL SPLIT (Lag features için güvenli)\n",
        "            # ═══════════════════════════════════════════════════════════════════════\n",
        "            print(f\"     Split method: TEMPORAL (last {early_stop_config.get('val_split', 0.15)*100:.0f}% of train data)\")\n",
        "\n",
        "            n_train_total = len(X_train_raw)\n",
        "            n_val = int(n_train_total * early_stop_config.get('val_split', 0.15))\n",
        "            split_idx = n_train_total - n_val\n",
        "\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            # SPLIT: First 85% = Train, Last 15% = Validation\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            X_val = X_train_raw.iloc[split_idx:].copy()\n",
        "            y_val = y_train.iloc[split_idx:].copy()\n",
        "\n",
        "            X_train_raw = X_train_raw.iloc[:split_idx].copy()\n",
        "            y_train = y_train.iloc[:split_idx].copy()\n",
        "\n",
        "            print(f\"\\n  ✅ Temporal split successful:\")\n",
        "            print(f\"     • Train samples:      {len(X_train_raw):,}\")\n",
        "            print(f\"     • Validation samples: {len(X_val):,}\")\n",
        "\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            # TEMPORAL VERIFICATION (Optional but recommended)\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            try:\n",
        "                if 'MatchDate' in df.columns:\n",
        "                    train_indices = X_train_raw.index\n",
        "                    val_indices = X_val.index\n",
        "\n",
        "                    train_dates = df.loc[train_indices, 'MatchDate']\n",
        "                    val_dates = df.loc[val_indices, 'MatchDate']\n",
        "\n",
        "                    print(f\"\\n  📅 Temporal boundaries:\")\n",
        "                    print(f\"     • Train period:      {train_dates.min().strftime('%Y-%m-%d')} → \"\n",
        "                          f\"{train_dates.max().strftime('%Y-%m-%d')}\")\n",
        "                    print(f\"     • Validation period: {val_dates.min().strftime('%Y-%m-%d')} → \"\n",
        "                          f\"{val_dates.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "                    # Critical check: No overlap\n",
        "                    if train_dates.max() < val_dates.min():\n",
        "                        print(f\"     ✅ No temporal overlap (lag features safe!)\")\n",
        "                    else:\n",
        "                        overlap_days = (train_dates.max() - val_dates.min()).days\n",
        "                        print(f\"     ⚠️  WARNING: {overlap_days} days overlap detected!\")\n",
        "                        print(f\"        This may cause leakage with lag features\")\n",
        "            except Exception as e_date:\n",
        "                print(f\"  ℹ️  Could not verify temporal boundaries: {e_date}\")\n",
        "\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            # CLASS DISTRIBUTION (Informational)\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            train_counts = y_train.value_counts().sort_index()\n",
        "            val_counts = y_val.value_counts().sort_index()\n",
        "\n",
        "            print(f\"\\n  📊 Class distribution (after temporal split):\")\n",
        "            print(f\"     Train:\")\n",
        "            print(f\"       • Away Win (0): {train_counts.get(0, 0):,} ({train_counts.get(0, 0)/len(y_train)*100:.1f}%)\")\n",
        "            print(f\"       • Draw (1):     {train_counts.get(1, 0):,} ({train_counts.get(1, 0)/len(y_train)*100:.1f}%)\")\n",
        "            print(f\"       • Home Win (2): {train_counts.get(2, 0):,} ({train_counts.get(2, 0)/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "            print(f\"     Validation:\")\n",
        "            print(f\"       • Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"       • Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"       • Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)\")\n",
        "\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            # IMBALANCE WARNING (Class imbalance may be worse with temporal split)\n",
        "            # ───────────────────────────────────────────────────────────────────────\n",
        "            val_max = val_counts.max()\n",
        "            val_min = val_counts.min()\n",
        "            val_imbalance_ratio = val_max / val_min if val_min > 0 else float('inf')\n",
        "\n",
        "            if val_imbalance_ratio > 2.0:\n",
        "                print(f\"\\n  ⚠️  Validation set imbalance: {val_imbalance_ratio:.1f}:1\")\n",
        "                print(f\"      (This is normal with temporal split - won't affect training)\")\n",
        "\n",
        "            print(f\"\\n  ✅ Validation set characteristics:\")\n",
        "            print(f\"     • PRE-SMOTE (contains ZERO synthetic samples)\")\n",
        "            print(f\"     • Temporally AFTER all training data\")\n",
        "            print(f\"     • Safe for lag features (no data leakage)\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  ℹ️  Early Stopping: DISABLED\")\n",
        "            print(f\"     Validation split skipped - models will train without early stopping\")\n",
        "            X_val = None\n",
        "            y_val = None\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # ADIM 4: SCALING PIPELINE (SMOTE deferred to CV)\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(\"\\n4️⃣ SCALING PIPELINE (SMOTE will be applied in CV)...\\n\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"📊 PIPELINE: SCALE ONLY (SMOTE deferred to CV)\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        print(\"  ℹ️  SMOTE will be applied inside ImbPipeline during cross-validation\")\n",
        "        print(\"  ℹ️  This ensures validation folds never see synthetic samples\\n\")\n",
        "\n",
        "        scaler = RobustScaler()\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # STEP 1: FIT SCALER ON ORIGINAL DATA\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(\"[STEP 1/3] Fitting scaler on original train data...\")\n",
        "        print(f\"  📊 Train samples: {len(X_train_raw):,}\")\n",
        "        print(f\"  📊 Features: {X_train_raw.shape[1]}\")\n",
        "\n",
        "        scaler.fit(X_train_raw)\n",
        "\n",
        "        # Debug: Show learned parameters (sample feature)\n",
        "        sample_feature = 'ClubValue_Diff' if 'ClubValue_Diff' in features_final else features_final[0]\n",
        "        feature_idx = features_final.index(sample_feature)\n",
        "\n",
        "        print(f\"\\n  📌 Scaler parameters (sample: {sample_feature}):\")\n",
        "        print(f\"     • Center (median): {scaler.center_[feature_idx]:,.2f}\")\n",
        "        print(f\"     • Scale (IQR):     {scaler.scale_[feature_idx]:,.2f}\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # STEP 2: TRANSFORM ALL DATASETS\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(f\"\\n[STEP 2/3] Scaling all datasets...\")\n",
        "\n",
        "        # Train\n",
        "        X_train_scaled = scaler.transform(X_train_raw)\n",
        "        print(f\"  ✅ Train scaled: {X_train_scaled.shape}\")\n",
        "\n",
        "        # Test\n",
        "        X_test_scaled = scaler.transform(X_test_raw)\n",
        "        print(f\"  ✅ Test scaled: {X_test_scaled.shape}\")\n",
        "\n",
        "        # Validation (if exists)\n",
        "        if X_val is not None:\n",
        "            X_val_scaled = scaler.transform(X_val)\n",
        "            print(f\"  ✅ Validation scaled: {X_val_scaled.shape}\")\n",
        "        else:\n",
        "            X_val_scaled = None\n",
        "            print(f\"  ℹ️  No validation set\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # STEP 3: CONVERT TO DATAFRAME (NO SMOTE!)\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(f\"\\n[STEP 3/3] Converting to DataFrames...\")\n",
        "\n",
        "        # ✅ ÇÖZÜM: SMOTE uygulanmamış veri!\n",
        "        X_train = pd.DataFrame(X_train_scaled, columns=features_final)\n",
        "        y_train = pd.Series(y_train, name='FTResult_Encoded')  # Original labels\n",
        "\n",
        "        X_test = pd.DataFrame(X_test_scaled, columns=features_final, index=X_test_raw.index)\n",
        "\n",
        "        if X_val_scaled is not None:\n",
        "            X_val = pd.DataFrame(X_val_scaled, columns=features_final)\n",
        "        else:\n",
        "            X_val = None\n",
        "            y_val = None\n",
        "\n",
        "        print(f\"  ✅ Train: {X_train.shape} (PRE-SMOTE)\")\n",
        "        print(f\"  ✅ Test:  {X_test.shape}\")\n",
        "        if X_val is not None:\n",
        "            print(f\"  ✅ Validation: {X_val.shape} (PRE-SMOTE)\")\n",
        "\n",
        "        # # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # # SAVE SCALER & METADATA\n",
        "        # # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # print(f\"\\n[SAVE] Persisting scaler and metadata...\")\n",
        "\n",
        "        # output_suffix = \"_WITH_IMBALANCED_PIPELINE\"\n",
        "\n",
        "        # scaler_path = os.path.join(OUT_DIR, f\"robust_scaler{output_suffix}.pkl\")\n",
        "        # joblib.dump(scaler, scaler_path)\n",
        "        # print(f\"  ✅ Scaler saved: {os.path.basename(scaler_path)}\")\n",
        "\n",
        "        # # Metadata\n",
        "        # scaling_metadata = {\n",
        "        #     'pipeline': 'SCALE → (SMOTE deferred to CV Pipeline)',\n",
        "        #     'smote_applied': 'Inside ImbPipeline during CV',\n",
        "        #     'smote_strategy': CONFIG.get(\"class_balancing\", {}).get(\"strategy\", \"auto\"),\n",
        "        #     'train_samples': len(X_train_raw),\n",
        "        #     'test_samples': len(X_test_raw),\n",
        "        #     'scaler_type': 'RobustScaler',\n",
        "        #     'note': 'SMOTE is NOT applied here - will be applied per CV fold'\n",
        "        # }\n",
        "\n",
        "        # metadata_path = os.path.join(OUT_DIR, f\"scaling_metadata{output_suffix}.json\")\n",
        "        # with open(metadata_path, 'w') as f:\n",
        "        #     json.dump(scaling_metadata, f, indent=2)\n",
        "        # print(f\"  ✅ Metadata saved: {os.path.basename(metadata_path)}\")\n",
        "\n",
        "        # print(\"\\n\" + \"=\"*80)\n",
        "        # print(f\"✅ SCALING COMPLETE (SMOTE deferred to CV)\")\n",
        "        # print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        print(\"[INFO] Class distribution (PRE-SMOTE):\")\n",
        "        y_counts_pre = y_train.value_counts().sort_index()\n",
        "        print(f\"  • Away Win (0): {y_counts_pre.get(0, 0):,} ({y_counts_pre.get(0, 0)/len(y_train)*100:.1f}%)\")\n",
        "        print(f\"  • Draw (1):     {y_counts_pre.get(1, 0):,} ({y_counts_pre.get(1, 0)/len(y_train)*100:.1f}%)\")\n",
        "        print(f\"  • Home Win (2): {y_counts_pre.get(2, 0):,} ({y_counts_pre.get(2, 0)/len(y_train)*100:.1f}%)\")\n",
        "        print(f\"\\n  ℹ️  SMOTE will balance this inside each CV fold\\n\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        # 🔍 CRITICAL VERIFICATION (FOR DEBUGGING)\n",
        "        # ═══════════════════════════════════════════════════════════════════════════\n",
        "        print(\"[VERIFICATION] Sanity checks...\")\n",
        "\n",
        "        # 1. Check for NaN/Inf\n",
        "        if np.isnan(X_train.values).any() or np.isinf(X_train.values).any():\n",
        "            print(\"  ❌ WARNING: NaN or Inf detected in X_train!\")\n",
        "            nan_cols = [features_final[i] for i in range(X_train.shape[1])\n",
        "                        if np.isnan(X_train.values[:, i]).any()]\n",
        "            print(f\"     Problematic features: {nan_cols}\")\n",
        "        else:\n",
        "            print(\"  ✅ No NaN/Inf in train data\")\n",
        "\n",
        "        # 2. Check scaling effectiveness\n",
        "        sample_feat_idx = 0\n",
        "        train_mean = X_train.iloc[:, sample_feat_idx].mean()\n",
        "        train_std = X_train.iloc[:, sample_feat_idx].std()\n",
        "        print(f\"  ✅ Sample feature (scaled): mean={train_mean:.4f}, std={train_std:.4f}\")\n",
        "\n",
        "        # 3. Check class balance (if SMOTE)\n",
        "        if USE_SMOTE:\n",
        "            y_counts = y_train.value_counts()\n",
        "            balance_ratio = y_counts.max() / y_counts.min()\n",
        "            print(f\"  ✅ Class balance ratio: {balance_ratio:.2f}:1 (lower is better)\")\n",
        "\n",
        "            if balance_ratio > 3.0:\n",
        "                print(f\"     ⚠️  Still highly imbalanced (>3:1)\")\n",
        "            else:\n",
        "                print(f\"     ✅ Reasonably balanced (<3:1)\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # # ═══════════════════════════════════════════════════════════════════════\n",
        "        # # CONVERT TO DATAFRAME (WITH PROPER INDEXING)\n",
        "        # # ═══════════════════════════════════════════════════════════════════════\n",
        "        # print(f\"\\n[FINALIZATION] Converting to DataFrames...\")\n",
        "\n",
        "        # # Train DataFrame (index may be reset if SMOTE applied)\n",
        "        # X_train = pd.DataFrame(\n",
        "        #     X_train_final,\n",
        "        #     columns=features_final\n",
        "        # )\n",
        "\n",
        "        # # y_train as Series (matching X_train index)\n",
        "        # y_train = pd.Series(\n",
        "        #     y_train_final,\n",
        "        #     name='FTResult_Encoded'\n",
        "        # )\n",
        "\n",
        "        # # Test DataFrame (preserve original index)\n",
        "        # X_test = pd.DataFrame(\n",
        "        #     X_test_scaled,\n",
        "        #     columns=features_final,\n",
        "        #     index=X_test_raw.index  # ✅ Keep original temporal index\n",
        "        # )\n",
        "\n",
        "        # # ✅ YENİ EKLEME: Validation DataFrame (if exists)\n",
        "        # if X_val_scaled is not None:\n",
        "        #     X_val = pd.DataFrame(\n",
        "        #         X_val_scaled,\n",
        "        #         columns=features_final\n",
        "        #     )\n",
        "        #     print(f\"  ✅ Validation converted: {X_val.shape}\")\n",
        "        #     print(f\"  ✅ Validation is SCALED (ready for training)\")\n",
        "        # else:\n",
        "        #     X_val = None\n",
        "        #     y_val = None  # Validation yoksa y_val'i de None yap\n",
        "        #     print(f\"  ℹ️  No validation set\")\n",
        "\n",
        "        # print(f\"  ✅ Final train: {X_train.shape}\")\n",
        "        # print(f\"  ✅ Final test:  {X_test.shape}\")\n",
        "        # print(f\"  ✅ Output suffix: {output_suffix}\")\n",
        "\n",
        "        # ═══════════════════════════════════════════════════════════════════════\n",
        "        # 🔍 PRE-TRAINING VALIDATION TEST  Kodun doğru çalıştığını test etmek için  Bu testler başarısız olursa, scaling pipeline'da bir sorun var demektir.\n",
        "        # ═══════════════════════════════════════════════════════════════════════\n",
        "        if X_val is not None and y_val is not None:\n",
        "            print(\"\\n[VALIDATION TEST]\")\n",
        "\n",
        "            # Test 1: Shape match\n",
        "            assert X_val.shape[0] == len(y_val), \"X_val and y_val length mismatch!\"\n",
        "            print(f\"  ✅ Shape match: {X_val.shape[0]} == {len(y_val)}\")\n",
        "\n",
        "            # Test 2: Scaling check\n",
        "            first_feature_mean = X_val.iloc[:, 0].mean()\n",
        "            assert abs(first_feature_mean) < 0.5, f\"X_val not scaled! Mean = {first_feature_mean}\"\n",
        "            print(f\"  ✅ Scaled correctly: mean = {first_feature_mean:.4f} (close to 0)\")\n",
        "\n",
        "            # Test 3: No NaN\n",
        "            assert not X_val.isnull().any().any(), \"X_val contains NaN!\"\n",
        "            print(f\"  ✅ No NaN values\")\n",
        "\n",
        "            # Test 4: Column match\n",
        "            assert list(X_val.columns) == features_final, \"X_val columns mismatch!\"\n",
        "            print(f\"  ✅ Columns match: {len(features_final)} features\")\n",
        "\n",
        "            print(f\"  ✅ All validation tests passed!\\n\")\n",
        "        else:\n",
        "            print(f\"\\n[VALIDATION TEST] Skipped (X_val is None)\\n\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 5: TRAIN MODELS (BASİTLEŞTİRİLDİ)\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n5️⃣ TRAINING MODELS...\\n\")\n",
        "\n",
        "        # ✅ CHECKPOINT: Validation durumunu göster\n",
        "        print(\"=\"*80)\n",
        "        print(\"📊 VALIDATION STATUS BEFORE TRAINING\")\n",
        "        print(\"=\"*80)\n",
        "        if X_val is not None:\n",
        "            print(f\"✅ X_val: {X_val.shape} (SCALED)\")\n",
        "            print(f\"✅ y_val: {y_val.shape if hasattr(y_val, 'shape') else len(y_val)}\")\n",
        "            print(f\"✅ Sample scaled value: {X_val.iloc[0, 0]:.4f} (should be ~0)\")\n",
        "\n",
        "            # ✅ Class distribution check\n",
        "            val_counts = y_val.value_counts().sort_index()\n",
        "            print(f\"\\n📊 Validation class distribution:\")\n",
        "            print(f\"   • Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"   • Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)\")\n",
        "            print(f\"   • Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"⚠️  X_val is None (early stopping disabled)\")\n",
        "            print(f\"   Models will train without early stopping\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        # ✅ DEĞİŞİKLİK: 4 değer return ediliyor (validation_data eklendi)\n",
        "        results, models_dict, xai_results, validation_data = train_models(\n",
        "            X_train, X_test, y_train, y_test,\n",
        "            X_val=X_val,      # ✅ ARTIK SCALED!\n",
        "            y_val=y_val,\n",
        "            features=features_final,\n",
        "            seed=SEED,\n",
        "            CONFIG=CONFIG,\n",
        "            OUT_DIR=OUT_DIR,\n",
        "            N_CLASSES=N_CLASSES\n",
        "        )\n",
        "\n",
        "        # ✅ YENİ: Validation set'i unpack et\n",
        "        X_val_returned, y_val_returned = validation_data\n",
        "\n",
        "        # ✅ YENİ: Validation set bilgisini yazdır\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"📊 POST-TRAINING VALIDATION STATUS\")\n",
        "        print(\"=\"*80)\n",
        "        if X_val_returned is not None:\n",
        "            print(f\"✅ Validation set received: {len(X_val_returned):,} samples\")\n",
        "            print(f\"   • Contains NO synthetic samples (correct!)\")\n",
        "            print(f\"   • Was used for early stopping (XGBoost, LightGBM, CatBoost)\")\n",
        "            print(f\"   • Will be used for ablation analysis (G28, G29)\")\n",
        "        else:\n",
        "            print(f\"⚠️  No validation set (early stopping disabled)\")\n",
        "            print(f\"   • Models trained without early stopping\")\n",
        "            print(f\"   • Ablation will use test set (less ideal)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\n✅ {output_suffix} training complete: {len(results)} models\\n\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 6: CREATE TABLES\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n6️⃣ CREATING TABLES...\\n\")\n",
        "        tables = create_dynamic_tables(results)\n",
        "        table_png_paths = save_tables_as_png(tables, TABLES_DIR)\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 7: CREATE GRAPHICS\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n7️⃣ CREATING GRAPHICS...\\n\")\n",
        "\n",
        "        # ✅ CHECKPOINT: Graphics'e gönderilecek validation'ı kontrol et\n",
        "        print(\"=\"*80)\n",
        "        print(\"📊 VALIDATION STATUS FOR GRAPHICS\")\n",
        "        print(\"=\"*80)\n",
        "        if X_val_returned is not None:\n",
        "            print(f\"✅ Will send validation to graphics:\")\n",
        "            print(f\"   • X_val shape: {X_val_returned.shape}\")\n",
        "            print(f\"   • y_val length: {len(y_val_returned)}\")\n",
        "            print(f\"   • For use in: G28 (XAI comparison), G29 (Cumulative importance)\")\n",
        "        else:\n",
        "            print(f\"⚠️  No validation - graphics will use test set\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        # ✅ DEĞİŞİKLİK: X_val ve y_val parametreleri eklendi\n",
        "        graphics = create_dynamic_graphics(\n",
        "            results=results,\n",
        "            xai_results=xai_results,\n",
        "            output_dir=GRAPHICS_DIR,\n",
        "            features=features_final,\n",
        "            models_dict=models_dict,\n",
        "            y_test=y_test,\n",
        "            X_test=X_test,\n",
        "            X_train=X_train,\n",
        "            y_train=y_train,\n",
        "            X_val=X_val_returned,    # ✅ RETURNED değeri kullan (train_models'dan gelen)\n",
        "            y_val=y_val_returned     # ✅ RETURNED değeri kullan\n",
        "        )\n",
        "\n",
        "        # Grafik listesi None ise kurtarma\n",
        "        if graphics is None:\n",
        "            print(\"  ⚠️ Graphics creation failed, collecting existing files...\")\n",
        "            graphics = []\n",
        "            if os.path.exists(GRAPHICS_DIR):\n",
        "                graphics = [os.path.join(GRAPHICS_DIR, f) for f in sorted(os.listdir(GRAPHICS_DIR))\n",
        "                            if f.endswith('.png')]\n",
        "                print(f\"  ✓ Found {len(graphics)} graphics in {GRAPHICS_DIR}\")\n",
        "            else:\n",
        "                print(f\"  ❌ Graphics directory not found: {GRAPHICS_DIR}\")\n",
        "\n",
        "        print(f\"\\n✅ Total graphics created: {len(graphics)}\")\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 8: CREATE PDF\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n8️⃣ CREATING PDF...\\n\")\n",
        "        pdf_path = create_pdf_report(OUT_DIR, graphics, tables)\n",
        "\n",
        "        # DÜZELTME: PDF'i senaryoya göre yeniden adlandır\n",
        "        if pdf_path:\n",
        "            new_pdf_path = os.path.join(OUT_DIR, f\"Football_Prediction_v18{output_suffix}.pdf\")\n",
        "            try:\n",
        "                os.rename(pdf_path, new_pdf_path)\n",
        "                print(f\"✅ PDF Report Renamed: {os.path.basename(new_pdf_path)}\")\n",
        "            except Exception as e_rename:\n",
        "                print(f\"⚠️  Could not rename PDF: {e_rename}. Original path: {pdf_path}\")\n",
        "                new_pdf_path = pdf_path # Yeni yolu eski yola eşitle\n",
        "        else:\n",
        "             new_pdf_path = None # PDF oluşturulamadıysa\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADIM 9: SAVE RESULTS\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n9️⃣ SAVING RESULTS...\\n\")\n",
        "        # DÜZELTME: Sonuçları da son ek ile kaydet\n",
        "        json_path = save_results_safely(results, OUT_DIR)\n",
        "        if json_path:\n",
        "            new_json_path = os.path.join(OUT_DIR, f\"results{output_suffix}.json\")\n",
        "            try:\n",
        "                os.rename(json_path, new_json_path)\n",
        "                print(f\"✅ Results Renamed: {os.path.basename(new_json_path)}\")\n",
        "            except Exception as e_rename_json:\n",
        "                print(f\"⚠️  Could not rename JSON: {e_rename_json}. Original path: {json_path}\")\n",
        "                new_json_path = json_path # Yeni yolu eski yola eşitle\n",
        "        else:\n",
        "            new_json_path = None # JSON oluşturulamadıysa\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # FINAL SUMMARY\n",
        "        # -----------------------------------------------------------------\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(f\"✨ v18.0 COMPLETE - SCENARIO: {output_suffix}\")\n",
        "        print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "        print(\"📊 EXECUTION SUMMARY:\\n\")\n",
        "        print(f\"✅ Data: {len(df):,} matches\")\n",
        "\n",
        "        matched_c = sum(1 for v in data_integration.team_mapping.values() if v is not None)\n",
        "        total_c = len(data_integration.team_mapping)\n",
        "        match_rate = (matched_c / total_c * 100) if total_c > 0 else 0\n",
        "        print(f\"✅ Team Mapping: {matched_c}/{total_c} ({match_rate:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n📈 FEATURE STATISTICS:\")\n",
        "        print(f\"  • Initial features (pre-lag): {len(features)}\")\n",
        "        print(f\"  • Final features (post-MI): {len(features_final)}\")\n",
        "        print(f\"  • Reduction: {len(features) - len(features_final)} ({(1 - len(features_final)/len(features))*100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n🎯 MODEL PERFORMANCE:\")\n",
        "        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys() if pd.notna(results[m].get('test_f1'))}\n",
        "        if valid_models_f1:\n",
        "            best_model_name = max(valid_models_f1, key=valid_models_f1.get)\n",
        "            print(f\"  ✅ Best Model: {best_model_name} (F1 = {valid_models_f1[best_model_name]:.4f})\")\n",
        "\n",
        "        print(f\"\\n📁 OUTPUTS:\")\n",
        "        print(f\"  ✅ Directory: {OUT_DIR}\")\n",
        "        print(f\"  ✅ Graphics: {len(graphics)}\")\n",
        "        print(f\"  ✅ Tables: {len(tables)}\")\n",
        "        if new_pdf_path:\n",
        "            print(f\"  ✅ PDF Report: {os.path.basename(new_pdf_path)}\")\n",
        "        if new_json_path:\n",
        "            print(f\"  ✅ Results JSON: {os.path.basename(new_json_path)}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time = end_time - start_time\n",
        "        print(f\"\\n⏱️  Total Execution Time ({output_suffix}):\")\n",
        "        print(f\"   {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(f\"✅ v18.0 EXECUTION COMPLETE (SCENARIO: {output_suffix})\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ EXECUTION FAILED!\")\n",
        "        print(f\"Error Type: {type(e).__name__}\")\n",
        "        print(f\"Error Message: {e}\\n\")\n",
        "\n",
        "        import traceback\n",
        "        print(\"=\"*100)\n",
        "        print(\"FULL ERROR TRACEBACK:\")\n",
        "        print(\"=\"*100)\n",
        "        traceback.print_exc()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(\"❌ v18.0 EXECUTION INCOMPLETE - ERROR OCCURRED\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # Data Statistics\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        print(f\"✅ Data: {len(df):,} matches\")\n",
        "\n",
        "        matched_c = sum(1 for v in data_integration.team_mapping.values() if v is not None)\n",
        "        total_c = len(data_integration.team_mapping)\n",
        "        match_rate = (matched_c / total_c * 100) if total_c > 0 else 0\n",
        "        print(f\"✅ Team Mapping: {matched_c}/{total_c} ({match_rate:.1f}%)\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # 🆕 EDA-Guided Improvements\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        eda_config = CONFIG.get(\"eda_integration\", {})\n",
        "\n",
        "        if eda_config.get(\"enabled\", False):\n",
        "            print(f\"\\n📋 EDA-GUIDED IMPROVEMENTS:\")\n",
        "\n",
        "            if eda_config.get(\"strategy\") == \"diff_only\":\n",
        "                print(f\"  ✅ Diff-Only Strategy applied\")\n",
        "\n",
        "                if 'features_to_drop_multicollinearity_REVISED' in globals():\n",
        "                    print(f\"  ✅ Features dropped: {len(features_to_drop_multicollinearity_REVISED)}\")\n",
        "\n",
        "                print(f\"  ✅ VIF reduction: High → <10 (expected)\")\n",
        "\n",
        "            if 'mi_scores_eda_df' in locals() and mi_scores_eda_df is not None:\n",
        "                print(f\"  ✅ MI scores loaded: {len(mi_scores_eda_df)} features\")\n",
        "\n",
        "            print(f\"\\n📋 EDA FILES USED:\")\n",
        "            print(f\"  • Base path: {eda_config.get('base_path', 'N/A')}\")\n",
        "            print(f\"  • Drop list: {os.path.basename(eda_config.get('drop_list_file', 'N/A'))}\")\n",
        "            print(f\"  • MI scores: {os.path.basename(eda_config.get('mi_scores_file', 'N/A'))}\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # Feature Statistics\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        print(f\"\\n📈 FEATURE STATISTICS:\")\n",
        "\n",
        "        if 'features' in locals() and 'features_final' in locals():\n",
        "            print(f\"  • Initial features: {len(features)}\")\n",
        "            print(f\"  • After processing: {len(features_final)}\")\n",
        "            print(f\"  • Reduction: {len(features) - len(features_final)} ({(1 - len(features_final)/len(features))*100:.1f}%)\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # 🆕 SMOTE Comparison Results (FIXED VERSION)\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        class_balancing_config = CONFIG.get(\"class_balancing\", {})\n",
        "        COMPARE_SMOTE = class_balancing_config.get(\"compare_smote\", False)\n",
        "\n",
        "        if COMPARE_SMOTE:\n",
        "                print(f\"\\n🔄 SMOTE COMPARISON:\")\n",
        "                print(f\"  • Mode: Dual training (SMOTE vs NO-SMOTE)\")\n",
        "                print(f\"  • Total models trained: {len(results)}\")\n",
        "\n",
        "                # Count versions\n",
        "                smote_models = sum(1 for k in results.keys() if 'WITH-SMOTE' in k)\n",
        "                no_smote_models = sum(1 for k in results.keys() if 'NO-SMOTE' in k)\n",
        "                print(f\"  • WITH-SMOTE: {smote_models} models\")\n",
        "                print(f\"  • NO-SMOTE: {no_smote_models} models\")\n",
        "\n",
        "                # ✅ FIX 1: INITIALIZE VARIABLES OUTSIDE IF BLOCKS\n",
        "                valid_smote = {}\n",
        "                valid_no_smote = {}\n",
        "                best_smote = None\n",
        "                best_no_smote = None\n",
        "                best_smote_f1 = 0.0\n",
        "                best_no_smote_f1 = 0.0\n",
        "\n",
        "                # Best from each version\n",
        "                smote_results = {k: v for k, v in results.items() if 'WITH-SMOTE' in k}\n",
        "                no_smote_results = {k: v for k, v in results.items() if 'NO-SMOTE' in k}\n",
        "\n",
        "                # ✅ FIX 2: NOW VARIABLES ARE ALWAYS DEFINED\n",
        "                if smote_results:\n",
        "                        valid_smote = {k: v.get('test_f1', -1) for k, v in smote_results.items() if pd.notna(v.get('test_f1'))}\n",
        "                        if valid_smote:\n",
        "                                best_smote = max(valid_smote, key=valid_smote.get)\n",
        "                                best_smote_f1 = valid_smote[best_smote]\n",
        "                                print(f\"\\n  🟢 Best WITH-SMOTE: {best_smote}\")\n",
        "                                print(f\"     Test F1: {best_smote_f1:.4f}\")\n",
        "\n",
        "                if no_smote_results:\n",
        "                        valid_no_smote = {k: v.get('test_f1', -1) for k, v in no_smote_results.items() if pd.notna(v.get('test_f1'))}\n",
        "                        if valid_no_smote:\n",
        "                                best_no_smote = max(valid_no_smote, key=valid_no_smote.get)\n",
        "                                best_no_smote_f1 = valid_no_smote[best_no_smote]\n",
        "                                print(f\"\\n  🔴 Best NO-SMOTE: {best_no_smote}\")\n",
        "                                print(f\"     Test F1: {best_no_smote_f1:.4f}\")\n",
        "\n",
        "                # ✅ FIX 3: SAFE COMPARISON (NOW NO SCOPE ERROR)\n",
        "                if valid_smote and valid_no_smote:\n",
        "                        if best_smote_f1 > best_no_smote_f1:\n",
        "                                diff = best_smote_f1 - best_no_smote_f1\n",
        "                                print(f\"\\n  🏆 OVERALL WINNER: WITH-SMOTE (+{diff:.4f})\")\n",
        "                        elif best_no_smote_f1 > best_smote_f1:\n",
        "                                diff = best_no_smote_f1 - best_smote_f1\n",
        "                                print(f\"\\n  🏆 OVERALL WINNER: NO-SMOTE (+{diff:.4f})\")\n",
        "                        else:\n",
        "                                print(f\"\\n  🤝 RESULT: TIE (both perform equally)\")\n",
        "                else:\n",
        "                        print(f\"\\n  ⚠️  Cannot compare: Missing SMOTE or NO-SMOTE results\")\n",
        "                        if not valid_smote:\n",
        "                                print(f\"     • WITH-SMOTE results: MISSING\")\n",
        "                        if not valid_no_smote:\n",
        "                                print(f\"     • NO-SMOTE results: MISSING\")\n",
        "\n",
        "        else:\n",
        "                # Single mode (unchanged)\n",
        "                print(f\"\\n🎯 MODEL PERFORMANCE:\")\n",
        "                valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys() if pd.notna(results[m].get('test_f1'))}\n",
        "                if valid_models_f1:\n",
        "                        best_model_name = max(valid_models_f1, key=valid_models_f1.get)\n",
        "                        print(f\"  ✅ Best Model: {best_model_name} (F1 = {valid_models_f1[best_model_name]:.4f})\")\n",
        "\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # Output Information\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        print(f\"\\n📁 OUTPUTS:\")\n",
        "        print(f\"  ✅ Directory: {OUT_DIR}\")\n",
        "        print(f\"  ✅ Graphics: {len(graphics) if 'graphics' in locals() else 'N/A'}\")\n",
        "        print(f\"  ✅ Tables: {len(tables) if 'tables' in locals() else 'N/A'}\")\n",
        "\n",
        "        if 'pdf_path' in locals() and pdf_path:\n",
        "            print(f\"  ✅ PDF Report: {os.path.basename(pdf_path)}\")\n",
        "\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "        # Execution Time\n",
        "        # ════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time = end_time - start_time\n",
        "        print(f\"\\n⏱️  Total Execution Time:\")\n",
        "        print(f\"   {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(\"✅ v18.1 EXECUTION COMPLETE - EDA-GUIDED WITH SMOTE COMPARISON\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "    except Exception as e:  # ← ✅ EKSİK OLAN KISIM!\n",
        "        print(f\"\\n❌ EXECUTION FAILED!\")\n",
        "        print(f\"Error Type: {type(e).__name__}\")\n",
        "        print(f\"Error Message: {e}\\n\")\n",
        "\n",
        "        import traceback\n",
        "        print(\"=\"*100)\n",
        "        print(\"FULL ERROR TRACEBACK:\")\n",
        "        print(\"=\"*100)\n",
        "        traceback.print_exc()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*100)\n",
        "        print(\"❌ v18.1 EXECUTION INCOMPLETE - ERROR OCCURRED\")\n",
        "        print(\"=\"*100)"
      ]
    }
  ]
}