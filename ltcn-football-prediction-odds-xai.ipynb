#!/usr/bin/env python3
"""
FOOTBALL MATCH PREDICTION SYSTEM v18.0 - ODDS-PLUS WITH EDA PIPELINE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

print("‚úì Installing and Importing libraries.\n")

# Install required packages
!pip install pandas numpy scikit-learn optuna matplotlib seaborn scipy shap xgboost lightgbm catboost rapidfuzz unidecode openpyxl reportlab joblib pygad   -q
!pip install imbalanced-learn pygad -q

import subprocess
import sys

try:
    subprocess.check_call([sys.executable, "-m", "pip", "-q", "install", "--upgrade", "pip"])
    subprocess.check_call([sys.executable, "-m", "pip", "-q", "install",
                          "pandas", "numpy", "scikit-learn", "optuna", "matplotlib", "seaborn",
                          "scipy", "shap", "joblib", "tqdm", "rapidfuzz", "unidecode", "pillow"])
    subprocess.check_call([sys.executable, "-m", "pip", "-q", "install",
                          "xgboost", "lightgbm", "catboost", "statsmodels", "openpyxl",
                          "reportlab", "plotly", "kaleido"])
except:
    print("[WARNING] Some libraries may not have installed properly")

# Core imports
import os
import json
import numpy as np
import pandas as pd
import warnings
import time
import pickle
import random
import traceback
import gc
import csv
import re
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages
from pathlib import Path
from functools import lru_cache
from joblib import Parallel, delayed
from datetime import datetime, timedelta
from tqdm import tqdm
from rapidfuzz import fuzz
from unidecode import unidecode
from PIL import Image
from io import BytesIO
import joblib

# Scikit-learn
from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import RobustScaler, LabelEncoder, label_binarize
from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, log_loss,
                             brier_score_loss, confusion_matrix, roc_curve, auc, precision_recall_curve,
                             classification_report)
from sklearn.feature_selection import mutual_info_classif, SelectKBest
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline as ImbPipeline  # Bu √∂zel pipeline, SMOTE i≈ülemini sadece fit (eƒüitim) anƒ±nda yapar, transform (tahmin/validasyon) anƒ±nda yapmaz. B√∂ylece validasyon seti hep "temiz" ve "ger√ßek" kalƒ±r.
from imblearn.over_sampling import SMOTE
from scipy.stats import entropy, skellam, poisson
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
from scipy.ndimage import uniform_filter1d  # ‚úÖ Ekle


# Import b√∂l√ºm√ºne:
try:
    from pygad import GA
    HAS_PYGAD = True
except ImportError:
    HAS_PYGAD = False
    print("‚ö†Ô∏è pygad not installed. SOFI analysis will be skipped.")
# ‚úÖ YENƒ∞ EKLEME: SHAP i√ßin paralel hesaplama
from joblib import Parallel, delayed  # ‚Üê Zaten var ama emin olun

# Optional libraries
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False

try:
    import catboost as cb
    HAS_CB = True
except ImportError:
    HAS_CB = False

try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False

print("\n‚úÖ All libraries loaded successfully!\n")

warnings.filterwarnings("ignore")
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# ====================================================================
# ‚úÖ Y√úKSEK KALƒ∞TELƒ∞ MATPLOTLIB √áIKTI AYARLARI (GLOBAL)
# ====================================================================
plt.rcParams.update({
    "font.family": "serif",         # PDF uyumluluƒüu i√ßin
    "font.size": 12,
    "axes.labelsize": 12,
    "legend.fontsize": 12,
    "xtick.labelsize": 12,
    "ytick.labelsize": 12,
    "savefig.dpi": 600,             # Y√ºksek √ß√∂z√ºn√ºrl√ºk DPI
    "savefig.format": "pdf",        # √áƒ±ktƒ± formatƒ±nƒ± PDF olarak zorla (vekt√∂rel)
    "text.usetex": False,
    "figure.autolayout": True,

    # ‚úÖ BEYAZ ARKA PLAN ƒ∞√áƒ∞N EKLENEN SATIRLAR:
    "axes.facecolor": "white",
    "figure.facecolor": "white",
    "savefig.facecolor": "white",
})
print("‚úÖ Global Matplotlib ayarlarƒ± uygulandƒ± (Vekt√∂rel PDF √ßƒ±kƒ±≈üƒ± zorunlu kƒ±lƒ±ndƒ±).")
# ====================================================================

# Tez terminolojisi ile kod senkronizasyonu
TERMINOLOGY = {
    'methodology': {
        'validation': 'Temporal Split (Leakage-Free)',
        'tuning': 'Shared Tuning Protocol (Optuna + TimeSeriesSplit)',
        'cv': 'Time-Series Cross-Validation',
        'preprocessing': 'Leakage-Free Preprocessing Pipeline'
    },
    'phases': {
        'rq1': 'RQ1: Predictive Performance Evaluation',
        'rq2': 'RQ2: Explainability Analysis',
        'rq3': 'RQ3: Feature Importance Comparison'
    },
    'components': {
        'data_integration': 'Multi-Source Data Integration (Fuzzy Matching)',
        'feature_engineering': 'EDA-Guided Feature Engineering',
        'xai': 'Explainable AI Analysis (6 Methods)'
    }
}

# CONFIGURATION
CONFIG = {
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ PRE-TUNED MODE SETTINGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    "use_pretrained_params": True,  # ‚Üê Bu flag ile Optuna'yƒ± bypass et

    "best_params": {
        "lr": {
            "C": 0.005701256861337162
        },
        "rf": {
            "n_estimators": 85,
            "max_depth": 7,
            "min_samples_split": 14
        },
        "gb": {
            "max_iter": 204,
            "learning_rate": 0.011337064034222235,
            "max_leaf_nodes": 21
        },
        "xgboost": {
            "learning_rate": 0.01351182947645082,
            "max_depth": 4,
            "subsample": 0.6180909155642152,
            "colsample_bytree": 0.7301321323053057,
            "min_child_weight": 3,
            "gamma": 0.13567451588694796,
            "reg_alpha": 0.8287375091519293,
            "reg_lambda": 0.3567533266935893
        },
        "lightgbm": {
            "learning_rate": 0.01351182947645082,
            "num_leaves": 35,
            "max_depth": 3,
            "min_child_samples": 23,
            "subsample": 0.7554709158757928,
            "colsample_bytree": 0.7085396127095583,
            "reg_alpha": 0.8287375091519293,
            "reg_lambda": 0.3567533266935893
        },
        "catboost": {
            "learning_rate": 0.018659959624904916,
            "depth": 6,
            "l2_leaf_reg": 5.72280788469014,
            "border_count": 128,
            "bagging_temperature": 0.2912291401980419
        },
        "ada": {
            "n_estimators": 476,
            "learning_rate": 0.346005767142578
        },
        "ltcn": {
            "T": 35,
            "alpha": 0.21159508852163347,
            "beta": 0.3337157188281745,
            "ridge_alpha": 0.0002920433847181409
        }
    },

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Diƒüer CONFIG ayarlarƒ± (mevcut kodunuzdan)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    "filters": {
        "leagues": ["E0", "D1", "I1", "SP1", "F1"],
        "min_date": "2015-07-01"
    },
    "temporal": {
        "cutoff_year": 2015,
        "elo_tolerance_days": 30
    },
    "data_integration": {
        "enabled": True,
        "elo_tolerance_days": 30,
        "split_method": "temporal",
        "validation_desc": TERMINOLOGY['methodology']['validation'],
        "use_transfermarkt_features": True
    },
    "eda_integration": {
        "enabled": True,
        "strategy": "diff_only",
        "base_path": "EDA_Outputs_11.11.25 V7",
        "load_preprocessed_data": True,
        "preprocessed_data_path": "EDA_Outputs_11.11.25 V7/Data_Exports/merged_final_with_lags_cleaned.csv",
        "skip_lag_features": True,
        "skip_smart_selection": True,
        "skip_mi_recalculation": True,
        "feature_list_path": "EDA_Outputs_11.11.25 V7/Data_Exports/final_feature_list.txt",
        "dropped_features_path": "EDA_Outputs_11.11.25 V7/Data_Exports/features_dropped.csv",
        "mi_scores_path": "EDA_Outputs_11.11.25 V7/Data_Exports/02_feature_relevance_mutual_information_train.csv",
        "validation": {
            "check_feature_count": True,
            "expected_features": 83,
            "check_leakage": True,
            "check_data_shape": True
        }
    },
    "feature_toggles": {
        "use_feature_selection": False,
        "n_features_to_select": 20,
    },
    "cv": {
        "folds": 5,
        "method": "TimeSeriesSplit",  # ‚Üê YENƒ∞
         "desc": TERMINOLOGY['methodology']['cv']  # ‚Üê YENƒ∞
    },
    "optuna": {
        "n_trials": 20,
        "timeout": 2000,
        "desc": TERMINOLOGY['methodology']['tuning']  # ‚Üê YENƒ∞
    },
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ YENƒ∞ EKLEME: EARLY STOPPING CONFƒ∞GURATƒ∞ON
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    "early_stopping": {
        "enabled": True,              # Master switch - early stopping kullan
        "rounds": 50,                 # 50 iteration boyunca iyile≈üme yoksa dur
        "val_split": 0.15,            # Train setinin %15'i validation olarak ayrƒ±lƒ±r
        "verbose": False,             # Debug output g√∂sterme
        "models": ["xgboost", "lightgbm", "catboost"]  # Hangi modellere uygulanacak
    },
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    "graphics_to_skip": [14, 15, 17, "18a", 29],
    "models": ["lr", "rf", "gb", "xgboost", "lightgbm", "catboost", "ada", "ltcn"],
    "xai": {
        "n_samples": 100,
        "n_shap_background": 50,
        "n_jobs_shap": 16,
        "use_shap": True,
        "use_permutation": True,
        "pfi_n_repeats": 5,
        "ablation_max_features": 90,
        "random_state": 42,
    },
    "sofi": {
        "n_generations": 25,
        "sol_per_pop": 50,
        "mutation_probability": 0.05,
        "random_seed": 42
    },
    "smart_feature_selection": {
        "enabled": True,
        "mi_threshold": 0.001,
        "corr_threshold": 0.85,
        "fallback_to_external": True
    },
    "class_balancing": {
        "use_smote": True,
        "strategy": "targeted",
        "min_class_ratio": 0.25,
        "target_draw_ratio": 1.1,
        "k_neighbors": 5,
        "use_class_weights": True
    },
    "graphics": {
        "show_titles": False,  # ‚Üê Ba≈ülƒ±klarƒ± g√∂ster/gizle
        "clean_model_names": True,  # ‚Üê (Pre-trained) vb. kaldƒ±r
    },
    "eda_pipeline": {
        "use_elo_diff": True,
        "use_mi_selection": False,
        "mi_threshold": 0.04,
        "drop_list_path": "EDA_Outputs_11.11.25 V7/Data_Exports/09b_features_to_drop_code_REVISED.py",
        "mi_scores_path": "EDA_Outputs_11.11.25 V7/Data_Exports/02_feature_relevance_mutual_information_train.csv"
    }
}

SEED = 42
np.random.seed(SEED)
random.seed(SEED)

N_CLASSES = 3
RANDOM_STATE = 42

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ GLOBAL TITLE HELPER (T√ºm fonksiyonlarda kullanƒ±labilir)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def set_title_if_enabled(ax, title_text, **kwargs):
    """Set title only if CONFIG graphics.show_titles is True"""
    graphics_config = CONFIG.get("graphics", {})
    if graphics_config.get("show_titles", True):
        ax.set_title(title_text, **kwargs)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ LEAKAGE FEATURES DEFINITION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
LEAKAGE_FEATURES = [
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CRITICAL: In-game statistics that reveal match outcome
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    'HomeTarget',       # Home team shots on target
    'AwayTarget',       # Away team shots on target
    'HomeShot',
    'HomeShots',
    'AwayShots',
    'AwayShot',         # Away team total shots
    'HomeCorners',      # Home team corners
    'AwayCorners',      # Away team corners
    'HomeFouls',        # Home team fouls
    'AwayFouls',        # Away team fouls
    'HomeYellow',       # Home team yellow cards
    'AwayYellow',       # Away team yellow cards
    'HomeRed',          # Home team red cards
    'AwayRed',          # Away team red cards

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # OUTCOME FEATURES (should never be in feature set)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    'FTHome',           # Full-time home goals (TARGET LEAKAGE!)
    'FTAway',           # Full-time away goals (TARGET LEAKAGE!)
    'HTHome',           # Half-time home goals
    'HTAway',           # Half-time away goals
    'FTResult',         # Match result (HOME/DRAW/AWAY)
    'HTResult',         # Half-time result
]

DATASET_1_NAME = "Matches.csv"
DATASET_1_COLOR = "#3498db"
DATASET_2_NAME = "ELO Ratings"
DATASET_2_COLOR = "#e74c3c"
DATASET_3_NAME = "Transfermarkt"
DATASET_3_COLOR = "#2ecc71"
DATASET_4_NAME = "Derived/Other"
DATASET_4_COLOR = "#f39c12"

DATASETS = {
    DATASET_1_NAME: {'color': DATASET_1_COLOR},
    DATASET_2_NAME: {'color': DATASET_2_COLOR},
    DATASET_3_NAME: {'color': DATASET_3_COLOR},
    DATASET_4_NAME: {'color': DATASET_4_COLOR}
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé® THESIS-COMPLIANT COLOR PALETTE (Global Definition)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

THESIS_COLORS = {
    'primary': {
        'dark_blue': '#1f3a93',      # Ana mavi (en koyu)
        'medium_blue': '#2e59d9',    # Orta mavi
        'light_blue': '#6c8cd5',     # A√ßƒ±k mavi
        'sky_blue': '#a4c2f4',       # √áok a√ßƒ±k mavi
    },
    'accent': {
        'highlight': '#ffa500',      # Turuncu (vurgu i√ßin)
        'warning': '#ff6b6b',        # A√ßƒ±k kƒ±rmƒ±zƒ± (uyarƒ±)
    },
    'neutral': {
        'dark_gray': '#2c3e50',      # Koyu gri (text)
        'medium_gray': '#7f8c8d',    # Orta gri
        'light_gray': '#ecf0f1',     # A√ßƒ±k gri (background)
    }
}


# FILE PATHS
try:
    from google.colab import drive
    print("üîß Google Colab tespit edildi - Google Drive mount ediliyor...")
    drive.mount('/content/drive', timeout_ms=120000, force_remount=False)
    BASE_PATH = "/content/drive/My Drive/Thesis Data/"
    print("‚úÖ Google Drive ba≈üarƒ±yla mount edildi!\n")
except ImportError:
    print("‚ö†Ô∏è Google Colab bulunamadƒ± - Yerel ortam kullanƒ±lƒ±yor\n")
    BASE_PATH = "./Thesis Data/"
    os.makedirs(BASE_PATH, exist_ok=True)
except Exception as e:
    print(f"‚ö†Ô∏è Google Drive mount hatasƒ±: {e}")
    print("Yerel ortama ge√ßi≈ü yapƒ±lƒ±yor...\n")
    BASE_PATH = "./Thesis Data/"
    os.makedirs(BASE_PATH, exist_ok=True)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üÜï LOAD EDA-GUIDED ARTIFACTS (FULLY FIXED)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

eda_config = CONFIG.get("eda_integration", {})

if eda_config.get("enabled", False):
    print("\n[EDA-GUIDED] Loading EDA artifacts...")

    try:
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ PATH CONSTRUCTION
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # Base path (eda_integration'dan)
        eda_base = os.path.join(BASE_PATH, eda_config.get("base_path", ""))

        # Drop list path (eda_pipeline'dan - √ß√ºnk√º orada .py dosyasƒ± var!)
        pipeline_config = CONFIG.get("eda_pipeline", {})
        drop_list_file = os.path.join(BASE_PATH, pipeline_config.get("drop_list_path", ""))

        # MI scores path (eda_integration'dan)
        mi_scores_file = os.path.join(BASE_PATH, eda_config.get("mi_scores_path", ""))

        print(f"\n  üìÅ Paths:")
        print(f"     EDA Base:   {eda_base}")
        print(f"     Drop list:  {drop_list_file}")
        print(f"     MI scores:  {mi_scores_file}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ LOAD DROP LIST (FIXED: Global scope i√ßin)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # Default values (eƒüer dosya yoksa)
        features_to_drop_multicollinearity_REVISED = []
        features_to_keep_diff = []

        if os.path.exists(drop_list_file):
            print(f"\n  üîÑ Loading feature drop list: {os.path.basename(drop_list_file)}")

            try:
                # ‚úÖ FIX: exec() yerine compile + exec with globals()
                with open(drop_list_file, 'r', encoding='utf-8') as f:
                    code_content = f.read()

                # Global namespace'e y√ºkle
                exec_globals = {}
                exec(compile(code_content, drop_list_file, 'exec'), exec_globals)

                # Deƒüi≈ükenleri al
                if 'features_to_drop_multicollinearity_REVISED' in exec_globals:
                    features_to_drop_multicollinearity_REVISED = exec_globals['features_to_drop_multicollinearity_REVISED']
                    print(f"  ‚úÖ Features to drop: {len(features_to_drop_multicollinearity_REVISED)}")

                    # Sample features g√∂ster
                    if len(features_to_drop_multicollinearity_REVISED) > 0:
                        print(f"     Sample (first 5):")
                        for feat in list(features_to_drop_multicollinearity_REVISED)[:5]:
                            print(f"       ‚Ä¢ {feat}")
                else:
                    print(f"  ‚ö†Ô∏è  features_to_drop_multicollinearity_REVISED not found")

                if 'features_to_keep_diff' in exec_globals:
                    features_to_keep_diff = exec_globals['features_to_keep_diff']
                    print(f"  ‚úÖ Diff features to keep: {len(features_to_keep_diff)}")

                    # Sample diff features
                    if len(features_to_keep_diff) > 0:
                        print(f"     Sample:")
                        for feat in features_to_keep_diff[:5]:
                            print(f"       ‚Ä¢ {feat}")
                else:
                    print(f"  ‚ö†Ô∏è  features_to_keep_diff not found")

            except Exception as e_drop:
                print(f"  ‚ùå Error executing drop list file: {e_drop}")
                import traceback
                traceback.print_exc()
        else:
            print(f"  ‚ö†Ô∏è  Drop list not found: {drop_list_file}")
            print(f"     File exists: {os.path.exists(drop_list_file)}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ LOAD MI SCORES
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        mi_scores_eda_df = None

        if os.path.exists(mi_scores_file):
            print(f"\n  üîÑ Loading MI scores: {os.path.basename(mi_scores_file)}")

            try:
                mi_scores_eda_df = pd.read_csv(mi_scores_file)
                print(f"  ‚úÖ Loaded MI scores for {len(mi_scores_eda_df)} features")

                # Column kontrol√º
                print(f"     Columns: {list(mi_scores_eda_df.columns)}")

                if len(mi_scores_eda_df) > 0:
                    # Column adƒ±nƒ± kontrol et (Feature vs feature vs MI_Feature)
                    feature_col = None
                    score_col = None

                    for col in mi_scores_eda_df.columns:
                        if 'feature' in col.lower():
                            feature_col = col
                        if 'mi' in col.lower() or 'score' in col.lower():
                            score_col = col

                    if feature_col and score_col:
                        top_feature = mi_scores_eda_df.iloc[0]
                        print(f"  ‚ÑπÔ∏è  Top feature: {top_feature[feature_col]} ({score_col}={top_feature[score_col]:.4f})")
                    else:
                        print(f"  ‚ö†Ô∏è  Could not identify feature/score columns")
                        # Fallback: ilk iki column'u kullan
                        if len(mi_scores_eda_df.columns) >= 2:
                            print(f"     Using first two columns as feature/score")

            except Exception as e_mi:
                print(f"  ‚ùå Error loading MI scores: {e_mi}")
                import traceback
                traceback.print_exc()
        else:
            print(f"  ‚ö†Ô∏è  MI scores not found: {mi_scores_file}")
            print(f"     File exists: {os.path.exists(mi_scores_file)}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ SUMMARY
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        print(f"\n  üìä EDA Artifacts Summary:")
        print(f"     Features to drop:  {len(features_to_drop_multicollinearity_REVISED)}")
        print(f"     Diff features:     {len(features_to_keep_diff)}")
        print(f"     MI scores loaded:  {'‚úÖ Yes' if mi_scores_eda_df is not None else '‚ùå No'}")
        print(f"\n  ‚úÖ EDA artifacts loading complete\n")

    except Exception as e:
        print(f"  ‚ùå Critical error loading EDA artifacts: {e}")
        print(f"  Continuing without EDA guidance...\n")

        # Full traceback
        import traceback
        print(f"  üìã Full error:")
        traceback.print_exc()
        print()

        # Fallback values
        features_to_drop_multicollinearity_REVISED = []
        features_to_keep_diff = []
        mi_scores_eda_df = None

else:
    print("\n[EDA-GUIDED] EDA integration disabled in CONFIG\n")
    features_to_drop_multicollinearity_REVISED = []
    features_to_keep_diff = []
    mi_scores_eda_df = None

print("="*100 + "\n")

MATCHES_PATH = os.path.join(BASE_PATH, "Matches.csv")
ELOR_PATH = os.path.join(BASE_PATH, "EloRatings.csv")
TEAM_FEATURES_PATH = os.path.join(BASE_PATH, "data.xlsx")
TEAM_LIST_PATH = os.path.join(BASE_PATH, "Takim_Listesi_Temiz_1.csv")

print("="*100)
print("üìÅ DOSYA YOLLARI VE KONTROL")
print("="*100)
print(f"üìÅ BASE_PATH: {BASE_PATH}")
print(f"üìÑ MATCHES_PATH: {MATCHES_PATH}")
print(f"   Status: {'‚úÖ BULUNDU' if os.path.exists(MATCHES_PATH) else '‚ùå BULUNAMADI'}")
print(f"üìÑ ELOR_PATH: {ELOR_PATH}")
print(f"   Status: {'‚úÖ BULUNDU' if os.path.exists(ELOR_PATH) else '‚ùå BULUNAMADI'}")
print(f"üìÑ TEAM_FEATURES_PATH: {TEAM_FEATURES_PATH}")
print(f"   Status: {'‚úÖ BULUNDU' if os.path.exists(TEAM_FEATURES_PATH) else '‚ùå BULUNAMADI'}")
print(f"\n[OUTPUT] {BASE_PATH} ODDS Plus SMOTE DRAW 1.1 V2 27.11.25 w1 approach ltcn   Main Model\n")

OUT_DIR = os.path.join(BASE_PATH, " ODDS Plus SMOTE DRAW 1.1 V2 27.11.25 w1 approach ltcn   Main Model")
DATA_DIR = os.path.join(OUT_DIR, "data")
GRAPHICS_DIR = os.path.join(OUT_DIR, "graphics")
TABLES_DIR = os.path.join(OUT_DIR, "tables")

for d in [OUT_DIR, DATA_DIR, GRAPHICS_DIR, TABLES_DIR]:
    os.makedirs(d, exist_ok=True)

print("="*100 + "\n")

# DATA INTEGRATION CLASS
class DataIntegration:
    """Advanced data integration with fuzzy matching"""

    def __init__(self, base_path=BASE_PATH):
        self.base_path = base_path
        self.df_matches = None
        self.df_elo = None
        self.df_tm = None
        self.team_mapping = {}
        self.team_scores = {}

        def _helper_normalize_for_key(s):
            if pd.isna(s):
                return None
            s = str(s).strip()
            s = unidecode(s)

            suffixes = [
                ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc', ' SK', ' sk', ' Sk',
                ' FK', ' fk', ' Fk', ' AC', ' ac', ' Ac', ' AS', ' as', ' As',
                ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf', ' HSC', ' hsc', ' Hsc',
                ' BC', ' bc', ' Bc', ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',
                ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',
                ' 1909', ' 1846', ' 1860', ' 2010', ' 1913', ' 1907', ' 1936', ' 1919', ' 2025',
                ' spor', ' kul√ºb√º', ' klub',
            ]

            for suffix in suffixes:
                s = s.replace(suffix, '')

            s = s.replace("'", "")
            s = s.replace("(", "").replace(")", "")
            words = sorted(s.lower().split())
            s = ' '.join(words)
            s = re.sub(r'\s+', ' ', s).strip()
            return s if s else None

        self.MANUAL_OVERRIDES = {}
        team_list_csv_path = TEAM_LIST_PATH

        try:
            encoding_options = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            df_temp = None
            for encoding in encoding_options:
                try:
                    df_temp = pd.read_csv(
                        team_list_csv_path,
                        header=None,
                        skiprows=1,
                        quotechar='"',
                        sep='|',
                        encoding=encoding
                    )
                    print(f"‚úÖ Dosya ba≈üarƒ±yla '{encoding}' ile okundu")
                    break
                except UnicodeDecodeError:
                    continue

            if df_temp is None:
                raise Exception("CSV dosyasƒ± hi√ßbir encoding ile okunamadƒ±!")

            df_team_list = df_temp[0].str.split('\t', expand=True)
            df_team_list.columns = ['Matches', 'Transfermarkt']

            def _process_team_row(row):
                matches_name = row['Matches']
                transfermarkt_name = row['Transfermarkt']

                if pd.notna(matches_name) and pd.notna(transfermarkt_name):
                    normalized_key = _helper_normalize_for_key(matches_name)
                    normalized_value = _helper_normalize_for_key(transfermarkt_name)

                    if normalized_key and normalized_value:
                        return normalized_key, normalized_value
                return None

            for result in df_team_list.apply(_process_team_row, axis=1):
                if result is not None:
                    key, value = result
                    self.MANUAL_OVERRIDES[key] = value

            print("Man√ºel kod i√ßi override kurallarƒ± uygulanƒ±yor...")

            tm_ac_ajaccio_normalized = _helper_normalize_for_key("AC Ajaccio")
            tm_gfc_ajaccio_normalized = _helper_normalize_for_key("GFC Ajaccio")
            tm_cordoba_normalized = _helper_normalize_for_key("C√≥rdoba CF")

            key_aja = _helper_normalize_for_key("Ajaccio")
            if key_aja and tm_ac_ajaccio_normalized:
                self.MANUAL_OVERRIDES.pop(key_aja, None)
                self.MANUAL_OVERRIDES[key_aja] = tm_ac_ajaccio_normalized
                print(f"  Override: '{key_aja}' -> '{tm_ac_ajaccio_normalized}' (AC Ajaccio)")

            key_gfco = _helper_normalize_for_key("Ajaccio GFCO")
            if key_gfco and tm_gfc_ajaccio_normalized:
                self.MANUAL_OVERRIDES[key_gfco] = tm_gfc_ajaccio_normalized
                print(f"  Override: '{key_gfco}' -> '{tm_gfc_ajaccio_normalized}' (GFC Ajaccio)")

            key_cor = _helper_normalize_for_key("Cordoba")
            if key_cor and tm_cordoba_normalized:
                self.MANUAL_OVERRIDES[key_cor] = tm_cordoba_normalized
                print(f"  Override: '{key_cor}' -> '{tm_cordoba_normalized}' (C√≥rdoba CF)")

            print(f"[OVERRIDE RULES] {len(self.MANUAL_OVERRIDES)} takƒ±m CSV dosyasƒ±ndan dinamik ve normalize olarak tanƒ±mlandƒ±\n")

        except FileNotFoundError:
            print(f"‚ùå UYARI: Takƒ±m listesi CSV'si bulunamadƒ±: {team_list_csv_path}")
            print("Manuel override listesi bo≈ü olacak! E≈üle≈üme oranƒ± √ßok d√º≈ü√ºk kalacak.")
        except Exception as e:
            print(f"‚ùå HATA: Takƒ±m listesi CSV'si okunurken hata: {e}")
            print("Manuel override listesi bo≈ü olabilir.")

    def load_data(self):
        """Load all three data sources"""
        print("[STEP 1] üìä VERƒ∞ Y√úKLEME\n")

        try:
            print("üìÑ Matches y√ºkl√ºyor...")
            self.df_matches = pd.read_csv(os.path.join(self.base_path, "Matches.csv"), low_memory=False)
            print(f"  ‚úì {len(self.df_matches):,} satƒ±r, {len(self.df_matches.columns)} s√ºtun")

            print("\nüìÑ ELO Ratings y√ºkl√ºyor...")
            self.df_elo = pd.read_csv(os.path.join(self.base_path, "EloRatings.csv"), low_memory=False)
            print(f"  ‚úì {len(self.df_elo):,} satƒ±r, {len(self.df_elo.columns)} s√ºtun")

            print("\nüìÑ Transfermarkt y√ºkl√ºyor...")
            self.df_tm = pd.read_excel(os.path.join(self.base_path, "data.xlsx"))
            print(f"  ‚úì {len(self.df_tm):,} satƒ±r, {len(self.df_tm.columns)} s√ºtun")

        except Exception as e:
            print(f"\n‚ùå HATA: {e}")
            import traceback
            traceback.print_exc()
            raise

    def normalize_team_name(self, s):
        """Normalize team names for matching"""
        if pd.isna(s):
            return None

        s = str(s).strip()
        s = unidecode(s)

        suffixes = [
            ' FC', ' fc', ' Fc', ' fC', ' SC', ' sc', ' Sc', ' SK', ' sk', ' Sk',
            ' FK', ' fk', ' Fk', ' AC', ' ac', ' Ac', ' AS', ' as', ' As',
            ' SS', ' ss', ' Ss', ' CF', ' cf', ' Cf', ' HSC', ' hsc', ' Hsc',
            ' BC', ' bc', ' Bc', ' SSC', ' ssc', ' Ssc', ' US', ' us', ' Us',
            ' CA', ' ca', ' Ca', ' UD', ' ud', ' Ud',
            ' 1909', ' 1846', ' 1860', ' 2010', ' 1913', ' 1907', ' 1936', ' 1919', ' 2025',
            ' spor', ' kul√ºb√º', ' klub',
        ]

        for suffix in suffixes:
            s = s.replace(suffix, '')

        s = s.replace("'", "")
        s = s.replace("(", "").replace(")", "")
        words = sorted(s.lower().split())
        s = ' '.join(words)
        s = re.sub(r'\s+', ' ', s).strip()

        if s in self.MANUAL_OVERRIDES:
            s = self.MANUAL_OVERRIDES[s]

        return s if s else None

    def find_best_match(self, match_team, tm_teams, threshold=80):
        """Find best team match using fuzzy matching"""
        if pd.isna(match_team):
            return None, 0

        match_norm = self.normalize_team_name(match_team)

        if not match_norm:
            return None, 0

        tm_norm_dict = {norm: orig for orig, norm in tm_teams if norm}

        if match_norm in tm_norm_dict:
            return tm_norm_dict[match_norm], 100

        best_score = 0
        best_match = None

        for tm_team_orig, tm_norm in tm_teams:
            if not tm_norm:
                continue

            score = fuzz.token_sort_ratio(match_norm, tm_norm)
            if score > best_score:
                best_score = score
                best_match = tm_team_orig

        if best_score >= threshold:
            return best_match, best_score
        return None, best_score

    def create_team_mapping(self):
        """Create team mapping between Matches and Transfermarkt"""
        print("\n" + "-"*100)
        print("[STEP 2] üîó TAKIMLAR E≈ûLE≈ûTIRILMESI (FUZZY MATCHING)\n")

        matches_teams = sorted(
            set(self.df_matches['HomeTeam'].dropna().unique()) | \
            set(self.df_matches['AwayTeam'].dropna().unique())
        )

        tm_teams_orig = sorted(self.df_tm['ClubName'].dropna().unique())

        tm_teams_normalized = []
        for team in tm_teams_orig:
            tm_teams_normalized.append((team, self.normalize_team_name(team)))

        tm_teams_norm_set = set(n for o, n in tm_teams_normalized if n)

        print(f"Matches'te unique takƒ±mlar: {len(matches_teams)}")
        print(f"Transfermarkt'ta unique takƒ±mlar: {len(tm_teams_orig)}\n")

        print("üîÑ Takƒ±m adƒ± e≈üle≈ütirmesi yapƒ±lƒ±yor...\n")

        for idx, team in enumerate(matches_teams):
            if idx % 50 == 0 and idx > 0:
                print(f"  Progress: {idx}/{len(matches_teams)}", end='\r')

            mapped_team, score = self.find_best_match(team, tm_teams_normalized, threshold=80)

            self.team_mapping[team] = mapped_team
            self.team_scores[team] = score

        print(f"   ‚úì Mapping tamamlandƒ± ({len(matches_teams)} takƒ±m)                 \n")

        matched_count = sum(1 for v in self.team_mapping.values() if v is not None)
        print(f"‚úÖ E≈üle≈ütirilen takƒ±mlar: {matched_count}/{len(self.team_mapping)} ({matched_count/len(self.team_mapping)*100:.1f}%)")

        unmatched = {k: v for k, v in self.team_mapping.items() if v is None}
        if unmatched:
            print(f"\n‚ö†Ô∏è E≈üle≈ümeyen takƒ±mlar ({len(unmatched)}):")
            unmatched_sorted = sorted(unmatched.keys())
            for i, team in enumerate(unmatched_sorted[:15]):
                print(f"  ‚Ä¢ {team}")
            if len(unmatched) > 15:
                print(f"  ... ve {len(unmatched)-15} daha")

    def clean_and_filter_data(self):
        """Clean and filter data"""
        print("\n" + "-"*100)
        print("[STEP 3] üîß VERƒ∞ TEMƒ∞ZLEME VE Fƒ∞LTRELEME\n")

        print("Matches temizleniyor...")
        self.df_matches['MatchDate'] = pd.to_datetime(self.df_matches['MatchDate'], errors='coerce')

        TOP_5_DIVISIONS = ['D1', 'F1', 'E0', 'I1', 'SP1']
        before = len(self.df_matches)
        self.df_matches = self.df_matches[self.df_matches['Division'].isin(TOP_5_DIVISIONS)].copy()
        after = len(self.df_matches)
        print(f"   TOP 5 Lƒ∞G filtresi: {before:,} ‚Üí {after:,}")

        before = len(self.df_matches)
        self.df_matches = self.df_matches[self.df_matches['MatchDate'] >= '2015-01-01'].copy()
        after = len(self.df_matches)
        print(f"   Tarih filtresi (2015+): {before:,} ‚Üí {after:,}")

        before = len(self.df_matches)
        self.df_matches = self.df_matches.dropna(subset=['HomeTeam', 'AwayTeam', 'MatchDate']).copy()
        after = len(self.df_matches)
        print(f"   Null takƒ±mlar/tarihler kaldƒ±rƒ±ldƒ±: {before:,} ‚Üí {after:,}")
        print(f"   ‚úÖ Final Matches: {len(self.df_matches):,} ma√ß\n")

        print("Transfermarkt temizleniyor...")
        TOP_5_LEAGUES_TM = ['ES1', 'FR1', 'GB1', 'IT1', 'L1']
        before = len(self.df_tm)
        self.df_tm = self.df_tm[self.df_tm['wettbewerb_id'].isin(TOP_5_LEAGUES_TM)].copy()
        after = len(self.df_tm)
        print(f"   TOP 5 Lƒ∞G filtresi: {before:,} ‚Üí {after:,}")

        before = len(self.df_tm)
        self.df_tm = self.df_tm.dropna(subset=['ClubName']).copy()
        after = len(self.df_tm)
        print(f"   Null ClubName kaldƒ±rƒ±ldƒ±: {before:,} ‚Üí {after:,}")
        print(f"   ‚úÖ Final Transfermarkt: {len(self.df_tm):,} satƒ±r\n")

    def add_season_info(self):
        """Add season info to matches"""
        print("[STEP 4] ‚è∞ SEZON VE TARIH Bƒ∞LGƒ∞Sƒ∞ EKLEME\n")

        self.df_matches['Season'] = self.df_matches['MatchDate'].dt.year.astype(str) + '-' + \
                                    (self.df_matches['MatchDate'].dt.year + 1).astype(str)

        mask = self.df_matches['MatchDate'].dt.month < 7
        self.df_matches.loc[mask, 'Season'] = \
            (self.df_matches.loc[mask, 'MatchDate'].dt.year - 1).astype(str) + '-' + \
            self.df_matches.loc[mask, 'MatchDate'].dt.year.astype(str)

        self.df_matches['YearMonth'] = self.df_matches['MatchDate'].dt.to_period('M').astype(str)

        print(f"Unique seasons (sample): {sorted(self.df_matches['Season'].unique())[:10]}\n")
        print(f"Unique YearMonths (sample): {sorted(self.df_matches['YearMonth'].unique())[:12]}\n")
        print(f"‚úÖ Season ve YearMonth eklendi\n")

    def merge_data(self):
        """Merge all data sources"""
        print("="*100)
        print("[STEP 5] üîÄ MERGE ƒ∞≈ûLEMƒ∞\n")

        self.load_data()

        if self.df_tm.columns.duplicated().any():
            print("[FIX] ‚ö†Ô∏è Transfermarkt'ta duplicate column names tespit edildi, temizleniyor...")
            cols = pd.Series(self.df_tm.columns)
            for dup in cols[cols.duplicated(keep=False)].unique():
                dups = np.where(cols == dup)[0]
                cols.iloc[dups] = [f"{dup}_{i}" for i in range(len(dups))]
            self.df_tm.columns = cols
            print(f"    ‚úÖ Temizlendi!\n")

        self.clean_and_filter_data()
        self.create_team_mapping()
        self.add_season_info()

        print("[MERGE] Fuzzy mapping uygulanƒ±yor...\n")
        self.df_matches['HomeTeam_TM'] = self.df_matches['HomeTeam'].map(self.team_mapping)
        self.df_matches['AwayTeam_TM'] = self.df_matches['AwayTeam'].map(self.team_mapping)

        unmapped_matches = self.df_matches[
            (self.df_matches['HomeTeam_TM'].isna()) | (self.df_matches['AwayTeam_TM'].isna())
        ]
        print(f"E≈üle≈ümeyen ma√ßlar: {len(unmapped_matches):,}\n")

        print("[FIX] Level 1 (YearMonth) verisi hazƒ±rlanƒ±yor...")
        df_tm_yearmonth = self.df_tm.drop_duplicates(
            subset=['ClubName', 'YearMonth'],
            keep='last'
        ).copy()
        print(f"   ‚úì {len(df_tm_yearmonth):,} unique satƒ±r")

        print("\n[FIX] Level 2 (Season) verisi hazƒ±rlanƒ±yor...")

        numeric_tm_cols = self.df_tm.select_dtypes(include=np.number).columns
        numeric_tm_cols = [c for c in numeric_tm_cols if c not in ['club_id']]

        non_numeric_tm_cols = self.df_tm.select_dtypes(exclude=np.number).columns.tolist()

        if 'club_id' in self.df_tm.columns and 'club_id' not in non_numeric_tm_cols:
            non_numeric_tm_cols.append('club_id')

        non_numeric_tm_cols = [c for c in non_numeric_tm_cols if c not in ['ClubName', 'Season', 'YearMonth', 'wettbewerb_id', 'LeagueName']]

        agg_dict = {}
        for col in numeric_tm_cols:
            agg_dict[col] = 'mean'
        for col in non_numeric_tm_cols:
            if col in self.df_tm.columns:
                agg_dict[col] = 'first'

        grouping_cols = ['ClubName', 'Season']
        valid_agg_dict = {k: v for k, v in agg_dict.items() if k in self.df_tm.columns}

        df_tm_season = self.df_tm.groupby(grouping_cols, as_index=False).agg(valid_agg_dict)
        print(f"   ‚úì {len(df_tm_season):,} unique satƒ±r\n")

        df_merged = self.df_matches.copy()

        print("   üè† [LEVEL 1] HomeTeam merge...")
        df_tm_home_l1 = df_tm_yearmonth.rename(columns={'ClubName': 'HomeTeam_TM'})

        df_merged = df_merged.merge(
            df_tm_home_l1,
            on=['HomeTeam_TM', 'YearMonth'],
            how='left',
            suffixes=('', '_home_tm')
        )

        home_cols = [col for col in df_tm_home_l1.columns if col not in ['HomeTeam_TM', 'YearMonth']]

        unmatched_l1_mask = pd.Series(False, index=df_merged.index)
        if home_cols:
            unmatched_l1_mask = df_merged[home_cols[0]].isna()
        unmatched_l1_indices = df_merged[unmatched_l1_mask].index

        if len(unmatched_l1_indices) > 0:
            print(f"       [LEVEL 2] {len(unmatched_l1_indices):,} satƒ±rda Season fallback...")

            fallback_data = df_merged.loc[unmatched_l1_indices, ['HomeTeam_TM', 'Season']]
            df_tm_home_l2 = df_tm_season.rename(columns={'ClubName': 'HomeTeam_TM'})

            fallback_merged = fallback_data.merge(
                df_tm_home_l2,
                on=['HomeTeam_TM', 'Season'],
                how='left',
                suffixes=('', '_home_tm')
            )

            for col in home_cols:
                if col in fallback_merged.columns:
                    fallback_merged.index = fallback_data.index
                    df_merged.loc[unmatched_l1_indices, col] = fallback_merged[col]

        non_null_home = df_merged[home_cols[0]].notna().sum() if home_cols else 0
        print(f"       Non-null HomeTeam: {non_null_home:,}")

        rename_dict_home = {col: 'HomeTeam_' + col for col in home_cols if col in df_merged.columns}
        df_merged = df_merged.rename(columns=rename_dict_home)

        print("   ‚úàÔ∏è  [LEVEL 1] AwayTeam merge...")
        df_tm_away_l1 = df_tm_yearmonth.rename(columns={'ClubName': 'AwayTeam_TM'})

        df_merged = df_merged.merge(
            df_tm_away_l1,
            on=['AwayTeam_TM', 'YearMonth'],
            how='left',
            suffixes=('', '_away_tm')
        )

        away_cols = [col for col in df_tm_away_l1.columns if col not in ['AwayTeam_TM', 'YearMonth']]

        unmatched_l1_mask_away = pd.Series(False, index=df_merged.index)
        if away_cols:
            away_check_col = away_cols[0]
            if away_check_col + '_away_tm' in df_merged.columns:
                away_check_col = away_check_col + '_away_tm'
            elif away_check_col not in df_merged.columns:
                for c in away_cols[1:]:
                    if c in df_merged.columns:
                        away_check_col = c
                        break
                    elif c + '_away_tm' in df_merged.columns:
                        away_check_col = c + '_away_tm'
                        break
                else:
                    away_check_col = None

            if away_check_col:
                unmatched_l1_mask_away = df_merged[away_check_col].isna()

        unmatched_l1_indices_away = df_merged[unmatched_l1_mask_away].index

        if len(unmatched_l1_indices_away) > 0:
            print(f"       [LEVEL 2] {len(unmatched_l1_indices_away):,} satƒ±rda Season fallback...")

            fallback_data_away = df_merged.loc[unmatched_l1_indices_away, ['AwayTeam_TM', 'Season']]
            df_tm_away_l2 = df_tm_season.rename(columns={'ClubName': 'AwayTeam_TM'})

            fallback_merged_away = fallback_data_away.merge(
                df_tm_away_l2,
                on=['AwayTeam_TM', 'Season'],
                how='left',
                suffixes=('', '_away_tm_l2')
            )

            for col in away_cols:
                target_col = col
                source_col = col

                if col + '_away_tm' in df_merged.columns:
                    target_col = col + '_away_tm'
                if col + '_away_tm_l2' in fallback_merged_away.columns:
                    source_col = col + '_away_tm_l2'

                if source_col in fallback_merged_away.columns and target_col in df_merged.columns:
                    fallback_merged_away.index = fallback_data_away.index
                    df_merged.loc[unmatched_l1_indices_away, target_col] = fallback_merged_away[source_col]

        non_null_away = 0
        if away_cols:
            away_check_col = away_cols[0]
            if away_check_col + '_away_tm' in df_merged.columns:
                away_check_col = away_check_col + '_away_tm'
            elif away_check_col not in df_merged.columns:
                away_check_col = None
            if away_check_col:
                non_null_away = df_merged[away_check_col].notna().sum()
        print(f"       Non-null AwayTeam: {non_null_away:,}")

        rename_dict_away = {}
        for col in away_cols:
            suffixed_col = col + '_away_tm'
            if suffixed_col in df_merged.columns:
                rename_dict_away[suffixed_col] = 'AwayTeam_' + col
            elif col in df_merged.columns and not col.startswith('HomeTeam_'):
                rename_dict_away[col] = 'AwayTeam_' + col

        df_merged = df_merged.rename(columns=rename_dict_away)
        df_merged = df_merged.sort_values('MatchDate').reset_index(drop=True)

        print(f"\n   ‚úÖ Merge tamamlandƒ±: {len(df_merged):,} √ó {len(df_merged.columns)}\n")

        print("[FEATURE ENGINEERING]")

        if 'HomeElo' in df_merged.columns and 'AwayElo' in df_merged.columns:
            df_merged['ELO_Diff'] = df_merged['HomeElo'] - df_merged['AwayElo']
            print("‚úì ELO_Diff olu≈üturuldu")
        else:
            print("‚úó ELO_Diff olu≈üturulamadƒ±")

        def create_diff_feature(df, feature_base_name):
            home_col = f'HomeTeam_{feature_base_name}'
            away_col = f'AwayTeam_{feature_base_name}'
            diff_col = f'{feature_base_name}_Diff'
            if home_col in df.columns and away_col in df.columns:
                df[diff_col] = df[home_col] - df[away_col]
                if 'Manager' in feature_base_name:
                    df[diff_col] = df[diff_col].fillna(0)
                print(f"‚úì {diff_col} olu≈üturuldu")
            else:
                print(f"‚úó {diff_col} olu≈üturulamadƒ±")

        create_diff_feature(df_merged, 'ClubValue')
        create_diff_feature(df_merged, 'MaxPlayerValue')
        create_diff_feature(df_merged, 'ManagerTrophies')
        create_diff_feature(df_merged, 'ManagerTenureDays')
        create_diff_feature(df_merged, 'NetTransferSpending')
        create_diff_feature(df_merged, 'n_players_injured')

        print("\n[INFO] Creating ValueRatio features...")
        try:
            df_merged['HomeTeam_ValueRatio'] = (
                df_merged['HomeTeam_ClubValue'] / df_merged['HomeTeam_LeagueValue'].replace(0, np.nan)
            ).fillna(0)

            df_merged['AwayTeam_ValueRatio'] = (
                df_merged['AwayTeam_ClubValue'] / df_merged['AwayTeam_LeagueValue'].replace(0, np.nan)
            ).fillna(0)

            df_merged['ValueRatio_Diff'] = df_merged['HomeTeam_ValueRatio'] - df_merged['AwayTeam_ValueRatio']
            print("‚úì ValueRatio features created")
        except Exception as e:
            print(f"‚úó ValueRatio error: {e}")

        print("\n[MISSING VALUES HANDLING]")
        numeric_cols = df_merged.select_dtypes(include=[np.number]).columns

        cols_to_exclude_fill = ['FTHome', 'FTAway', 'HTHome', 'HTAway',
                                'HomeTeam_club_id', 'AwayTeam_club_id']

        for col in numeric_cols:
            if col in cols_to_exclude_fill:
                continue

            if df_merged[col].isnull().sum() > 0:
                median_val = df_merged[col].median()
                if pd.isna(median_val):
                    median_val = 0
                df_merged[col] = df_merged[col].fillna(median_val)

        print("‚úì Missing values filled\n")

        print("\n[NORMALIZED ODDS]")
        odds_cols_main = ['OddHome', 'OddDraw', 'OddAway']
        if all(c in df_merged.columns for c in odds_cols_main):
            try:
                df_merged['Prob_H_Raw'] = np.where(df_merged['OddHome'].fillna(0) > 0, 1 / df_merged['OddHome'], np.nan)
                df_merged['Prob_D_Raw'] = np.where(df_merged['OddDraw'].fillna(0) > 0, 1 / df_merged['OddDraw'], np.nan)
                df_merged['Prob_A_Raw'] = np.where(df_merged['OddAway'].fillna(0) > 0, 1 / df_merged['OddAway'], np.nan)

                df_merged['Overround'] = df_merged[['Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw']].fillna(0).sum(axis=1)

                valid_overround_mask = (df_merged['Overround'].notna()) & (df_merged['Overround'] > 0.1)
                df_merged['Prob_H_Norm'] = np.where(valid_overround_mask, df_merged['Prob_H_Raw'] / df_merged['Overround'], df_merged['Prob_H_Raw'])
                df_merged['Prob_D_Norm'] = np.where(valid_overround_mask, df_merged['Prob_D_Raw'] / df_merged['Overround'], df_merged['Prob_D_Raw'])
                df_merged['Prob_A_Norm'] = np.where(valid_overround_mask, df_merged['Prob_A_Raw'] / df_merged['Overround'], df_merged['Prob_A_Raw'])

                df_merged = df_merged.drop(columns=['Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw', 'Overround'], errors='ignore')
                print("‚úì Normalized odds created")

                norm_odds_cols = ['Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm']
                for col in norm_odds_cols:
                    if col in df_merged.columns and df_merged[col].isnull().any():
                        median_val_odds = df_merged[col].median()
                        if pd.isna(median_val_odds):
                            median_val_odds = 1/3
                        df_merged[col] = df_merged[col].fillna(median_val_odds)

            except Exception as e_odds_main:
                print(f"‚úó Odds error: {e_odds_main}")
        else:
            print("‚úó Odds columns not found")

        print("="*100)
        print("‚úÖ DATA INTEGRATION COMPLETE\n")
        print(f"Final dataset: {len(df_merged):,} √ó {len(df_merged.columns)}")

        merged_file = os.path.join(self.base_path, "merged_final_complete.csv")
        df_merged.to_csv(merged_file, index=False)
        print(f"‚úÖ Saved: merged_final_complete.csv\n")

        return df_merged

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ LAG FEATURES GENERATOR
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_lag_features(df, feature_cols, windows=[3, 5], home_away_split=True):
    """
    Create lag features from in-game statistics (FIXED VERSION)

    Args:
        df: DataFrame with MatchDate, HomeTeam, AwayTeam, and feature columns
        feature_cols: List of feature names to create lags for
        windows: List of window sizes (e.g., [3, 5] for last 3 and 5 matches)
        home_away_split: If True, separate home/away performance

    Returns:
        DataFrame with lag features added, list of created feature names
    """

    print("\n" + "="*100)
    print("üîÑ CREATING LAG FEATURES (Leakage-Free Historical Averages)")
    print("="*100 + "\n")

    # ‚úÖ FIX 1: Ensure sorted by date and reset index
    df = df.sort_values('MatchDate').reset_index(drop=True).copy()

    lag_features_created = []

    for feat in feature_cols:
        if feat not in df.columns:
            print(f"  ‚ö†Ô∏è  Skipping {feat} (not in dataset)")
            continue

        print(f"  Processing: {feat}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # HOME TEAM LAG FEATURES
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        for window in windows:
            if home_away_split:
                # Home performance at home
                col_name = f'Home_{feat}_AtHome_Last{window}_Avg'

                # ‚úÖ FIX 2: Use transform() instead of apply() to avoid MultiIndex
                df[col_name] = df.groupby('HomeTeam')[feat].transform(
                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                )
                lag_features_created.append(col_name)

                # Home performance overall (fallback)
                col_name_overall = f'Home_{feat}_Last{window}_Avg'
                df[col_name_overall] = df.groupby('HomeTeam')[feat].transform(
                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                )
                lag_features_created.append(col_name_overall)
            else:
                # Simple rolling average
                col_name = f'Home_{feat}_Last{window}_Avg'
                df[col_name] = df.groupby('HomeTeam')[feat].transform(
                    lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                )
                lag_features_created.append(col_name)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # AWAY TEAM LAG FEATURES
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        for window in windows:
            if home_away_split:
                # Determine away feature name
                if feat.startswith('Home'):
                    away_feat = feat.replace('Home', 'Away')
                elif feat.startswith('Away'):
                    away_feat = feat
                else:
                    away_feat = f'Away{feat}'

                # Away performance at away
                if away_feat in df.columns:
                    col_name = f'Away_{feat}_AtAway_Last{window}_Avg'
                    df[col_name] = df.groupby('AwayTeam')[away_feat].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                    )
                    lag_features_created.append(col_name)

                # Away performance overall
                if away_feat in df.columns:
                    col_name_overall = f'Away_{feat}_Last{window}_Avg'
                    df[col_name_overall] = df.groupby('AwayTeam')[away_feat].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                    )
                    lag_features_created.append(col_name_overall)
            else:
                # Simple rolling average for away team
                if feat.startswith('Home'):
                    away_feat = feat.replace('Home', 'Away')
                else:
                    away_feat = f'Away{feat}'

                if away_feat in df.columns:
                    col_name = f'Away_{feat}_Last{window}_Avg'
                    df[col_name] = df.groupby('AwayTeam')[away_feat].transform(
                        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()
                    )
                    lag_features_created.append(col_name)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # FILL NaN VALUES (ilk ma√ßlar i√ßin yeterli data yok)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print(f"\n  Handling NaN values (early season matches)...")

    for col in lag_features_created:
        if col in df.columns:
            # Strategy 1: Forward fill (use last known value)
            df[col] = df[col].fillna(method='ffill')

            # Strategy 2: Fill remaining with league average
            if df[col].isna().any():
                league_avg = df[col].mean()
                df[col] = df[col].fillna(league_avg)

            # Strategy 3: Fill any remaining with 0
            df[col] = df[col].fillna(0)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SUMMARY
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print(f"\n  ‚úÖ Created {len(lag_features_created)} lag features")
    print(f"  üìä Original features: {len(feature_cols)}")
    print(f"  üìä Lag features: {len(lag_features_created)}")
    print(f"  üìä Windows used: {windows}")
    print(f"  üìä Home/Away split: {home_away_split}\n")

    # Sample output
    if lag_features_created:
        print("  Sample lag features created:")
        for feat in lag_features_created[:5]:
            non_null = df[feat].notna().sum()
            mean_val = df[feat].mean()
            print(f"     ‚Ä¢ {feat:<50s} (mean: {mean_val:6.2f}, coverage: {non_null/len(df)*100:5.1f}%)")
        if len(lag_features_created) > 5:
            print(f"     ... and {len(lag_features_created)-5} more")

    print("\n" + "="*100 + "\n")

    return df, lag_features_created

# FEATURE PREPARATION
def prepare_features(df, n_features=None):
    """Prepare features for modeling"""

    print("\n2Ô∏è‚É£ FEATURE PREPARATION...\n")

    all_cols = df.columns.tolist()

    target_related = ['FTResult', 'FTHome', 'FTAway', 'HTHome', 'HTAway', 'HTResult']
    non_numeric_like = df.select_dtypes(exclude=[np.number]).columns.tolist()
    temp_cols = ['HomeTeam_TM', 'AwayTeam_TM', 'Season', 'YearMonth']
    original_teams = ['HomeTeam', 'AwayTeam', 'Division', 'MatchTime']
    date_cols = ['MatchDate']
    id_cols = [col for col in df.columns if 'club_id' in col]

    cols_to_drop = set(target_related + non_numeric_like + temp_cols + original_teams + date_cols + id_cols)

    numeric_features = [col for col in df.select_dtypes(include=[np.number]).columns if col not in cols_to_drop]

    print(f"  Features: {len(numeric_features)}")

    X = df[numeric_features].copy()
    y = df['FTResult'].copy()

    original_y_index = y.index

    le = LabelEncoder()
    y_transformed = le.fit_transform(y)
    y = pd.Series(y_transformed, index=original_y_index, name='FTResult_Encoded')

    print(f"  Target encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}")

    if X.isnull().sum().sum() > 0:
        print("  Filling NaNs...")
        X = X.fillna(X.median())

    if np.isinf(X.values).any() or X.isnull().values.any():
        print("  Fixing inf/NaN values...")
        bad_cols = X.columns[X.isnull().any()].tolist() + X.columns[np.isinf(X.values).any(axis=0)].tolist()
        for col in bad_cols:
            X[col] = X[col].fillna(0).replace([np.inf, -np.inf], 0)

    selected_features = numeric_features

    if n_features and n_features > 0 and n_features < X.shape[1]:
        print(f"  Feature selection (Top {n_features})...")
        try:
            selector = SelectKBest(mutual_info_classif, k=n_features)
            X_new = selector.fit_transform(X, y)
            selected_features = np.array(numeric_features)[selector.get_support()].tolist()
            X = pd.DataFrame(X_new, columns=selected_features, index=X.index)
        except Exception as e:
            print(f"  Selection error: {e}")
            selected_features = numeric_features
            X = pd.DataFrame(X, columns=selected_features, index=X.index)
    else:
        X = pd.DataFrame(X, columns=selected_features, index=X.index)

    return X, y, selected_features

# LTCN MODEL
class LTCN(BaseEstimator, ClassifierMixin):
    """Liquid Time-Constant Network"""

    def __init__(self, T=25, alpha=0.8, beta=0.2, method="inverse",
                 function="sigmoid", ridge_alpha=1e-3, random_state=None):
        self.T = T
        self.alpha = alpha
        self.beta = beta
        self.method = method
        self.function = function
        self.ridge_alpha = ridge_alpha
        self.random_state = random_state

        self.W1 = None
        self.W2 = None
        self.model_ltcn = None
        self.classes_ = None
        self.n_classes_ = None

    def fit(self, X, y):
        X = np.atleast_2d(X)
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)

        Y = np.eye(self.n_classes_)[np.searchsorted(self.classes_, y)]

        if self.W1 is None:
            try:
                _, _, Vt = np.linalg.svd(X, full_matrices=False)
                W = Vt.T
                W_max = np.max(np.abs(W))
                self.W1 = W / (W_max + 1e-10)
            except (np.linalg.LinAlgError, ValueError):
                if self.random_state is not None:
                    np.random.seed(self.random_state)
                self.W1 = np.random.randn(X.shape[1], X.shape[1]) * 0.1

        H = self._compute_chain(X)

        try:
            if self.method == "inverse":
                X_aug = np.c_[H, np.ones((H.shape[0], 1))]
                self.W2 = np.linalg.pinv(X_aug) @ Y
            else:
                self.model_ltcn = Ridge(alpha=self.ridge_alpha).fit(H, Y)
        except Exception as e:
            self.model_ltcn = Ridge(alpha=self.ridge_alpha).fit(H, Y)
            self.method = "ridge"

        return self

    def predict(self, X):
        X = np.atleast_2d(X)
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        H = self._compute_chain(X)

        if self.method == "inverse" and self.W2 is not None:
            X_aug = np.c_[H, np.ones((H.shape[0], 1))]
            raw = X_aug @ self.W2
        elif self.model_ltcn is not None:
            raw = self.model_ltcn.predict(H)
        else:
            raise RuntimeError("Model not fitted")

        pred_indices = np.argmax(raw, axis=1)
        return self.classes_[pred_indices]

    def predict_proba(self, X):
        X = np.atleast_2d(X)
        if X.ndim == 1:
            X = X.reshape(-1, 1)

        H = self._compute_chain(X)

        if self.method == "inverse" and self.W2 is not None:
            X_aug = np.c_[H, np.ones((H.shape[0], 1))]
            raw = X_aug @ self.W2
        elif self.model_ltcn is not None:
            raw = self.model_ltcn.predict(H)
        else:
            raise RuntimeError("Model not fitted")

        raw_max = raw.max(axis=1, keepdims=True)
        exps = np.exp(np.clip(raw - raw_max, -500, 500))
        proba = exps / exps.sum(axis=1, keepdims=True)

        return proba

    def _compute_chain(self, A):
        A0 = A.copy()
        H = A.copy()

        for t in range(self.T):
            pre_activation = A @ self.W1

            if self.function == "sigmoid":
                Z = 1.0 / (1.0 + np.exp(-np.clip(pre_activation, -500, 500)))
            else:
                Z = np.tanh(pre_activation)

            if Z.ndim > 2:
                Z = np.squeeze(Z, axis=-1)

            A = self.alpha * Z + self.beta * A0
            H = np.c_[H, A]

        return H

# HELPER FUNCTIONS
def ranked_probability_score(y_true, y_pred_proba):
    if y_pred_proba.ndim == 1:
        y_pred_proba = np.column_stack([1 - y_pred_proba, y_pred_proba])
    K = y_pred_proba.shape[1]
    y_true_one_hot = np.eye(K)[y_true]
    rps_scores = []
    for i in range(len(y_true)):
        cum_pred = np.cumsum(y_pred_proba[i])
        cum_true = np.cumsum(y_true_one_hot[i])
        rps_scores.append(np.sum((cum_pred - cum_true) ** 2))
    return np.mean(rps_scores)

# ============================================================================
# DIRECTORY HELPER
# ============================================================================
def ensure_directory_exists(path):
    """Create directory if it doesn't exist"""
    if path and not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def expected_calibration_error(y_true, y_pred_proba, n_bins=10):
    if y_pred_proba.ndim == 1:
        y_pred_proba = np.column_stack([1 - y_pred_proba, y_pred_proba])

    pred_confidence = np.max(y_pred_proba, axis=1)
    pred_label = np.argmax(y_pred_proba, axis=1)

    bin_edges = np.linspace(0, 1, n_bins + 1)
    ece = 0
    for i in range(n_bins):
        mask = (pred_confidence >= bin_edges[i]) & (pred_confidence < bin_edges[i + 1])
        if np.sum(mask) > 0:
            acc = np.mean(pred_label[mask] == y_true[mask])
            conf = np.mean(pred_confidence[mask])
            ece += np.abs(acc - conf) * np.sum(mask) / len(y_true)
    return ece
# ============================================================================
# ‚úÖ YENƒ∞ EKLEME: SMART FEATURE SELECTION
# ============================================================================
def smart_feature_selection(X, y, mi_threshold=0.001, corr_threshold=0.85,
                            graphics_dir=None, data_dir=None):
    """
    Hybrid feature selection: MI + Correlation based removal

    Args:
        X: Training features (DataFrame)
        y: Target variable
        mi_threshold: Minimum MI score to keep (default: 0.001)
        corr_threshold: Maximum correlation allowed (default: 0.85)
        graphics_dir: Path to save visualizations
        data_dir: Path to save analysis results

    Returns:
        X_clean: Cleaned feature set
        all_drops: List of dropped features
        mi_df: MI scores dataframe
        report: Dictionary with analysis results
    """
    from sklearn.feature_selection import mutual_info_classif

    print(f"\n{'='*100}")
    print("üî¨ SMART FEATURE SELECTION (MI + CORRELATION)")
    print(f"{'='*100}\n")

    initial_features = X.shape[1]
    print(f"  üìä Initial features: {initial_features}")
    print(f"  üéØ MI Threshold: {mi_threshold}")
    print(f"  üéØ Correlation Threshold: {corr_threshold}\n")

    # ========================================================================
    # STEP 1: MI CALCULATION
    # ========================================================================
    print("[STEP 1] üìà Calculating Mutual Information scores...")
    mi_scores = mutual_info_classif(X, y, random_state=42, n_neighbors=3)
    mi_df = pd.DataFrame({
        'feature': X.columns,
        'mi_score': mi_scores
    }).sort_values('mi_score', ascending=False)

    print(f"  ‚úÖ MI scores calculated")
    print(f"     Mean MI: {mi_scores.mean():.6f}")
    print(f"     Median MI: {np.median(mi_scores):.6f}")
    print(f"     Max MI: {mi_scores.max():.6f}\n")

    # ========================================================================
    # STEP 2: LOW MI DROP
    # ========================================================================
    print(f"[STEP 2] üóëÔ∏è  Identifying low-MI features (< {mi_threshold})...")
    low_mi_features = mi_df[mi_df['mi_score'] < mi_threshold]['feature'].tolist()

    print(f"  ‚ö†Ô∏è  Low MI features found: {len(low_mi_features)}")
    if low_mi_features:
        print("     Features to drop:")
        for i, feat in enumerate(low_mi_features[:10], 1):
            mi_val = mi_df[mi_df['feature'] == feat]['mi_score'].iloc[0]
            print(f"       {i:2d}. {feat:40s} (MI = {mi_val:.6f})")
        if len(low_mi_features) > 10:
            print(f"       ... and {len(low_mi_features)-10} more\n")
    else:
        print("  ‚úÖ All features have sufficient MI scores\n")

    # ========================================================================
    # STEP 3: CORRELATION MATRIX (on remaining features)
    # ========================================================================
    print(f"[STEP 3] üîó Computing correlation matrix...")
    X_temp = X.drop(columns=low_mi_features) if low_mi_features else X.copy()
    corr_matrix = X_temp.corr().abs()

    # Visualization
    if graphics_dir:
        fig, ax = plt.subplots(figsize=(20, 16))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(
            corr_matrix,
            mask=mask,
            annot=False,
            cmap='RdYlGn_r',
            vmin=0, vmax=1,
            center=0.5,
            square=True,
            linewidths=0.5,
            cbar_kws={"shrink": 0.8}
        )
        set_title_if_enabled(ax,f'Feature Correlation Matrix (After Low-MI Drop)\n{X_temp.shape[1]} features',
                     fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        corr_path = os.path.join(graphics_dir, '00_correlation_matrix_after_mi.png')
        plt.savefig(corr_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ Correlation heatmap saved: {os.path.basename(corr_path)}\n")

    # ========================================================================
    # STEP 4: HIGH CORRELATION DROP (keep higher MI)
    # ========================================================================
    print(f"[STEP 4] ‚öñÔ∏è  Identifying high-correlation pairs (> {corr_threshold})...")
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    features_to_drop_corr = []
    mi_dict = dict(zip(mi_df['feature'], mi_df['mi_score']))
    processed_pairs = set()
    high_corr_details = []

    for column in upper.columns:
        high_corr_features = upper.index[upper[column] > corr_threshold].tolist()
        for corr_feat in high_corr_features:
            pair_key = tuple(sorted([column, corr_feat]))
            if pair_key in processed_pairs:
                continue
            processed_pairs.add(pair_key)

            mi_col = mi_dict.get(column, 0)
            mi_corr = mi_dict.get(corr_feat, 0)
            corr_value = corr_matrix.loc[column, corr_feat]

            if mi_col < mi_corr:
                if column not in features_to_drop_corr:
                    features_to_drop_corr.append(column)
                    high_corr_details.append({
                        'Dropped': column,
                        'Kept': corr_feat,
                        'Correlation': corr_value,
                        'Dropped_MI': mi_col,
                        'Kept_MI': mi_corr
                    })
            else:
                if corr_feat not in features_to_drop_corr:
                    features_to_drop_corr.append(corr_feat)
                    high_corr_details.append({
                        'Dropped': corr_feat,
                        'Kept': column,
                        'Correlation': corr_value,
                        'Dropped_MI': mi_corr,
                        'Kept_MI': mi_col
                    })

    print(f"  ‚ö†Ô∏è  High-correlation pairs found: {len(high_corr_details)}")
    if high_corr_details:
        print(f"\n  {'='*90}")
        print(f"  {'DROPPED FEATURE':<35} {'KEPT FEATURE':<35} {'|r|':>8} {'Strategy':<10}")
        print(f"  {'='*90}")
        for detail in high_corr_details[:15]:  # Top 15
            print(f"  {detail['Dropped']:<35} {detail['Kept']:<35} "
                  f"{detail['Correlation']:>8.4f} Keep Higher MI")
        if len(high_corr_details) > 15:
            print(f"  ... and {len(high_corr_details)-15} more pairs")
        print(f"  {'='*90}\n")
    else:
        print("  ‚úÖ No high-correlation pairs found\n")

    # Save high-corr details
    if high_corr_details and data_dir:
        high_corr_df = pd.DataFrame(high_corr_details)
        high_corr_path = os.path.join(data_dir, 'high_correlation_dropped.csv')
        high_corr_df.to_csv(high_corr_path, index=False)
        print(f"  ‚úÖ High-correlation analysis saved: {os.path.basename(high_corr_path)}\n")

    # ========================================================================
    # STEP 5: FINAL DROP
    # ========================================================================
    print(f"[STEP 5] ‚úÇÔ∏è  Applying feature removal...")
    all_drops = list(set(low_mi_features + features_to_drop_corr))
    X_clean = X.drop(columns=all_drops)

    print(f"\n  üìä SUMMARY:")
    print(f"  {'='*80}")
    print(f"     Initial features:              {initial_features}")
    print(f"     Dropped (Low MI):              {len(low_mi_features)}")
    print(f"     Dropped (High Correlation):    {len(features_to_drop_corr)}")
    print(f"     Dropped (Total):               {len(all_drops)}")
    print(f"     Remaining features:            {X_clean.shape[1]}")
    print(f"     Reduction:                     {(1 - X_clean.shape[1]/initial_features)*100:.1f}%")
    print(f"  {'='*80}\n")

    # Report dictionary
    report = {
        'initial_features': initial_features,
        'final_features': X_clean.shape[1],
        'dropped_low_mi': len(low_mi_features),
        'dropped_high_corr': len(features_to_drop_corr),
        'total_dropped': len(all_drops),
        'reduction_pct': (1 - X_clean.shape[1]/initial_features)*100
    }

    print(f"‚úÖ Smart Feature Selection Complete!\n")
    print(f"{'='*100}\n")

    return X_clean, all_drops, mi_df, report

def add_numeric_values_to_bars(ax, bars, format_str='.2f', color='black', fontsize=12):
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:{format_str}}',
                ha='center', va='bottom', fontsize=fontsize,
                fontweight='bold', color=color)

# XAI FUNCTION
def get_feature_importance(model, X, y, model_name, feature_names, config=None):
    """XAI analysis with 6 methods"""
    if config is None:
        config = {}

    if hasattr(X, 'values'):
        X_np = X.values
    else:
        X_np = X

    importance_dict = {}
    timing_dict = {}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ YENƒ∞ EKLEME: XAI CONFIG VE TIMING SETUP
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    import time

    xai_config = config.get("xai", {})
    PFI_N_REPEATS = xai_config.get("pfi_n_repeats", 5)
    XAI_SEED = xai_config.get("random_state", 42)
    timing_dict = {}  # Her metodun s√ºresini sakla

    print(f"\n  [XAI CONFIG] PFI repeats: {PFI_N_REPEATS}, Seed: {XAI_SEED}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 1. PFI (G√úNCELLEME: n_repeats d√º≈ü√ºr√ºld√º, timing eklendi)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print(f"  [PFI] Computing...")
        start_time = time.time()

        if hasattr(model, 'predict'):
            perm_importance = permutation_importance(
                model, X_np, y,
                n_repeats=PFI_N_REPEATS,  # ‚Üê 25'ten d√º≈ü√ºr√ºld√º
                random_state=XAI_SEED,    # ‚Üê Config'ten seed
                n_jobs=-1
            )
            mean_imp = perm_importance.importances_mean
            mean_imp[mean_imp < 0] = 0

            if mean_imp.sum() > 0:
                importance_dict['PFI'] = mean_imp / np.sum(mean_imp)
            else:
                importance_dict['PFI'] = np.ones(len(feature_names)) / len(feature_names)

            elapsed = time.time() - start_time
            timing_dict['PFI'] = elapsed
            print(f"  ‚úì PFI completed in {elapsed:.2f}s")

    except Exception as e_pfi:
        print(f"  ‚úó PFI failed: {e_pfi}")
        importance_dict['PFI'] = np.ones(len(feature_names)) / len(feature_names)
        timing_dict['PFI'] = 0.0


    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 2. PMI (G√úNCELLEME: timing eklendi, seed eklendi)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print(f"    [PMI] Computing...")
        start_time = time.time()

        n_samples = X_np.shape[0]
        n_neighbors_adaptive = min(5, (n_samples // 100))
        n_neighbors_final = max(3, n_neighbors_adaptive)

        mi = mutual_info_classif(
            X_np,
            y,
            random_state=XAI_SEED,  # ‚Üê Seed eklendi
            n_neighbors=n_neighbors_final,
            discrete_features=False
        )

        if mi.sum() < 1e-6:
            importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)
        else:
            mi = np.clip(mi, 0, None)
            if mi.sum() < 1e-6:
                importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)
            else:
                importance_dict['PMI'] = mi / mi.sum()

        elapsed = time.time() - start_time
        timing_dict['PMI'] = elapsed
        print(f"       ‚úì PMI completed in {elapsed:.2f}s")

    except Exception as e_pmi:
        print(f"       ‚úó PMI failed: {e_pmi}")
        importance_dict['PMI'] = np.ones(len(feature_names)) / len(feature_names)
        timing_dict['PMI'] = 0.0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 3. SOFI (G√úNCELLEME: timing eklendi, GA seed eklendi)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print(f"    [SOFI] Computing...")
        start_time = time.time()

        if not HAS_PYGAD:
            print(f"    [SOFI] pygad not available, using fallback sensitivity method")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # FALLBACK: Sensitivity-based method
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            sensitivity_scores = np.zeros(X_np.shape[1])

            try:
                base_proba = model.predict_proba(X_np)
                use_proba = True
            except:
                base_pred = model.predict(X_np).astype(float)
                use_proba = False

            # Set seed for shuffling
            np.random.seed(XAI_SEED)

            for i in range(X_np.shape[1]):
                X_perturbed = X_np.copy()
                np.random.shuffle(X_perturbed[:, i])

                if use_proba:
                    try:
                        perturbed_proba = model.predict_proba(X_perturbed)
                        sensitivity_scores[i] = np.mean(np.sum(np.abs(base_proba - perturbed_proba), axis=1))
                    except:
                        sensitivity_scores[i] = 0.0
                else:
                    perturbed_pred = model.predict(X_perturbed).astype(float)
                    sensitivity_scores[i] = np.mean(np.abs(base_pred - perturbed_pred))

            if sensitivity_scores.sum() > 0:
                importance_dict['SOFI'] = sensitivity_scores / np.sum(sensitivity_scores)
            else:
                importance_dict['SOFI'] = np.ones(len(feature_names)) / len(feature_names)

        else:
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ GA-BASED SOFI (FULL VERSION WITH SEED)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            print(f"    [SOFI] Using GA-based optimization (pygad)")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Global variables for pygad fitness function
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            global GA_MODEL, GA_X, GA_Y, GA_BASE_F1

            def feature_flip_sofi(model, X_num, y, permutation):
                """
                Replace each feature with its mean sequentially and compute F1 after each flip.
                """
                from sklearn.metrics import f1_score
                scores = []
                X_copy = X_num.copy()

                for idx in permutation:
                    X_copy[:, idx] = X_copy[:, idx].mean()
                    preds = model.predict(X_copy)
                    scores.append(f1_score(y, preds, average="weighted"))

                return scores

            def fit_value_sofi(model, X_num, y, permutation, base_f1):
                """
                SOFI objective = area under g(pi)-curve + 0.1 * penalty for positive jumps.
                """
                n = len(permutation)
                weights = np.arange(1.0, 0.0, -1 / (n + 1))
                g_vals = feature_flip_sofi(model, X_num, y, permutation)
                g_vals = [base_f1] + g_vals
                area = np.sum(weights * g_vals)
                penalty = 0.1 * np.sum(np.maximum(np.diff(g_vals), 0))
                return area + penalty

            def sofi_fitness(ga_instance, solution, solution_idx):
                """
                Fitness = 1 / (SOFI objective + epsilon), so GA maximizes fitness.
                """
                return 1.0 / (fit_value_sofi(GA_MODEL, GA_X, GA_Y, solution, GA_BASE_F1) + 1e-6)

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Set global variables
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            GA_MODEL = model
            GA_X = X_np
            GA_Y = y

            from sklearn.metrics import f1_score
            GA_BASE_F1 = f1_score(y, model.predict(X_np), average="weighted")

            n_features = X_np.shape[1]

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Get SOFI config from CONFIG
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            sofi_config = config.get("sofi", {})
            n_generations = sofi_config.get("n_generations", 25)
            sol_per_pop = sofi_config.get("sol_per_pop", 50)
            mutation_prob = sofi_config.get("mutation_probability", 0.05)

            # ‚úÖ USE XAI_SEED INSTEAD OF HARDCODED SEED
            ga_random_seed = XAI_SEED

            num_parents = max(2, sol_per_pop // 2)

            print(f"       GA params: gen={n_generations}, pop={sol_per_pop}, mut={mutation_prob}, seed={ga_random_seed}")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Create GA instance
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            ga_kwargs = {
                "num_generations":       n_generations,
                "sol_per_pop":           sol_per_pop,
                "num_parents_mating":    num_parents,
                "fitness_func":          sofi_fitness,
                "num_genes":             n_features,
                "gene_space":            list(range(n_features)),
                "allow_duplicate_genes": False,
                "mutation_probability":  mutation_prob,
                "gene_type":             int,
                "random_seed":           ga_random_seed,  # ‚Üê CONFIG'TEN SEED
                "suppress_warnings":     True
            }

            ga_instance = GA(**ga_kwargs)
            ga_instance.run()

            best_solution, _, _ = ga_instance.best_solution()

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Convert permutation to importance scores
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            sofi_importance = np.zeros(n_features)
            for rank, feature_idx in enumerate(best_solution):
                sofi_importance[feature_idx] = n_features - rank

            # Normalize
            if sofi_importance.sum() > 0:
                importance_dict['SOFI'] = sofi_importance / np.sum(sofi_importance)
            else:
                importance_dict['SOFI'] = np.ones(n_features) / n_features

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Timing
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        elapsed = time.time() - start_time
        timing_dict['SOFI'] = elapsed
        print(f"       ‚úì SOFI completed in {elapsed:.2f}s")

    except Exception as e_sofi:
        print(f"    [SOFI ERROR] {str(e_sofi)[:100]}")
        import traceback
        traceback.print_exc()
        importance_dict['SOFI'] = np.ones(len(feature_names)) / len(feature_names)
        timing_dict['SOFI'] = 0.0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 4. SHAP - ENHANCED WITH COMPREHENSIVE DEBUG & VALIDATION (FULL FIX)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if HAS_SHAP and model_name not in ['svm', 'ada']:
        try:
            print(f"    [SHAP] Computing feature importance...")
            start_time = time.time()

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ CONFIG'TEN PARAMETRELERƒ∞ AL
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            xai_config = config.get("xai", {})
            n_shap_samples = xai_config.get("n_samples", 100)  # ‚Üê 250'den d√º≈ü√ºr√ºld√º
            n_background = xai_config.get("n_shap_background", 50)  # ‚Üê 200'den d√º≈ü√ºr√ºld√º
            n_jobs_shap = xai_config.get("n_jobs_shap", 16)

            # Convert to numpy
            if isinstance(X, pd.DataFrame):
                X_np_shap = X.values
            else:
                X_np_shap = X_np

            # ‚úÖ DEBUG 1: Feature count validation
            n_features_expected = len(feature_names)
            print(f"       [DEBUG] Expected features: {n_features_expected}")
            print(f"       [DEBUG] X shape: {X_np_shap.shape}")
            print(f"       [DEBUG] Samples: {n_shap_samples}, Background: {n_background}")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # (A) Select background samples
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if X_np_shap.shape[0] > n_background:
                np.random.seed(XAI_SEED)  # ‚Üê SEED KULLAN
                idx = np.random.choice(X_np_shap.shape[0], n_background, replace=False)
                background = X_np_shap[idx]
            else:
                background = X_np_shap

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # (B) Select evaluation samples
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if X_np_shap.shape[0] > n_shap_samples:
                np.random.seed(XAI_SEED)  # ‚Üê SEED KULLAN
                idx_eval = np.random.choice(X_np_shap.shape[0], n_shap_samples, replace=False)
                X_eval = X_np_shap[idx_eval]
            else:
                X_eval = X_np_shap

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ MODEL-SPECIFIC EXPLAINER SELECTION (ENHANCED & FIXED)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

            if model_name == 'lr':
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # LOGISTIC REGRESSION: LinearExplainer
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                print(f"       Using LinearExplainer (LR-specific)")

                explainer = shap.LinearExplainer(model, background)
                shap_values = explainer.shap_values(X_eval)

                print(f"       [DEBUG] SHAP output type: {type(shap_values)}")

                if isinstance(shap_values, list):
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # ‚úÖ MULTICLASS LIST HANDLING
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    print(f"       [DEBUG] SHAP is multiclass list, length: {len(shap_values)}")

                    # Stack all classes
                    try:
                        shap_values_all_classes = np.array(shap_values)
                        print(f"       [DEBUG] Stacked shape: {shap_values_all_classes.shape}")
                    except Exception as e_stack:
                        print(f"       [ERROR] Stacking failed: {e_stack}")
                        raise

                    # Aggregate correctly based on shape
                    if shap_values_all_classes.shape[0] == 3:
                        # Shape: (3, 100, 83) - classes first
                        print(f"       [INFO] Aggregating: (classes, samples, features)")
                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes), axis=(0, 1))

                    elif shap_values_all_classes.shape[2] == 3:
                        # Shape: (100, 83, 3) - classes last
                        print(f"       [INFO] Aggregating: (samples, features, classes)")
                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes), axis=(0, 2))

                    else:
                        # Fallback
                        print(f"       [WARNING] Using fallback aggregation")
                        mean_abs_shap = np.mean(np.abs(shap_values_all_classes),
                                               axis=tuple(range(len(shap_values_all_classes.shape)-1)))

                    print(f"       [DEBUG] After aggregation shape: {mean_abs_shap.shape}")

                    # Shape validation
                    if mean_abs_shap.ndim != 1:
                        if mean_abs_shap.size == n_features_expected:
                            mean_abs_shap = mean_abs_shap.flatten()
                            print(f"       [FIX] Flattened to: {mean_abs_shap.shape}")
                        else:
                            raise ValueError(f"Cannot fix shape: {mean_abs_shap.shape}")

                    if mean_abs_shap.shape[0] != n_features_expected:
                        raise ValueError(
                            f"SHAP shape mismatch!\n"
                            f"  Got:      {mean_abs_shap.shape[0]} features\n"
                            f"  Expected: {n_features_expected} features"
                        )

                elif len(shap_values.shape) == 3:
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # ‚úÖ 3D ARRAY HANDLING
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    print(f"       [DEBUG] SHAP 3D array shape: {shap_values.shape}")

                    if shap_values.shape[0] == X_eval.shape[0]:
                        # (n_samples, n_features, n_classes)
                        print(f"       [INFO] Shape format: (samples, features, classes)")
                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 2))
                    else:
                        # (n_classes, n_samples, n_features)
                        print(f"       [INFO] Shape format: (classes, samples, features)")
                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 1))

                else:
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # ‚úÖ 2D ARRAY (Binary or single-class)
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    print(f"       [DEBUG] SHAP 2D array shape: {shap_values.shape}")
                    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)

            elif model_name in ['xgboost', 'lightgbm', 'rf', 'gb', 'catboost']:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # TREE-BASED MODELS: TreeExplainer (very fast)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                print(f"       Using TreeExplainer (Tree-based)")

                try:
                    explainer = shap.TreeExplainer(model)
                    shap_values = explainer.shap_values(X_eval)
                except Exception as e_tree:
                    print(f"       [WARNING] TreeExplainer failed: {e_tree}")
                    print(f"       Falling back to KernelExplainer...")

                    def pred_proba(x_array):
                        return model.predict_proba(x_array)

                    explainer = shap.KernelExplainer(pred_proba, background)
                    shap_values = explainer.shap_values(X_eval, nsamples=100)

                print(f"       [DEBUG] SHAP output type: {type(shap_values)}")

                # Multiclass output handling
                if isinstance(shap_values, list):
                    print(f"       [DEBUG] Multiclass list, using ALL classes")
                    shap_values_all = np.array(shap_values)
                    print(f"       [DEBUG] Stacked shape: {shap_values_all.shape}")
                    mean_abs_shap = np.mean(np.abs(shap_values_all), axis=(0, 1))

                elif len(shap_values.shape) == 3:
                    print(f"       [DEBUG] 3D array shape: {shap_values.shape}")

                    if shap_values.shape[0] == X_eval.shape[0]:
                        # (n_samples, n_features, n_classes)
                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 2))
                    else:
                        # (n_classes, n_samples, n_features)
                        mean_abs_shap = np.mean(np.abs(shap_values), axis=(0, 1))

                else:
                    print(f"       [DEBUG] 2D array shape: {shap_values.shape}")
                    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)

            elif model_name == 'ltcn':
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # LTCN: KernelExplainer (model-agnostic, parallel)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                print(f"       Using KernelExplainer (LTCN, parallel)")

                def _compute_shap_for_instance(explainer, x_instance, nsamples):
                    """Compute KernelSHAP for a single instance"""
                    try:
                        shap_vals = explainer.shap_values(x_instance, nsamples=nsamples)
                        return np.abs(np.array(shap_vals).reshape(-1))
                    except Exception as e_inst:
                        print(f"       [WARNING] Instance SHAP failed: {e_inst}")
                        return np.zeros(n_features_expected)

                def pred_positive(x_array):
                    """Predict probability for Draw class (class 1)"""
                    try:
                        if hasattr(model, 'predict_proba'):
                            proba = model.predict_proba(x_array)
                            if proba.shape[1] == 3:
                                return proba[:, 1]  # Draw class
                            elif proba.shape[1] == 2:
                                return proba[:, 1]  # Binary positive
                            else:
                                return proba.ravel()
                        else:
                            return model.predict(x_array)
                    except Exception as e_pred:
                        print(f"       [ERROR] Prediction failed: {e_pred}")
                        return np.zeros(x_array.shape[0])

                explainer = shap.KernelExplainer(pred_positive, background, link="identity")

                print(f"       Background: {background.shape[0]}, Eval: {X_eval.shape[0]}, " +
                      f"nsamples: {n_shap_samples}, n_jobs: {n_jobs_shap}")

                # Parallel computation
                from joblib import Parallel, delayed

                try:
                    results = Parallel(n_jobs=n_jobs_shap, verbose=0)(
                        delayed(_compute_shap_for_instance)(explainer, X_eval[i:i+1], n_shap_samples)
                        for i in range(X_eval.shape[0])
                    )

                    shap_matrix = np.vstack(results)
                    mean_abs_shap = np.mean(shap_matrix, axis=0)

                except Exception as e_parallel:
                    print(f"       [ERROR] Parallel SHAP failed: {e_parallel}")
                    print(f"       Using uniform fallback...")
                    mean_abs_shap = np.ones(n_features_expected) / n_features_expected

            else:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # OTHER MODELS: KernelExplainer (fallback)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                print(f"       Using KernelExplainer (generic)")

                def pred_positive(x_array):
                    """Predict probability for Draw class"""
                    proba = model.predict_proba(x_array)
                    if proba.shape[1] == 3:
                        return proba[:, 1]
                    return proba[:, 1]

                explainer = shap.KernelExplainer(pred_positive, background, link="identity")
                shap_values = explainer.shap_values(X_eval, nsamples=n_shap_samples)
                mean_abs_shap = np.mean(np.abs(shap_values), axis=0)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ COMPREHENSIVE DEBUG OUTPUT
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            print(f"       [DEBUG] SHAP importance statistics:")
            print(f"         Shape:  {mean_abs_shap.shape}")
            print(f"         Min:    {mean_abs_shap.min():.6f}")
            print(f"         Max:    {mean_abs_shap.max():.6f}")
            print(f"         Mean:   {mean_abs_shap.mean():.6f}")
            print(f"         Median: {np.median(mean_abs_shap):.6f}")
            print(f"         Sum:    {mean_abs_shap.sum():.6f}")

            # ‚úÖ Top 5 features BEFORE normalization
            top_5_idx = np.argsort(mean_abs_shap)[-5:][::-1]
            print(f"       [DEBUG] Top 5 features (raw SHAP values):")
            for rank, idx in enumerate(top_5_idx, 1):
                feat_name = feature_names[int(idx)]
                feat_value = mean_abs_shap[int(idx)]
                print(f"         #{rank}: {feat_name:40s} = {feat_value:.6f}")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ NORMALIZE WITH VALIDATION
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if mean_abs_shap.sum() > 0:
                importance_dict['SHAP'] = mean_abs_shap / np.sum(mean_abs_shap)

                # Post-normalization verification
                normalized_sum = importance_dict['SHAP'].sum()
                print(f"       [DEBUG] After normalization:")
                print(f"         Sum: {normalized_sum:.10f} (should be ‚âà 1.0)")

                if abs(normalized_sum - 1.0) > 0.001:
                    print(f"       [WARNING] Normalization sum != 1.0 (got {normalized_sum:.6f})")
                    importance_dict['SHAP'] = importance_dict['SHAP'] / normalized_sum
                    print(f"       [FIX] Re-normalized to sum = {importance_dict['SHAP'].sum():.10f}")
            else:
                print(f"       [WARNING] SHAP sum = 0! Using uniform fallback.")
                importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # Timing
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            elapsed = time.time() - start_time
            timing_dict['SHAP'] = elapsed
            print(f"       ‚úì SHAP completed in {elapsed:.2f}s")

        except Exception as e:
            print(f"    [SHAP ERROR] {model_name}: {str(e)[:150]}")
            print(f"    [SHAP ERROR] Full traceback:")
            import traceback
            traceback.print_exc()

            importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)
            timing_dict['SHAP'] = 0.0
            print(f"    [SHAP ERROR] Using uniform fallback")

    else:
        # SHAP not available or model excluded
        if model_name in ['svm', 'ada']:
            print(f"    [SHAP] Skipped for {model_name} (not supported)")
        else:
            print(f"    [SHAP] Not available (library not installed)")

        importance_dict['SHAP'] = np.ones(len(feature_names)) / len(feature_names)
        timing_dict['SHAP'] = 0.0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 5. Model-based importance (W1 APPROACH for LTCN)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    model_based_scores = np.ones(len(feature_names))
    try:
        if hasattr(model, 'coef_'):
            # Linear models (Logistic Regression)
            scores = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)
            if scores.sum() > 0:
                model_based_scores = scores / np.sum(scores)
        elif hasattr(model, 'feature_importances_'):
            # Tree-based models (RF, GB, XGBoost, LightGBM, CatBoost, AdaBoost)
            fi = model.feature_importances_
            if fi.sum() > 0:
                model_based_scores = fi / np.sum(fi)
        elif model_name == 'ltcn' and hasattr(model, 'W1') and model.W1 is not None:
            # LTCN: W1 approach (input layer) - consistent with SHAP, LIME, PFI
            W1 = model.W1  # shape: (n_features, n_features)
            importance = np.sum(np.abs(W1), axis=1)
            if importance.sum() > 0:
                model_based_scores = importance / importance.sum()
            else:
                model_based_scores = np.ones(len(feature_names)) / len(feature_names)
            print(f"    [LTCN] W1 input-layer importance computed")
        else:
            model_based_scores = np.ones(len(feature_names)) / len(feature_names)

    except Exception as e:
        print(f"    [Model-based] Error: {str(e)[:50]}")
        model_based_scores = np.ones(len(feature_names)) / len(feature_names)

    importance_dict['LTCN'] = model_based_scores
    importance_dict['XGBoost'] = model_based_scores

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # 6. XAI Timing Summary
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    total_time = sum(timing_dict.values())

    print(f"\n  [XAI TIMING SUMMARY]")
    print(f"  {'='*60}")
    for method, elapsed in sorted(timing_dict.items(), key=lambda x: x[1], reverse=True):
        pct = (elapsed / total_time * 100) if total_time > 0 else 0
        print(f"    {method:10s}: {elapsed:6.2f}s ({pct:5.1f}%)")
    print(f"  {'='*60}")
    print(f"    TOTAL:      {total_time:6.2f}s\n")

    return importance_dict

# TRAIN MODELS
def multiclass_brier_score(y_true_one_hot, y_prob, n_classes_local=2):
    if y_true_one_hot is None:
        return np.nan
    if not isinstance(y_prob, np.ndarray):
        y_prob = np.array(y_prob)

    if y_prob.ndim == 1:
        if n_classes_local == 2:
            y_prob = np.vstack([1 - y_prob, y_prob]).T
        else:
            return np.nan

    if y_prob.shape[1] != n_classes_local:
        if y_prob.shape[1] == 1 and n_classes_local == 2:
            y_prob = np.hstack([1 - y_prob, y_prob])
        else:
            return np.nan

    y_prob = np.nan_to_num(y_prob)

    total_brier = 0
    for i in range(n_classes_local):
        y_true_class_i = y_true_one_hot[:, i]
        y_prob_class_i = y_prob[:, i]
        y_prob_class_i = np.clip(y_prob_class_i, 0, 1)
        total_brier += brier_score_loss(y_true_class_i, y_prob_class_i)

    return total_brier / n_classes_local if n_classes_local > 0 else np.nan

def _safe_proba_v13(P, K=2):
    if not isinstance(P, np.ndarray):
        P = np.array(P)
    if P.ndim == 1:
        if K == 2:
            P = np.column_stack([1 - P, P])
        else:
            temp_P = np.zeros((len(P), K))
            temp_P[:, 0] = P
            P = temp_P
    if P.shape[1] < K:
        P_new = np.zeros((P.shape[0], K))
        P_new[:, :P.shape[1]] = P
        P = P_new
    P = np.clip(P, 1e-15, 1 - 1e-15)
    P = P / P.sum(axis=1, keepdims=True)
    return P

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ EARLY STOPPING ƒ∞√áƒ∞N VALƒ∞DATƒ∞ON SPLIT FONKSƒ∞YONU 18.11.25 tarihinde eklendi. gayemiz overfitting engellemek agac tipi modeller adina
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_validation_split(X, y, test_size=0.15, random_state=42):
    """
    Create validation split for early stopping

    Args:
        X: Training features
        y: Training labels
        test_size: Validation set size (default: 15%)
        random_state: Random seed

    Returns:
        X_train, X_val, y_train, y_val
    """
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(
        X, y,
        test_size=test_size,
        stratify=y,
        random_state=random_state
    )

    print(f"  [VAL SPLIT] Train: {len(X_train):,} | Val: {len(X_val):,}")

    return X_train, X_val, y_train, y_val

# TRAIN MODELS
def train_models(X_train, X_test, y_train, y_test,
                 X_val=None, y_val=None,
                 features=None, seed=42,
                 CONFIG=None, OUT_DIR="./outputs", N_CLASSES=2):
    """
    Model training with Optuna OR Pre-trained Parameters

    Args:
        X_train: Training features (SMOTE'd if enabled)
        X_test: Test features
        y_train: Training labels (SMOTE'd if enabled)
        y_test: Test labels
        X_val: Validation features (NEVER SMOTE'd, optional)
        y_val: Validation labels (NEVER SMOTE'd, optional)
        features: Feature names
        seed: Random seed
        CONFIG: Configuration dict
        OUT_DIR: Output directory
        N_CLASSES: Number of classes

    Returns:
        results: Model performance metrics
        models_dict: Trained model objects
        xai_results: XAI analysis results
        validation_data: Tuple of (X_val, y_val) for ablation
    """

    if CONFIG is None:
        CONFIG = {}

    print("\n5Ô∏è‚É£ TRAINING MODELS...\n")
    results = {}
    models_dict = {}
    xai_results = {}
    best_params_dict = {}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ PRE-TRAINED MODE CHECK
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    USE_PRETRAINED = CONFIG.get("use_pretrained_params", False)
    PRETRAINED_PARAMS = CONFIG.get("best_params", {})

    if USE_PRETRAINED:
        print("="*80)
        print("üöÄ PRE-TRAINED MODE ACTIVE")
        print("="*80)
        print(f"  ‚Ä¢ Models with pre-trained params: {len(PRETRAINED_PARAMS)}")
        print(f"  ‚Ä¢ Available: {', '.join(PRETRAINED_PARAMS.keys())}")
        print(f"  ‚Ä¢ Optuna optimization: SKIPPED")
        print(f"  ‚Ä¢ Expected time saving: ~85-90%")
        print("="*80 + "\n")
    else:
        print("  ‚ÑπÔ∏è  Standard mode: Optuna optimization will be used\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ONE-HOT ENCODING
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        y_train_one_hot = label_binarize(y_train, classes=np.unique(y_train))
        y_test_one_hot = label_binarize(y_test, classes=np.unique(y_train))
        n_classes_local = y_train_one_hot.shape[1]
        print(f"  One-hot encoded ({n_classes_local} classes)")
    except ValueError as e:
        print(f"  Binarize error: {e}")
        y_train_one_hot = None
        y_test_one_hot = None
        n_classes_local = N_CLASSES

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CV & OPTUNA CONFIG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    n_splits = CONFIG.get('cv', {}).get('folds', 5)
    cv_splitter = TimeSeriesSplit(n_splits=n_splits)

    n_trials = CONFIG.get('optuna', {}).get('n_trials', 10)
    timeout = CONFIG.get('optuna', {}).get('timeout', 1000)

    all_models_in_config = CONFIG.get('models', [])

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ VALIDATION SET KONTROL√ú
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    early_stop_config = CONFIG.get('early_stopping', {})
    USE_EARLY_STOPPING = early_stop_config.get('enabled', False)

    print("="*80)
    print("üîç VALIDATION SET STATUS")
    print("="*80 + "\n")

    if USE_EARLY_STOPPING:
        if X_val is None or y_val is None:
            print("‚ö†Ô∏è  WARNING: Early stopping enabled but no validation set provided!")
            print("   Validation should be created in ADIM 3.7 (before SMOTE)")
            print("   Falling back to no early stopping for this run\n")
            USE_EARLY_STOPPING = False
            X_train_for_models = X_train
            y_train_for_models = y_train
        else:
            print(f"‚úÖ Validation set received from main execution")
            print(f"   ‚Ä¢ Samples: {len(X_val):,}")
            print(f"   ‚Ä¢ Features: {X_val.shape[1]}")
            print(f"   ‚Ä¢ Source: PRE-SMOTE split (ADIM 3.7)")
            print(f"   ‚Ä¢ Contains ZERO synthetic samples ‚úÖ")

            val_counts = y_val.value_counts().sort_index()
            print(f"\n   üìä Validation class distribution:")
            print(f"      ‚Ä¢ Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)")
            print(f"      ‚Ä¢ Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)")
            print(f"      ‚Ä¢ Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)")

            print(f"\n   üéØ Will be used for:")
            print(f"      ‚Ä¢ Early stopping (XGBoost, LightGBM, CatBoost)")
            print(f"      ‚Ä¢ Ablation analysis (G28, G29)")
            print(f"      ‚Ä¢ XAI verification\n")

            X_train_for_models = X_train
            y_train_for_models = y_train

            print(f"   üìä Training data (post-SMOTE):")
            print(f"      ‚Ä¢ Samples: {len(X_train_for_models):,}")
            print(f"      ‚Ä¢ May contain synthetic samples (correct!)\n")
    else:
        print("‚ÑπÔ∏è  Early stopping DISABLED")
        print("   ‚Ä¢ All training data will be used")
        print("   ‚Ä¢ No validation split needed")
        print("   ‚Ä¢ Ablation will use test set (less ideal)\n")

        X_train_for_models = X_train
        y_train_for_models = y_train
        X_val = None
        y_val = None

    print("="*80 + "\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ SMOTE CONFIG VE PIPELINE HAZIRLIƒûI
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print("\n" + "="*80)
    print("üî¨ CROSS-VALIDATION STRATEGY")
    print("="*80)

    class_balancing_config = CONFIG.get("class_balancing", {})
    USE_SMOTE_IN_CV = class_balancing_config.get("use_smote", False)

    if USE_SMOTE_IN_CV:
        print("  ‚úÖ SMOTE will be applied PER FOLD (ImbPipeline)")
        print("  ‚úÖ Validation folds will be SYNTHETIC-FREE")
        print("  ‚úÖ This prevents overfitting to synthetic samples\n")

        SMOTE_STRATEGY = class_balancing_config.get("strategy", "auto")
        K_NEIGHBORS = class_balancing_config.get("k_neighbors", 5)

        print(f"  üìä SMOTE Strategy: {SMOTE_STRATEGY}")
        print(f"  üìä K-Neighbors: {K_NEIGHBORS}\n")

        if SMOTE_STRATEGY == "targeted":
            y_counts = y_train.value_counts().sort_index()
            current_away = y_counts.get(0, 0)
            current_draw = y_counts.get(1, 0)
            current_home = y_counts.get(2, 0)

            avg_other = int((current_away + current_home) / 2)
            target_draw_ratio = class_balancing_config.get("target_draw_ratio", 1.2)
            target_draw = int(avg_other * target_draw_ratio)
            target_draw = max(target_draw, current_draw)

            SAMPLING_STRATEGY_DICT = {
                0: current_away,
                1: target_draw,
                2: current_home
            }

            print(f"  üéØ Targeted Sampling Strategy:")
            print(f"     ‚Ä¢ Away Win (0): {current_away:,} ‚Üí {SAMPLING_STRATEGY_DICT[0]:,} (no change)")
            print(f"     ‚Ä¢ Draw (1):     {current_draw:,} ‚Üí {SAMPLING_STRATEGY_DICT[1]:,} (+{target_draw-current_draw:,})")
            print(f"     ‚Ä¢ Home Win (2): {current_home:,} ‚Üí {SAMPLING_STRATEGY_DICT[2]:,} (no change)")
        else:
            SAMPLING_STRATEGY_DICT = 'auto'
            print(f"  üìä Using 'auto' strategy (balance to majority class)")
    else:
        print("  ‚ÑπÔ∏è  SMOTE DISABLED in CV")
        print("  ‚ÑπÔ∏è  Models will train on original class distribution")
        SAMPLING_STRATEGY_DICT = 'auto'
        K_NEIGHBORS = 5

    print("="*80 + "\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # MODEL TRAINING LOOP
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    for model_name in all_models_in_config:

        start_model_time = time.time()
        model = None
        optimization_failed = False

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ DECISION: PRE-TRAINED vs OPTUNA
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if USE_PRETRAINED and model_name in PRETRAINED_PARAMS:
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üöÄ FAST PATH: PRE-TRAINED PARAMETERS
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            results_key = f"{model_name.upper()} (Pre-trained)"
            best_params = PRETRAINED_PARAMS[model_name]
            best_params_dict[model_name] = best_params

            print(f"\n{'='*60}")
            print(f"üöÄ {model_name.upper()} - USING PRE-TRAINED PARAMETERS")
            print(f"{'='*60}")
            print(f"  Parameters: {best_params}")
            print(f"  Optuna: SKIPPED ‚úÖ")

        elif CONFIG.get('optuna', {}).get('n_trials', 0) > 0:
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üî¨ SLOW PATH: OPTUNA OPTIMIZATION
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            results_key = f"{model_name.upper()} (Optuna)"
            print(f"\nOptimizing {model_name.upper()}...")

            needs_validation = model_name in ['xgboost', 'lightgbm', 'catboost']

            if needs_validation and USE_EARLY_STOPPING and X_val is not None:
                X_val_model = X_val
                y_val_model = y_val
                print(f"  [EARLY STOP] Using PRE-SMOTE validation set: {len(X_val_model):,} samples")
            else:
                X_val_model = None
                y_val_model = None

            def objective(trial):
                try:
                    if USE_SMOTE_IN_CV:
                        smote_step = SMOTE(
                            sampling_strategy=SAMPLING_STRATEGY_DICT,
                            random_state=seed,
                            k_neighbors=K_NEIGHBORS
                        )

                    # LOGISTIC REGRESSION
                    if model_name == 'lr':
                        C = trial.suggest_float('C', 1e-4, 1e1, log=True)

                        base_model = LogisticRegression(
                            C=C,
                            max_iter=1000,
                            random_state=seed,
                            multi_class='ovr',
                            n_jobs=-1
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    # RANDOM FOREST
                    elif model_name == 'rf':
                        n_estimators = trial.suggest_int('n_estimators', 50, 300)
                        max_depth = trial.suggest_int('max_depth', 5, 12)
                        min_samples_split = trial.suggest_int('min_samples_split', 5, 30)

                        base_model = RandomForestClassifier(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            min_samples_split=min_samples_split,
                            random_state=seed,
                            n_jobs=-1
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    # GRADIENT BOOSTING
                    elif model_name == 'gb':
                        max_iter = trial.suggest_int('max_iter', 50, 500)
                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)
                        max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 10, 50)

                        base_model = HistGradientBoostingClassifier(
                            max_iter=max_iter,
                            learning_rate=learning_rate,
                            max_leaf_nodes=max_leaf_nodes,
                            random_state=seed
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    # XGBOOST
                    elif model_name == 'xgboost' and HAS_XGB:
                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
                        max_depth = trial.suggest_int('max_depth', 3, 10)
                        subsample = trial.suggest_float('subsample', 0.6, 1.0)
                        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)
                        min_child_weight = trial.suggest_int('min_child_weight', 1, 7)
                        gamma = trial.suggest_float('gamma', 0, 0.5)
                        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)
                        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)

                        base_model = xgb.XGBClassifier(
                            n_estimators=300,
                            learning_rate=learning_rate,
                            max_depth=max_depth,
                            subsample=subsample,
                            colsample_bytree=colsample_bytree,
                            min_child_weight=min_child_weight,
                            gamma=gamma,
                            reg_alpha=reg_alpha,
                            reg_lambda=reg_lambda,
                            random_state=seed,
                            eval_metric='mlogloss',
                            objective='multi:softprob',
                            n_jobs=-1,
                            verbosity=0
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()

                        return score

                    # LIGHTGBM
                    elif model_name == 'lightgbm' and HAS_LGB:
                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
                        num_leaves = trial.suggest_int('num_leaves', 20, 100)
                        max_depth = trial.suggest_int('max_depth', 3, 12)
                        min_child_samples = trial.suggest_int('min_child_samples', 10, 50)
                        subsample = trial.suggest_float('subsample', 0.6, 1.0)
                        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)
                        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)
                        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)

                        base_model = lgb.LGBMClassifier(
                            n_estimators=300,
                            learning_rate=learning_rate,
                            num_leaves=num_leaves,
                            max_depth=max_depth,
                            min_child_samples=min_child_samples,
                            subsample=subsample,
                            colsample_bytree=colsample_bytree,
                            reg_alpha=reg_alpha,
                            reg_lambda=reg_lambda,
                            random_state=seed,
                            objective='multiclass',
                            num_class=n_classes_local,
                            n_jobs=-1,
                            verbose=-1
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()

                        return score

                    # CATBOOST
                    elif model_name == 'catboost' and HAS_CB:
                        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
                        depth = trial.suggest_int('depth', 4, 10)
                        l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1, 10)
                        border_count = trial.suggest_int('border_count', 32, 255)
                        bagging_temperature = trial.suggest_float('bagging_temperature', 0, 1)

                        base_model = cb.CatBoostClassifier(
                            iterations=300,
                            learning_rate=learning_rate,
                            depth=depth,
                            l2_leaf_reg=l2_leaf_reg,
                            border_count=border_count,
                            bagging_temperature=bagging_temperature,
                            random_state=seed,
                            loss_function='MultiClass',
                            classes_count=n_classes_local,
                            verbose=False,
                            thread_count=-1
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()

                        return score

                    # SVM
                    elif model_name == 'svm':
                        C = trial.suggest_float('C', 0.1, 10, log=True)
                        gamma = trial.suggest_float('gamma', 0.01, 1, log=True)

                        base_model = SVC(
                            C=C,
                            gamma=gamma,
                            kernel='rbf',
                            probability=True,
                            random_state=seed
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    # ADABOOST
                    elif model_name == 'ada':
                        n_estimators = trial.suggest_int('n_estimators', 50, 500)
                        learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0, log=True)

                        base_est = DecisionTreeClassifier(max_depth=1, random_state=seed)
                        base_model = AdaBoostClassifier(
                            estimator=base_est,
                            n_estimators=n_estimators,
                            learning_rate=learning_rate,
                            random_state=seed,
                            algorithm='SAMME'
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    # LTCN
                    elif model_name == 'ltcn':
                        T = trial.suggest_int('T', 10, 50)
                        alpha = trial.suggest_float('alpha', 0.1, 0.9)
                        beta = trial.suggest_float('beta', 0.1, 0.9)
                        ridge_alpha = trial.suggest_float('ridge_alpha', 1e-5, 1e-1, log=True)

                        base_model = LTCN(
                            T=T,
                            alpha=alpha,
                            beta=beta,
                            ridge_alpha=ridge_alpha,
                            random_state=seed
                        )

                        if USE_SMOTE_IN_CV:
                            trial_model = ImbPipeline([
                                ('smote', smote_step),
                                ('clf', base_model)
                            ])
                        else:
                            trial_model = base_model

                        score = cross_val_score(
                            trial_model,
                            X_train,
                            y_train,
                            cv=cv_splitter,
                            scoring='f1_weighted',
                            n_jobs=-1
                        ).mean()
                        return score

                    else:
                        return 0.0

                except Exception as e_trial:
                    print(f"  ‚ö†Ô∏è Trial failed: {str(e_trial)[:50]}")
                    return 0.0

            # OPTUNA OPTIMIZATION
            try:
                study = optuna.create_study(
                    direction='maximize',
                    pruner=MedianPruner(n_warmup_steps=3),
                    sampler=TPESampler(seed=seed)
                )
                optuna.logging.set_verbosity(optuna.logging.WARNING)
                study.optimize(objective, n_trials=n_trials, timeout=timeout, n_jobs=1, show_progress_bar=True)
                optuna.logging.set_verbosity(optuna.logging.INFO)

                best_params = study.best_trial.params
                best_params_dict[model_name] = best_params
                print(f"  ‚úì Best CV F1: {study.best_trial.value:.4f}")

                if USE_SMOTE_IN_CV:
                    print(f"  ‚ÑπÔ∏è  (CV with ImbPipeline - validation folds are synthetic-free)")

            except Exception as e_opt:
                print(f"‚ùå Optimization error ({model_name}): {e_opt}")
                best_params_dict[model_name] = "Optimization Failed"
                optimization_failed = True
                continue

        else:
            # No optimization configured
            print(f"\n‚ö†Ô∏è  {model_name.upper()} - No optimization configured, skipping...")
            continue

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ COMMON PATH: FINAL MODEL TRAINING (Both Pre-trained & Optuna)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print(f"  [FINAL] Training final model with best params...")

        # SMOTE'u t√ºm train setine uygula (final model i√ßin)
        if USE_SMOTE_IN_CV:
            print(f"  [SMOTE] Applying SMOTE to full train set...")
            smote_final = SMOTE(
                sampling_strategy=SAMPLING_STRATEGY_DICT,
                random_state=seed,
                k_neighbors=K_NEIGHBORS
            )
            X_train_smote_final, y_train_smote_final = smote_final.fit_resample(X_train, y_train)
            print(f"  [SMOTE] {len(X_train):,} ‚Üí {len(X_train_smote_final):,} samples")
        else:
            X_train_smote_final = X_train
            y_train_smote_final = y_train

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # MODEL-SPECIFIC FINAL TRAINING
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        try:
            if model_name == 'lr':
                model = LogisticRegression(
                    max_iter=1000,
                    random_state=seed,
                    multi_class='ovr',
                    n_jobs=-1,
                    **best_params
                )
                model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'rf':
                model = RandomForestClassifier(
                    random_state=seed,
                    n_jobs=-1,
                    **best_params
                )
                model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'gb':
                model = HistGradientBoostingClassifier(
                    random_state=seed,
                    **best_params
                )
                model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'xgboost' and HAS_XGB:
                import xgboost
                xgb_version = tuple(map(int, xgboost.__version__.split('.')[:2]))
                early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)

                if xgb_version >= (2, 0):
                    model = xgb.XGBClassifier(
                        n_estimators=500,
                        random_state=seed,
                        eval_metric='mlogloss',
                        objective='multi:softprob',
                        n_jobs=-1,
                        verbosity=0,
                        early_stopping_rounds=early_stop_rounds if X_val is not None else None,
                        **best_params
                    )

                    if X_val is not None:
                        model.fit(
                            X_train_smote_final, y_train_smote_final,
                            eval_set=[(X_val, y_val)],
                            verbose=False
                        )
                    else:
                        model.fit(X_train_smote_final, y_train_smote_final, verbose=False)

                else:
                    model = xgb.XGBClassifier(
                        n_estimators=500,
                        random_state=seed,
                        eval_metric='mlogloss',
                        objective='multi:softprob',
                        n_jobs=-1,
                        verbosity=0,
                        **best_params
                    )

                    if X_val is not None:
                        model.fit(
                            X_train_smote_final, y_train_smote_final,
                            eval_set=[(X_val, y_val)],
                            early_stopping_rounds=early_stop_rounds,
                            verbose=False
                        )
                    else:
                        model.fit(X_train_smote_final, y_train_smote_final, verbose=False)

                try:
                    if hasattr(model, 'best_iteration'):
                        print(f"  ‚úì Best iteration: {model.best_iteration}")
                    elif hasattr(model, 'get_booster'):
                        print(f"  ‚úì Best iteration: {model.get_booster().best_iteration}")
                except:
                    pass

            elif model_name == 'lightgbm' and HAS_LGB:
                early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)

                model = lgb.LGBMClassifier(
                    n_estimators=500,
                    random_state=seed,
                    objective='multiclass',
                    num_class=n_classes_local,
                    n_jobs=-1,
                    verbose=-1,
                    **best_params
                )

                if X_val is not None:
                    model.fit(
                        X_train_smote_final, y_train_smote_final,
                        eval_set=[(X_val, y_val)],
                        eval_metric='multi_logloss',
                        callbacks=[
                            lgb.early_stopping(stopping_rounds=early_stop_rounds, verbose=False),
                            lgb.log_evaluation(period=0)
                        ]
                    )
                    print(f"  ‚úì Best iteration: {model.best_iteration_} (out of 500)")
                else:
                    model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'catboost' and HAS_CB:
                early_stop_rounds = CONFIG.get('early_stopping', {}).get('rounds', 50)

                model = cb.CatBoostClassifier(
                    iterations=500,
                    random_state=seed,
                    loss_function='MultiClass',
                    classes_count=n_classes_local,
                    verbose=False,
                    thread_count=-1,
                    early_stopping_rounds=early_stop_rounds if X_val is not None else None,
                    **best_params
                )

                if X_val is not None:
                    model.fit(
                        X_train_smote_final, y_train_smote_final,
                        eval_set=(X_val, y_val),
                        verbose=False,
                        plot=False
                    )
                    print(f"  ‚úì Best iteration: {model.get_best_iteration()} (out of 500)")
                else:
                    model.fit(X_train_smote_final, y_train_smote_final, verbose=False, plot=False)

            elif model_name == 'svm':
                model = SVC(
                    kernel='rbf',
                    probability=True,
                    random_state=seed,
                    **best_params
                )
                model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'ada':
                base_est = DecisionTreeClassifier(max_depth=1, random_state=seed)
                model = AdaBoostClassifier(
                    estimator=base_est,
                    random_state=seed,
                    algorithm='SAMME',
                    **best_params
                )
                model.fit(X_train_smote_final, y_train_smote_final)

            elif model_name == 'ltcn':
                model = LTCN(random_state=seed, **best_params)
                model.fit(X_train_smote_final, y_train_smote_final)

            models_dict[model_name] = model
            print(f"  ‚úÖ Final model trained successfully")

        except Exception as e_train:
            print(f"  ‚ùå Training error: {e_train}")
            continue

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # METRICS CALCULATION
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if model is not None:
            try:
                y_train_pred = model.predict(X_train)
                y_test_pred = model.predict(X_test)
                y_train_proba = _safe_proba_v13(model.predict_proba(X_train), n_classes_local)
                y_test_proba = _safe_proba_v13(model.predict_proba(X_test), n_classes_local)

                results[results_key] = {
                    'train_acc': accuracy_score(y_train, y_train_pred),
                    'test_acc': accuracy_score(y_test, y_test_pred),
                    'train_f1': f1_score(y_train, y_train_pred, average='weighted'),
                    'test_f1': f1_score(y_test, y_test_pred, average='weighted'),
                    'train_kappa': cohen_kappa_score(y_train, y_train_pred),
                    'test_kappa': cohen_kappa_score(y_test, y_test_pred),
                    'train_auc': roc_auc_score(y_train, y_train_proba, multi_class='ovr'),
                    'test_auc': roc_auc_score(y_test, y_test_proba, multi_class='ovr'),
                    'train_loss': log_loss(y_train, y_train_proba),
                    'test_loss': log_loss(y_test, y_test_proba),
                    'train_brier': multiclass_brier_score(y_train_one_hot, y_train_proba, n_classes_local),
                    'test_brier': multiclass_brier_score(y_test_one_hot, y_test_proba, n_classes_local),
                    'test_rps': ranked_probability_score(y_test, y_test_proba),
                    'test_ece': expected_calibration_error(y_test, y_test_proba),
                }

                # Per-class metrics
                try:
                    from sklearn.metrics import precision_recall_fscore_support

                    cm_train = confusion_matrix(y_train, y_train_pred)
                    cm_test = confusion_matrix(y_test, y_test_pred)

                    train_per_class_acc = cm_train.diagonal() / cm_train.sum(axis=1)
                    test_per_class_acc = cm_test.diagonal() / cm_test.sum(axis=1)

                    precision_train, recall_train, f1_train_pc, _ = precision_recall_fscore_support(
                        y_train, y_train_pred, average=None, zero_division=0
                    )
                    precision_test, recall_test, f1_test_pc, support_test = precision_recall_fscore_support(
                        y_test, y_test_pred, average=None, zero_division=0
                    )

                    class_names = ['Away Win', 'Draw', 'Home Win']

                    results[results_key]['per_class_metrics_train'] = {
                        'accuracy': {class_names[i]: float(train_per_class_acc[i]) for i in range(len(class_names))},
                        'precision': {class_names[i]: float(precision_train[i]) for i in range(len(class_names))},
                        'recall': {class_names[i]: float(recall_train[i]) for i in range(len(class_names))},
                        'f1': {class_names[i]: float(f1_train_pc[i]) for i in range(len(class_names))},
                        'support': {class_names[i]: int(cm_train[i].sum()) for i in range(len(class_names))}
                    }

                    results[results_key]['per_class_metrics'] = {
                        'accuracy': {class_names[i]: float(test_per_class_acc[i]) for i in range(len(class_names))},
                        'precision': {class_names[i]: float(precision_test[i]) for i in range(len(class_names))},
                        'recall': {class_names[i]: float(recall_test[i]) for i in range(len(class_names))},
                        'f1': {class_names[i]: float(f1_test_pc[i]) for i in range(len(class_names))},
                        'support': {class_names[i]: int(support_test[i]) for i in range(len(class_names))}
                    }

                    print(f"  ‚úì Per-class metrics calculated")

                except Exception as e_per_class:
                    print(f"  ‚ö†Ô∏è Per-class metrics error: {e_per_class}")

            except Exception as e_metrics:
                print(f"  ‚ùå Metrics calculation error: {e_metrics}")

        end_model_time = time.time()
        print(f"  Time: {(end_model_time - start_model_time):.2f}s")
        gc.collect()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # XAI ANALYSIS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print("\n" + "="*100)
    print("üéØ XAI ANALYSIS - BEST MODEL SELECTION")
    print("="*100 + "\n")

    valid_models_f1 = {}
    for model_key in results.keys():
        f1_value = results[model_key].get('test_f1', np.nan)
        if pd.notna(f1_value):
            valid_models_f1[model_key] = f1_value

    if not valid_models_f1:
        print("‚ùå No valid models found! Skipping XAI analysis.\n")
        print("="*100 + "\n")
    else:
        sorted_models = sorted(
            valid_models_f1.items(),
            key=lambda x: x[1],
            reverse=True
        )

        print("üìä Model Rankings (Top 5):\n")
        for rank, (model_name, f1_value) in enumerate(sorted_models[:5], 1):
            emoji = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "  "
            print(f"  {emoji} #{rank}: {model_name:25s} ‚Üí F1 = {f1_value:.6f}")

        best_model_key = sorted_models[0][0]
        best_f1_score = sorted_models[0][1]
        best_model_short = best_model_key.split(' ')[0].lower()

        print(f"\nüèÜ SELECTED BEST MODEL: {best_model_key}")
        print(f"üìä Test F1-Score: {best_f1_score:.6f}")
        print(f"üî¨ Running Explainable AI Analysis...\n")

        if len(sorted_models) > 1:
            second_best_f1 = sorted_models[1][1]
            f1_diff = best_f1_score - second_best_f1

            if f1_diff < 0.001:
                print(f"‚ö†Ô∏è  WARNING: Very close to 2nd place ({sorted_models[1][0]})")
                print(f"   Difference: {f1_diff:.6f} ({f1_diff*100:.3f}%)\n")

        print("   XAI Methods:")
        print("   ‚îú‚îÄ Permutation Feature Importance (PFI)")
        print("   ‚îú‚îÄ Predictive Mutual Information (PMI)")
        print("   ‚îú‚îÄ Sensitivity-based Feature Importance (SOFI)")
        print("   ‚îú‚îÄ SHAP Values (if applicable)")
        print("   ‚îú‚îÄ Model-based Importance")
        print("   ‚îî‚îÄ Aggregated Analysis\n")

        if best_model_short in models_dict:
            try:
                start_xai_time = time.time()

                xai_results[best_model_short] = get_feature_importance(
                    model=models_dict[best_model_short],
                    X=X_train,
                    y=y_train,
                    model_name=best_model_short,
                    feature_names=features,
                    config=CONFIG
                )

                end_xai_time = time.time()
                xai_duration = end_xai_time - start_xai_time

                print(f"\n   ‚úÖ XAI Analysis Completed Successfully!")
                print(f"   ‚è±Ô∏è  Duration: {xai_duration:.2f}s ({xai_duration/60:.2f} minutes)")
                print(f"   üìã Methods Analyzed: {len(xai_results[best_model_short])}")

                print("\n   Method Results:")
                for method_name, importance_scores in xai_results[best_model_short].items():
                    if importance_scores is not None and len(importance_scores) > 0:
                        top_3_idx = np.argsort(importance_scores)[-3:][::-1]
                        top_3_idx = np.atleast_1d(top_3_idx).flatten()
                        top_3_features = [features[int(i)] for i in top_3_idx]
                        print(f"   ‚îú‚îÄ {method_name:8s}: {', '.join(top_3_features[:2])}...")

            except Exception as e_xai:
                print(f"\n   ‚ùå XAI Analysis Failed: {e_xai}")
                print(f"   Continuing without XAI results...\n")
                import traceback
                traceback.print_exc()
        else:
            print(f"   ‚ö†Ô∏è  Warning: Best model '{best_model_short}' not found in models_dict")
            print(f"   Available models: {list(models_dict.keys())}\n")

    print("="*100 + "\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PER-CLASS PERFORMANCE ANALYSIS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print("\n" + "="*100)
    print("üìä PER-CLASS PERFORMANCE ANALYSIS")
    print("="*100 + "\n")

    try:
        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys()
                           if pd.notna(results[m].get('test_f1'))}

        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            metrics = results[best_model]

            print(f"üèÜ BEST MODEL: {best_model}\n")

            if 'per_class_metrics' in metrics:
                class_names = ['Away Win', 'Draw', 'Home Win']
                emojis = ['‚úàÔ∏è', 'ü§ù', 'üè†']

                print("  Class-wise Performance:\n")
                print("  " + "-"*80)

                for emoji, class_name in zip(emojis, class_names):
                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)
                    prec = metrics['per_class_metrics']['precision'].get(class_name, 0)
                    rec = metrics['per_class_metrics']['recall'].get(class_name, 0)
                    f1 = metrics['per_class_metrics']['f1'].get(class_name, 0)
                    sup = metrics['per_class_metrics']['support'].get(class_name, 0)

                    print(f"  {emoji} {class_name:10s}:")
                    print(f"     ‚Ä¢ Accuracy:  {acc:.4f} ({acc*100:5.2f}%)")
                    print(f"     ‚Ä¢ Precision: {prec:.4f}")
                    print(f"     ‚Ä¢ Recall:    {rec:.4f}")
                    print(f"     ‚Ä¢ F1-Score:  {f1:.4f}")
                    print(f"     ‚Ä¢ Support:   {sup:,} samples")
                    print()

                print("  " + "-"*80)

                overall_acc = metrics.get('test_acc', 0)
                overall_f1 = metrics.get('test_f1', 0)

                print(f"\n  üìà Overall Performance:")
                print(f"     ‚Ä¢ Weighted Accuracy: {overall_acc:.4f} ({overall_acc*100:.2f}%)")
                print(f"     ‚Ä¢ Weighted F1-Score: {overall_f1:.4f}")

                acc_dict = metrics['per_class_metrics']['accuracy']
                best_class = max(acc_dict, key=acc_dict.get)
                worst_class = min(acc_dict, key=acc_dict.get)

                print(f"\n  üéØ Insights:")
                print(f"     ‚Ä¢ Best Predicted:  {best_class} ({acc_dict[best_class]*100:.2f}%)")
                print(f"     ‚Ä¢ Worst Predicted: {worst_class} ({acc_dict[worst_class]*100:.2f}%)")
                print(f"     ‚Ä¢ Class Imbalance: {max(acc_dict.values()) - min(acc_dict.values()):.4f}")

    except Exception as e:
        print(f"  ‚ùå Per-class analysis error: {e}")

    print("\n" + "="*100 + "\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HYPERPARAMETER KAYDETME
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        params_path = os.path.join(OUT_DIR, "best_hyperparameters.json")
        safe_params_dict = {}
        for k, v in best_params_dict.items():
            if isinstance(v, dict):
                safe_params_dict[k] = {
                    p_k: (int(p_v) if isinstance(p_v, np.integer)
                          else float(p_v) if isinstance(p_v, np.floating)
                          else p_v)
                    for p_k, p_v in v.items()
                }
            else:
                safe_params_dict[k] = v

        with open(params_path, 'w') as f:
            filtered_params = {k: v for k, v in safe_params_dict.items()
                             if v != "Optimization Failed"}
            json.dump(filtered_params, f, indent=2)
        print(f"\n‚úÖ Hyperparameters saved: {params_path}")
    except Exception as e:
        print(f"\n‚ùå Could not save hyperparameters: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ RETURN PREPARATION & VALIDATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print("\n" + "="*100)
    print("üì¶ PREPARING RETURN VALUES")
    print("="*100 + "\n")

    print(f"[RETURN INFO] Function will return 4 values:")
    print(f"  1. results:       Performance metrics for {len(results)} models")
    print(f"  2. models_dict:   {len(models_dict)} trained model objects")
    print(f"  3. xai_results:   {len(xai_results)} XAI analyses")
    print(f"  4. validation_data: Tuple of (X_val, y_val)")

    if X_val is not None and y_val is not None:
        print(f"\n  ‚úÖ Validation set prepared:")
        print(f"     ‚Ä¢ X_val shape: {X_val.shape}")
        print(f"     ‚Ä¢ y_val length: {len(y_val)}")
        print(f"     ‚Ä¢ Source: PRE-SMOTE split (ADIM 3.7)")
        print(f"     ‚Ä¢ Contains: 100% REAL data (zero synthetic samples)")

        try:
            val_counts = y_val.value_counts().sort_index()
            print(f"\n     üìä Validation class distribution:")
            print(f"        ‚Ä¢ Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)")
            print(f"        ‚Ä¢ Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)")
            print(f"        ‚Ä¢ Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)")
        except Exception as e_dist:
            print(f"        ‚ö†Ô∏è  Could not compute distribution: {e_dist}")

        print(f"\n     üéØ Usage:")
        print(f"        ‚Ä¢ Ablation analysis (G28: XAI comparison)")
        print(f"        ‚Ä¢ Ablation analysis (G29: Cumulative importance)")
        print(f"        ‚Ä¢ XAI method verification")
        print(f"        ‚Ä¢ Early stopping (already used during training)")

        validation_data = (X_val, y_val)

    else:
        print(f"\n  ‚ö†Ô∏è  Validation set: NOT AVAILABLE")
        print(f"     ‚Ä¢ Reason: Early stopping disabled in CONFIG")
        print(f"     ‚Ä¢ Impact: Ablation will use test set (less ideal)")
        print(f"     ‚Ä¢ Recommendation: Enable early stopping for better ablation")

        validation_data = (None, None)

    print(f"\n[SANITY CHECKS]")

    if not results:
        print(f"  ‚ö†Ô∏è  WARNING: results dictionary is EMPTY!")
        print(f"     No models were successfully trained")
    else:
        print(f"  ‚úÖ Results: {len(results)} models trained")

    if len(models_dict) != len(results):
        print(f"  ‚ö†Ô∏è  WARNING: Mismatch between models_dict and results")
        print(f"     models_dict: {len(models_dict)}, results: {len(results)}")
    else:
        print(f"  ‚úÖ Models dict: Consistent with results")

    if not xai_results:
        print(f"  ‚ÑπÔ∏è  XAI results: Empty (will be populated after best model selection)")
    else:
        print(f"  ‚úÖ XAI results: {len(xai_results)} analyses")

    if validation_data[0] is not None:
        if validation_data[1] is None:
            print(f"  ‚ùå ERROR: X_val exists but y_val is None!")
        elif len(validation_data[0]) != len(validation_data[1]):
            print(f"  ‚ùå ERROR: X_val and y_val length mismatch!")
            print(f"     X_val: {len(validation_data[0])}, y_val: {len(validation_data[1])}")
        else:
            print(f"  ‚úÖ Validation data: Consistent")

    print(f"\n" + "="*100 + "\n")

    return results, models_dict, xai_results, validation_data

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ FINAL RETURN STATEMENT
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    return results, models_dict, xai_results, validation_data

    # ============================================================================
    # HYPERPARAMETER KAYDETME
    # ============================================================================
    try:
        params_path = os.path.join(OUT_DIR, "best_hyperparameters.json")
        safe_params_dict = {}
        for k, v in best_params_dict.items():
            if isinstance(v, dict):
                safe_params_dict[k] = {p_k: (int(p_v) if isinstance(p_v, np.integer) else float(p_v) if isinstance(p_v, np.floating) else p_v) for p_k, p_v in v.items()}
            else:
                safe_params_dict[k] = v

        with open(params_path, 'w') as f:
            filtered_params = {k: v for k, v in safe_params_dict.items() if v != "Optimization Failed"}
            json.dump(filtered_params, f, indent=2)
        print(f"\n‚úÖ Hyperparameters saved: {params_path}")
    except Exception as e:
        print(f"\n‚ùå Could not save hyperparameters: {e}")

    return results, models_dict, xai_results  # ‚Üê RETURN STATEMENT

# CREATE TABLES
def create_dynamic_tables(results):
    """Create result tables"""

    print("\n6Ô∏è‚É£ CREATING TABLES...\n")
    tables = {}
    models = list(results.keys())

    if not results:
        print("  No results")
        return tables

    # T1: Performance
    perf_data = []
    for model_key in models:
        metrics = results.get(model_key, {})
        perf_data.append({
            'Model': model_key,
            'Train Acc': f"{metrics.get('train_acc', np.nan):.4f}",
            'Test Acc': f"{metrics.get('test_acc', np.nan):.4f}",
            'Train F1': f"{metrics.get('train_f1', np.nan):.4f}",
            'Test F1': f"{metrics.get('test_f1', np.nan):.4f}",
        })
    tables['T1_Model_Performance'] = pd.DataFrame(perf_data)
    print("  ‚úì T1")

    # T3: AUC
    auc_data = []
    for model_key in models:
        metrics = results.get(model_key, {})
        auc_data.append({
            'Model': model_key,
            'Train AUC': f"{metrics.get('train_auc', np.nan):.4f}",
            'Test AUC': f"{metrics.get('test_auc', np.nan):.4f}",
            'ECE (Test)': f"{metrics.get('test_ece', np.nan):.4f}",
            'Kappa (Test)': f"{metrics.get('test_kappa', np.nan):.4f}",
        })
    tables['T3_AUC_Calibration'] = pd.DataFrame(auc_data)
    print("  ‚úì T3")

    # T4: Generalization
    cons_data = []
    for model_key in models:
        metrics = results.get(model_key, {})
        train_f1 = metrics.get('train_f1', np.nan)
        test_f1 = metrics.get('test_f1', np.nan)
        gap = abs(train_f1 - test_f1) if pd.notna(train_f1) and pd.notna(test_f1) else np.nan
        generalization = 'N/A'
        if pd.notna(gap):
            generalization = 'Good' if gap < 0.1 else 'Fair' if gap < 0.2 else 'Poor'

        cons_data.append({
            'Model': model_key,
            'Train-Test Gap (F1)': f"{gap:.4f}",
            'Generalization': generalization,
        })
    tables['T4_Generalization'] = pd.DataFrame(cons_data)
    print("  ‚úì T4")

    # T7: Kappa
    tables['T7_Kappa_Analysis'] = pd.DataFrame({
        'Model': models,
        'Train Kappa': [f"{results.get(m, {}).get('train_kappa', np.nan):.4f}" for m in models],
        'Test Kappa': [f"{results.get(m, {}).get('test_kappa', np.nan):.4f}" for m in models],
    })
    print("  ‚úì T7")

    # T8: Summary
    try:
        mean_test_acc = np.nanmean([results.get(m, {}).get('test_acc') for m in models])
        mean_test_f1 = np.nanmean([results.get(m, {}).get('test_f1') for m in models])
        mean_test_auc = np.nanmean([results.get(m, {}).get('test_auc') for m in models])
        mean_test_loss = np.nanmean([results.get(m, {}).get('test_loss') for m in models])

        tables['T8_Summary_Stats'] = pd.DataFrame({
            'Metric': ['Mean Test Acc', 'Mean Test F1', 'Mean Test AUC', 'Mean Test Loss'],
            'Value': [
                f"{mean_test_acc:.4f}", f"{mean_test_f1:.4f}",
                f"{mean_test_auc:.4f}", f"{mean_test_loss:.4f}",
            ],
        })
        print("  ‚úì T8")
    except:
        pass

    # T9: Best Model
    try:
        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}
        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            metrics = results[best_model]

            comparable_data = {
                'Metric': ['Accuracy', 'F1-Score', 'AUC-ROC', 'Cohen\'s Kappa'],
                'Value': [
                    f"{metrics.get('test_acc', np.nan):.4f}",
                    f"{metrics.get('test_f1', np.nan):.4f}",
                    f"{metrics.get('test_auc', np.nan):.4f}",
                    f"{metrics.get('test_kappa', np.nan):.4f}",
                ]
            }

            tables['T9_Best_Model_Performance'] = pd.DataFrame(comparable_data)
            print(f"  ‚úì T9 ({best_model})")
    except:
        pass
    # T10: Per-Class Performance Metrics
    try:
        per_class_rows = []
        class_names = ['Home Win', 'Draw', 'Away Win']

        for model_key in models:
            metrics = results.get(model_key, {})
            if 'per_class_metrics' in metrics:
                for class_name in class_names:
                    per_class_rows.append({
                        'Model': model_key,
                        'Class': class_name,
                        'Precision': f"{metrics['per_class_metrics']['precision'].get(class_name, 0):.4f}",
                        'Recall': f"{metrics['per_class_metrics']['recall'].get(class_name, 0):.4f}",
                        'F1-Score': f"{metrics['per_class_metrics']['f1'].get(class_name, 0):.4f}",
                        'Support': metrics['per_class_metrics']['support'].get(class_name, 0)
                    })

        if per_class_rows:
            tables['T10_Per_Class_Metrics'] = pd.DataFrame(per_class_rows)
            print("  ‚úì T10")
    except Exception as e:
        print(f"  ‚úó T10: {e}")

    print(f"\n‚úÖ Created {len(tables)} tables\n")
    return tables

    # ‚úÖ YENƒ∞ TABLO: T11 - Per-Class Accuracy Comparison
    try:
        per_class_acc_rows = []
        class_names = ['Away Win', 'Draw', 'Home Win']

        for model_key in models:
            metrics = results.get(model_key, {})

            # Test per-class accuracy
            if 'per_class_metrics' in metrics and 'accuracy' in metrics['per_class_metrics']:
                for class_name in class_names:
                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)
                    per_class_acc_rows.append({
                        'Model': model_key,
                        'Class': class_name,
                        'Accuracy': f"{acc:.4f}",
                        'Accuracy %': f"{acc*100:.2f}%"
                    })

        if per_class_acc_rows:
            tables['T11_Per_Class_Accuracy'] = pd.DataFrame(per_class_acc_rows)
            print("  ‚úì T11 (Per-Class Accuracy)")

    except Exception as e:
        print(f"  ‚úó T11: {e}")

    # ‚úÖ YENƒ∞ TABLO: T12 - Best Model Detailed Breakdown
    try:
        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}

        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            metrics = results[best_model]

            breakdown_rows = []

            if 'per_class_metrics' in metrics:
                class_names = ['Away Win', 'Draw', 'Home Win']

                for class_name in class_names:
                    breakdown_rows.append({
                        'Class': class_name,
                        'Accuracy': f"{metrics['per_class_metrics']['accuracy'].get(class_name, 0):.4f}",
                        'Precision': f"{metrics['per_class_metrics']['precision'].get(class_name, 0):.4f}",
                        'Recall': f"{metrics['per_class_metrics']['recall'].get(class_name, 0):.4f}",
                        'F1-Score': f"{metrics['per_class_metrics']['f1'].get(class_name, 0):.4f}",
                        'Support': metrics['per_class_metrics']['support'].get(class_name, 0)
                    })

            if breakdown_rows:
                tables['T12_Best_Model_Breakdown'] = pd.DataFrame(breakdown_rows)
                print(f"  ‚úì T12 (Best Model: {best_model})")

    except Exception as e:
        print(f"  ‚úó T12: {e}")

def create_feature_importance_comparison_v19_ABLATION(
    models_dict, xai_results, X_train, X_test, y_train, y_test,
    X_val, y_val,
    features, output_dir, results_dict
):
    """
    G28: XAI Method Comparison (Ablation-Based - Academic Style)
    """
    print("\n[G28-v19] Creating XAI Method Comparison (Ablation-Based - Academic Style)...")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 1: EN ƒ∞Yƒ∞ MODELƒ∞ BUL
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    valid_models_f1 = {
        m: results_dict[m].get('test_f1', -1)
        for m in results_dict.keys()
        if pd.notna(results_dict[m].get('test_f1'))
    }

    if not valid_models_f1:
        print("  ‚ö†Ô∏è No valid models found")
        return None

    best_model_key = max(valid_models_f1, key=valid_models_f1.get)
    best_model_short = best_model_key.split(' ')[0].lower()
    best_f1_score = valid_models_f1[best_model_key]

    print(f"  Best Model: {best_model_key} (F1: {best_f1_score:.4f})")

    # Validation set kontrol√º
    if X_val is None or y_val is None:
        print(f"  ‚ö†Ô∏è  WARNING: No validation set provided!")
        print(f"     Falling back to test set (less ideal for ablation)")
        X_ablation = X_test
        y_ablation = y_test
        ablation_set_name = "Test Set"
    else:
        print(f"  ‚úÖ Using validation set for ablation: {len(X_val):,} samples")
        X_ablation = X_val
        y_ablation = y_val
        ablation_set_name = "Validation Set"

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 2: XAI SONU√áLARINI AL
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if best_model_short not in xai_results:
        print(f"  ‚ö†Ô∏è No XAI results for {best_model_key}")
        return None

    xai_data = xai_results[best_model_short]
    model = models_dict[best_model_short]

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPER: Compute Ablation Curve (FIXED - No Early Stop, No Monotonicity)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    def compute_ablation_curve_normalized(model, X_data, y_data, feature_order,
                                          base_score, max_feats):
        """
        Compute normalized ablation curve: g(pi) = F1_ablated / F1_baseline

        FIXED VERSION:
        - No monotonicity constraint (natural behavior)
        - Runs until ALL features are ablated
        - All curves will converge to same point (~random guess)
        """
        from sklearn.metrics import f1_score

        if hasattr(X_data, 'values'):
            X_work = X_data.values.copy()
        else:
            X_work = X_data.copy()

        scores = [1.0]  # Start at 100%

        # ‚úÖ FIX: Use ALL features, not just max_feats
        n_features = X_work.shape[1]
        limit = min(len(feature_order), n_features)  # T√ºm feature'larƒ± kullan

        means = np.mean(X_work, axis=0)

        for i in range(limit):
            idx = feature_order[i]
            X_work[:, idx] = means[idx]

            y_pred = model.predict(X_work)
            current_f1 = f1_score(y_data, y_pred, average='weighted', zero_division=0)

            norm_score = current_f1 / base_score if base_score > 0 else 0

            # ‚úÖ FIX: Monotonicity constraint KALDIRILDI
            # Doƒüal davranƒ±≈ü - eƒüri inip √ßƒ±kabilir
            # (Eski kod: if norm_score > scores[-1]: norm_score = scores[-1])

            scores.append(norm_score)

        return scores

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 3: COMPUTE CURVES
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    import time
    from sklearn.metrics import f1_score

    methods_to_plot = ['PFI', 'SOFI', 'SHAP', 'LTCN']
    available_methods = [m for m in methods_to_plot if m in xai_data]

    # ‚úÖ FIX: T√ºm feature'larƒ± kullan
    n_total_features = X_ablation.shape[1]
    print(f"  Computing curves (ALL {n_total_features} features will be ablated)...")

    curves_normalized = {}

    # Baseline F1
    y_pred_base = model.predict(X_ablation)
    base_f1 = f1_score(y_ablation, y_pred_base, average='weighted')
    print(f"  Baseline F1: {base_f1:.4f}")

    for method in available_methods:
        start_t = time.time()
        importance = xai_data[method]
        feature_order = np.argsort(importance)[::-1]

        # ‚úÖ FIX: max_feats = n_total_features (t√ºm√º)
        curve = compute_ablation_curve_normalized(
            model, X_ablation, y_ablation, feature_order, base_f1, n_total_features
        )
        curves_normalized[method] = curve
        print(f"    ‚úì {method}: {time.time()-start_t:.2f}s (points: {len(curve)})")

    # Random Baseline
    import random
    random.seed(42)
    random_order = list(range(n_total_features))
    random.shuffle(random_order)
    curves_normalized['Random'] = compute_ablation_curve_normalized(
        model, X_ablation, y_ablation, random_order, base_f1, n_total_features
    )
    print(f"    ‚úì Random baseline computed")

    # ‚úÖ DEBUG: Final values (should be same for all)
    print(f"\n  üìä Final g(œÄ) values (should converge):")
    for method, scores in curves_normalized.items():
        print(f"      {method:10s}: {scores[-1]:.4f}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 4: PLOTTING (ACADEMIC STYLE - FIXED)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print(f"\n  Generating Academic Style Plot...")

    plt.rcParams["font.family"] = "serif"
    plt.rcParams["mathtext.fontset"] = "stix"

    fig, ax = plt.subplots(figsize=(10, 8))

    # ‚úÖ FIX: x_max = t√ºm feature sayƒ±sƒ±
    x_max = n_total_features
    x_all = np.arange(x_max + 1)

    # Envelope calculation
    envelope = []
    for i in x_all:
        vals_at_i = []
        for scores in curves_normalized.values():
            if i < len(scores):
                vals_at_i.append(scores[i])
        envelope.append(min(vals_at_i) if vals_at_i else 0.0)

    # Shading
    ax.fill_between(x_all, 0, envelope, facecolor="lightgray", alpha=0.5, label="_nolegend_")

    # Colors and markers
    colors = {
        'LTCN': '#1A9EC6', 'SOFI': '#FF0000', 'SHAP': '#FFD700',
        'LIME': '#15BD0C', 'PFI': '#8A2BE2', 'Random': 'black'
    }
    markers = {'LTCN': 'o', 'SOFI': 'o', 'SHAP': 'o', 'LIME': 's', 'PFI': 'o', 'Random': None}
    linestyles = {'Random': '--'}

    # ‚úÖ FIX: Plot ALL points (no early stopping)
    for label, scores in curves_normalized.items():
        # ‚úÖ REMOVED: Early stopping logic
        # ESKƒ∞ (YANLI≈û):
        # min_score = min(scores)
        # idx_stop = scores.index(min_score)
        # x_vals = list(range(idx_stop + 1))
        # y_vals = scores[:idx_stop + 1]

        # YENƒ∞ (DOƒûRU): T√ºm noktalarƒ± √ßiz
        x_vals = list(range(len(scores)))
        y_vals = scores

        style = linestyles.get(label, '-')
        color = colors.get(label, 'gray')
        marker = markers.get(label, 'o')

        # ‚úÖ FIX: markevery ayarlandƒ± (√ßok fazla marker olmasƒ±n)
        marker_interval = max(1, len(x_vals) // 15)

        ax.plot(x_vals, y_vals, color=color, linestyle=style, marker=marker,
                markersize=7, linewidth=2.5, label=label, markevery=marker_interval)

    # Styling
    set_title_if_enabled("Feature Importance Comparison", fontsize=18, fontweight='bold')
    ax.set_xlabel(r"$\pi_i$", fontsize=16, fontweight='bold')
    ax.set_ylabel(r"$g(\pi)$", fontsize=16, fontweight='bold')
    ax.set_xlim(0, x_max)
    ax.set_ylim(-0.02, 1.02)
    ax.grid(True, linestyle='--', alpha=0.5)
    ax.legend(fontsize=12, loc='upper right', framealpha=0.95, edgecolor='black')

    # ‚úÖ ADD: Convergence point annotation
    final_vals = [scores[-1] for scores in curves_normalized.values()]
    convergence_point = np.mean(final_vals)
    ax.axhline(y=convergence_point, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)
    ax.annotate(f'Convergence ‚âà {convergence_point:.3f}',
                xy=(x_max * 0.7, convergence_point + 0.03),
                fontsize=10, color='gray', style='italic')

    plt.tight_layout()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 5: SAVE (PDF + PNG)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    fig.set_facecolor('white')
    ax.set_facecolor('white')

    output_path_pdf = os.path.join(output_dir, '28_xai_ablation_comparison_academic.pdf')
    plt.savefig(output_path_pdf, format='pdf', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')

    output_path_png = output_path_pdf.replace('.pdf', '.png')
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight', facecolor='white')

    plt.close()

    print(f"\n  ‚úÖ G28 Saved: {os.path.basename(output_path_pdf)}")

    # AUC Report
    print("\n  === Normalized AUC Values (Lower is Better) ===")
    for label, scores in curves_normalized.items():
        auc_val = np.trapz(scores, dx=1)
        print(f"  {label:10s}: {auc_val:.4f}")

    return output_path_pdf

# ============================================================================
# BURAYA KADAR ‚¨ÜÔ∏è
# ============================================================================

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìê DUAL FORMAT SAVE FUNCTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Satƒ±r ~2480'e EKLE (THESIS_COLORS'dan sonra, create_dynamic_graphics'ten √∂nce)

def save_figure_dual_format(fig, base_path, dpi=300):
    """Save figure in both PNG and PDF formats"""
    save_params = {
        'bbox_inches': 'tight',
        'facecolor': 'white',
        'edgecolor': 'none',
        'pad_inches': 0.1
    }

    png_path = f"{base_path}.png"
    fig.savefig(png_path, dpi=dpi, format='png', **save_params)

    pdf_path = f"{base_path}.pdf"
    pdf_metadata = {
        'Title': os.path.basename(base_path),
        'Author': 'Dokumus - Tilburg University',
        'Subject': 'Football Match Prediction System v18.0',
        'Keywords': 'Machine Learning, XAI, Football Prediction',
        'Creator': 'Matplotlib',
        'Producer': 'Thesis Research'
    }

    fig.savefig(pdf_path, format='pdf', dpi=300, metadata=pdf_metadata, **save_params)

    return png_path, pdf_path

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé® THESIS TITLE HELPER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_thesis_compliant_title(graphic_id, short_title, rq=None):
    """Generate thesis-compliant title format"""
    title = f"Figure {graphic_id}: {short_title}"
    if rq:
        title = f"{title}\n({rq}: Predictive Performance Evaluation)"
    return title

# CREATE GRAPHICS FUNCTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìä CREATE DYNAMIC GRAPHICS FUNCTION (COMPLETE REWRITE)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìä CREATE DYNAMIC GRAPHICS FUNCTION (COMPLETE REWRITE)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_dynamic_graphics(results, xai_results, output_dir, features, models_dict,
                           y_test=None, X_test=None, X_train=None, y_train=None,
                           X_val=None, y_val=None):
    """
    Create comprehensive graphics with thesis-compliant styling

    All graphics use THESIS_COLORS (blue tones) except G28 (XAI comparison)
    All graphics saved in dual format: PNG (preview) + PDF (thesis)
    """

    print("\n7Ô∏è‚É£ CREATING GRAPHICS (Thesis-Compliant)...\n")
    print("  üìê Format: Dual (PNG + PDF)")
    print("  üé® Color Palette: Blue tones (G28 exception)")
    print("  üìä Resolution: 300 DPI\n")

    graphics = []  # Will store PDF paths only

    if not results:
        print("  No results")
        return graphics
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ GLOBAL GRAPHICS SETTINGS (CONFIG'DEN AL)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    graphics_config = CONFIG.get("graphics", {})
    SHOW_TITLES = graphics_config.get("show_titles", True)
    CLEAN_MODEL_NAMES = graphics_config.get("clean_model_names", True)

    print(f"  ‚öôÔ∏è  Settings: show_titles={SHOW_TITLES}, clean_model_names={CLEAN_MODEL_NAMES}\n")

    def clean_model_name(name):
        """Remove suffixes like (Pre-trained), (Optuna) from model names"""
        if CLEAN_MODEL_NAMES:
            clean = name.replace(" (Pre-trained)", "").replace(" (Optuna)", "")
            return clean.strip()
        return name

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ TITLE WRAPPER FUNCTION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    def set_title_if_enabled(ax, title_text, **kwargs):
        """Set title only if SHOW_TITLES is True"""
        if SHOW_TITLES:
            ax.set_title(title_text, **kwargs)
        # SHOW_TITLES False ise hi√ßbir ≈üey yapma (ba≈ülƒ±k yok)

    # Model isimlerini temizle
    results_clean = {clean_model_name(k): v for k, v in results.items()}
    results = results_clean

    models = list(results.keys())
    x_pos = np.arange(len(models))
    width = 0.35

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G1: MODEL ACCURACY COMPARISON
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G1] Creating Model Accuracy Comparison...")
        fig, ax = plt.subplots(figsize=(12, 6))

        # ‚úÖ THESIS COLORS: Blue tones
        train_bars = ax.bar(
            x_pos - width/2,
            [results[m].get('train_acc', np.nan) for m in models],
            width,
            label='Train',
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        test_bars = ax.bar(
            x_pos + width/2,
            [results[m].get('test_acc', np.nan) for m in models],
            width,
            label='Test',
            color=THESIS_COLORS['primary']['light_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)
        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G1', 'Model Accuracy Comparison'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim([0, 1.1])
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '01_model_accuracy')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G1 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G1: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G2: F1-SCORE COMPARISON
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G2] Creating F1-Score Comparison...")
        fig, ax = plt.subplots(figsize=(12, 6))

        train_bars = ax.bar(
            x_pos - width/2,
            [results[m].get('train_f1', np.nan) for m in models],
            width,
            label='Train',
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        test_bars = ax.bar(
            x_pos + width/2,
            [results[m].get('test_f1', np.nan) for m in models],
            width,
            label='Test',
            color=THESIS_COLORS['primary']['light_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)
        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('F1-Score', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G2', 'Model F1-Score Comparison'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim([0, 1.1])
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '02_model_f1_score')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G2 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G2: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G3: AUC-ROC COMPARISON
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G3] Creating AUC-ROC Comparison...")
        fig, ax = plt.subplots(figsize=(12, 6))

        train_bars = ax.bar(
            x_pos - width/2,
            [results[m].get('train_auc', np.nan) for m in models],
            width,
            label='Train',
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        test_bars = ax.bar(
            x_pos + width/2,
            [results[m].get('test_auc', np.nan) for m in models],
            width,
            label='Test',
            color=THESIS_COLORS['primary']['light_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)
        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('AUC-ROC', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G3', 'Model AUC-ROC Comparison'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim([0, 1.1])
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '03_model_auc')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G3 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G3: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G5: COHEN'S KAPPA SCORE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G5] Creating Cohen's Kappa Score...")
        fig, ax = plt.subplots(figsize=(12, 6))

        train_bars = ax.bar(
            x_pos - width/2,
            [results[m].get('train_kappa', np.nan) for m in models],
            width,
            label='Train',
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        test_bars = ax.bar(
            x_pos + width/2,
            [results[m].get('test_kappa', np.nan) for m in models],
            width,
            label='Test',
            color=THESIS_COLORS['primary']['light_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, train_bars, format_str='.2f', fontsize=12)
        add_numeric_values_to_bars(ax, test_bars, format_str='.2f', fontsize=12)

        ax.set_ylabel("Cohen's Kappa", fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G5', "Cohen's Kappa Score"),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '05_kappa_score')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G5 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G5: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G7: RANKED PROBABILITY SCORE (RPS)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G7] Creating Ranked Probability Score...")
        fig, ax = plt.subplots(figsize=(12, 6))

        rps_scores = [results[m].get('test_rps', np.nan) for m in models]
        bars = ax.bar(
            x_pos,
            rps_scores,
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('RPS', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G7', 'Ranked Probability Score'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '07_rps_score')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G7 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G7: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G8: EXPECTED CALIBRATION ERROR (ECE)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G8] Creating Expected Calibration Error...")
        fig, ax = plt.subplots(figsize=(12, 6))

        ece_scores = [results[m].get('test_ece', np.nan) for m in models]
        bars = ax.bar(
            x_pos,
            ece_scores,
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8,
            edgecolor='black'
        )

        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('ECE', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G8', 'Expected Calibration Error'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '08_ece_score')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G8 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G8: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G9: PROBABILISTIC METRICS COMPARISON (Brier, RPS, ECE)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G9] Creating Probabilistic Metrics Comparison...")
        fig, ax = plt.subplots(figsize=(14, 7))

        x_pos_prob = np.arange(len(models))
        width_prob = 0.25

        brier_scores = [results[m].get('test_brier', np.nan) for m in models]
        rps_scores = [results[m].get('test_rps', np.nan) for m in models]
        ece_scores = [results[m].get('test_ece', np.nan) for m in models]

        # ‚úÖ THESIS COLORS: Blue gradient
        bars1 = ax.bar(
            x_pos_prob - width_prob, brier_scores, width_prob,
            label='Brier Score',
            color=THESIS_COLORS['primary']['dark_blue'],
            alpha=0.8, edgecolor='black'
        )
        bars2 = ax.bar(
            x_pos_prob, rps_scores, width_prob,
            label='RPS',
            color=THESIS_COLORS['primary']['medium_blue'],
            alpha=0.8, edgecolor='black'
        )
        bars3 = ax.bar(
            x_pos_prob + width_prob, ece_scores, width_prob,
            label='ECE',
            color=THESIS_COLORS['primary']['light_blue'],
            alpha=0.8, edgecolor='black'
        )

        for bars in [bars1, bars2, bars3]:
            add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('Score (Lower is Better)', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G9', 'Probabilistic Metrics Comparison'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos_prob)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend(fontsize=12, loc='upper right')
        ax.grid(axis='y', linestyle='--', alpha=0.4)

        plt.tight_layout()
        base_path = os.path.join(output_dir, '09_probabilistic_metrics_comparison')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G9 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G9: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G11: TRAIN-TEST GENERALIZATION GAP
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G11] Creating Generalization Gap...")
        fig, ax = plt.subplots(figsize=(12, 6))

        gaps = [abs(results[m].get('train_f1', np.nan) - results[m].get('test_f1', np.nan)) for m in models]

        # ‚úÖ THESIS COLORS: Color-coded bars (blue-based)
        colors = [
            THESIS_COLORS['primary']['sky_blue'] if g < 0.1
            else THESIS_COLORS['accent']['highlight'] if g < 0.2
            else THESIS_COLORS['accent']['warning']
            for g in np.nan_to_num(gaps)
        ]

        bars = ax.bar(x_pos, np.nan_to_num(gaps), color=colors, alpha=0.8, edgecolor='black')
        add_numeric_values_to_bars(ax, bars, format_str='.2f', fontsize=12)

        ax.set_ylabel('Absolute F1 Gap', fontsize=11, fontweight='bold')
        ax.set_xlabel('Model', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G11', 'Train-Test Generalization Gap'),
            fontsize=12, fontweight='bold'
        )
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right')

        # Legend
        good = mpatches.Patch(color=THESIS_COLORS['primary']['sky_blue'], label='Good (<0.1)')
        fair = mpatches.Patch(color=THESIS_COLORS['accent']['highlight'], label='Fair (0.1-0.2)')
        poor = mpatches.Patch(color=THESIS_COLORS['accent']['warning'], label='Poor (>0.2)')
        ax.legend(handles=[good, fair, poor])
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '11_generalization_gap')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G11 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G11: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G12: MODEL RANKING BY TEST F1-SCORE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G12] Creating Model Ranking...")
        fig, ax = plt.subplots(figsize=(12, 8))

        f1_scores = [(m, results[m].get('test_f1', 0)) for m in models]
        f1_scores_sorted = sorted(f1_scores, key=lambda x: x[1], reverse=True)

        model_names_sorted = [m[0] for m in f1_scores_sorted]
        scores_sorted = [m[1] for m in f1_scores_sorted]

        # ‚úÖ THESIS COLORS: Blue gradient (dark to light)
        n_models = len(model_names_sorted)
        colors_grad = [
            THESIS_COLORS['primary']['dark_blue'] if i == 0
            else THESIS_COLORS['primary']['medium_blue'] if i < n_models//3
            else THESIS_COLORS['primary']['light_blue'] if i < 2*n_models//3
            else THESIS_COLORS['primary']['sky_blue']
            for i in range(n_models)
        ]

        y_pos = np.arange(len(model_names_sorted))
        bars = ax.barh(y_pos, scores_sorted, color=colors_grad, alpha=0.8, edgecolor='black')

        ax.set_yticks(y_pos)
        ax.set_yticklabels(model_names_sorted, fontsize=12, fontweight='bold')
        ax.set_xlabel('Test F1-Score', fontsize=12, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G12', 'Model Ranking by Test F1-Score'),
            fontsize=13, fontweight='bold'
        )
        ax.grid(axis='x', linestyle='--', alpha=0.4)
        ax.set_xlim([0, 1.0])

        # Add ranking numbers and scores
        for i, (bar, score) in enumerate(zip(bars, scores_sorted)):
            width = bar.get_width()
            ax.text(0.01, bar.get_y() + bar.get_height()/2.,
                   f'#{i+1}', ha='left', va='center', fontsize=12,
                   fontweight='bold', color='white')
            ax.text(width - 0.01, bar.get_y() + bar.get_height()/2.,
                   f'{score:.4f}', ha='right', va='center', fontsize=12,
                   fontweight='bold', color='black')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '12_model_ranking_f1')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G12 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G12: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G13: TEST ACCURACY VS LOSS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G13] Creating Accuracy vs Loss...")
        fig, ax = plt.subplots(figsize=(10, 6))

        # ‚úÖ THESIS COLORS: Single blue tone with varying sizes
        for i, model in enumerate(models):
            loss = results[model].get('test_loss', np.nan)
            acc = results[model].get('test_acc', np.nan)
            if pd.notna(loss) and pd.notna(acc):
                ax.scatter(
                    loss, acc,
                    s=150,
                    alpha=0.7,
                    label=model,
                    color=THESIS_COLORS['primary']['medium_blue'],
                    edgecolors='black',
                    linewidths=1.5
                )
                ax.text(loss, acc, model, fontsize=12, ha='center', va='bottom')

        ax.set_xlabel('Test Log Loss', fontsize=11, fontweight='bold')
        ax.set_ylabel('Test Accuracy', fontsize=11, fontweight='bold')
        set_title_if_enabled(ax,
            get_thesis_compliant_title('G13', 'Test Accuracy vs Loss'),
            fontsize=12, fontweight='bold'
        )
        ax.grid(alpha=0.4, linestyle='--')

        plt.tight_layout()
        base_path = os.path.join(output_dir, '13_accuracy_vs_loss')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close(fig)

        graphics.append(pdf_path)
        print(f"    ‚úì G13 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G13: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G14: PERFORMANCE METRICS HEATMAP
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if 14 not in CONFIG.get("graphics_to_skip", []):
        try:
            print("  [G14] Creating Performance Metrics Heatmap...")
            fig, ax = plt.subplots(figsize=(14, 8))

            metrics_to_plot = ['test_acc', 'test_f1', 'test_auc', 'test_kappa',
                              'test_brier', 'test_rps', 'test_ece']
            metrics_labels = ['Accuracy', 'F1-Score', 'AUC', 'Kappa',
                             'Brier', 'RPS', 'ECE']

            data_matrix = []
            for model in models:
                row = []
                for metric in metrics_to_plot:
                    value = results[model].get(metric, np.nan)
                    if metric in ['test_brier', 'test_rps', 'test_ece']:
                        value = 1 - value if pd.notna(value) else np.nan
                    row.append(value)
                data_matrix.append(row)

            data_matrix = np.array(data_matrix)

            # ‚úÖ THESIS COLORS: Blue-based colormap
            from matplotlib.colors import LinearSegmentedColormap
            colors_heatmap = [
                THESIS_COLORS['accent']['warning'],      # Low (red)
                THESIS_COLORS['accent']['highlight'],    # Medium (orange)
                THESIS_COLORS['primary']['light_blue'],  # Good (light blue)
                THESIS_COLORS['primary']['medium_blue']  # Excellent (blue)
            ]
            n_bins = 100
            cmap = LinearSegmentedColormap.from_list('thesis', colors_heatmap, N=n_bins)

            im = ax.imshow(data_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

            ax.set_xticks(np.arange(len(metrics_labels)))
            ax.set_yticks(np.arange(len(models)))
            ax.set_xticklabels(metrics_labels, fontsize=12, fontweight='bold')
            ax.set_yticklabels(models, fontsize=12)

            plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

            for i in range(len(models)):
                for j in range(len(metrics_labels)):
                    if pd.notna(data_matrix[i, j]):
                        text = ax.text(j, i, f'{data_matrix[i, j]:.3f}',
                                     ha="center", va="center", color="black", fontsize=12)

            set_title_if_enabled(ax,
                get_thesis_compliant_title('G14', 'Performance Metrics Heatmap (Normalized)'),
                fontsize=13, fontweight='bold', pad=20
            )

            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('Normalized Score', rotation=270, labelpad=20, fontsize=12, fontweight='bold')

            plt.tight_layout()
            base_path = os.path.join(output_dir, '14_metrics_heatmap')
            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
            plt.close(fig)

            graphics.append(pdf_path)
            print(f"    ‚úì G14 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

        except Exception as e:
            print(f"    ‚úó G14: {e}")
    else:
        print(f"    ‚è∏Ô∏è  G14 - Skipped (CONFIG)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G15: MODEL PREDICTIONS CORRELATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if 15 not in CONFIG.get("graphics_to_skip", []):
        try:
            if y_test is not None and X_test is not None:
                print("  [G15] Creating Predictions Correlation...")
                fig, ax = plt.subplots(figsize=(12, 10))

                predictions_dict = {}
                for model_name in models:
                    short_name = model_name.split(' ')[0].lower()
                    if short_name in models_dict:
                        try:
                            y_pred = models_dict[short_name].predict(X_test)
                            predictions_dict[model_name] = y_pred.flatten()
                        except:
                            pass

                if len(predictions_dict) > 1:
                    pred_df = pd.DataFrame(predictions_dict)
                    corr_matrix = pred_df.corr()

                    # ‚úÖ THESIS COLORS: Blue-based colormap
                    from matplotlib.colors import LinearSegmentedColormap
                    colors_corr = [
                        THESIS_COLORS['primary']['sky_blue'],    # Low correlation
                        THESIS_COLORS['primary']['light_blue'],
                        THESIS_COLORS['primary']['medium_blue'],
                        THESIS_COLORS['primary']['dark_blue']    # High correlation
                    ]
                    cmap = LinearSegmentedColormap.from_list('thesis_corr', colors_corr, N=100)

                    im = ax.imshow(corr_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

                    ax.set_xticks(np.arange(len(corr_matrix)))
                    ax.set_yticks(np.arange(len(corr_matrix)))
                    ax.set_xticklabels(corr_matrix.columns, fontsize=12, rotation=45, ha='right')
                    ax.set_yticklabels(corr_matrix.index, fontsize=12)

                    for i in range(len(corr_matrix)):
                        for j in range(len(corr_matrix)):
                            text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                                         ha="center", va="center",
                                         color="white" if corr_matrix.iloc[i, j] > 0.5 else "black",
                                         fontsize=12, fontweight='bold')

                    set_title_if_enabled(ax,
                        get_thesis_compliant_title(
                            'G15',
                            'Model Predictions Correlation\n(High correlation = Similar predictions)'
                        ),
                        fontsize=12, fontweight='bold', pad=20
                    )

                    cbar = plt.colorbar(im, ax=ax)
                    cbar.set_label('Correlation', rotation=270, labelpad=20, fontsize=12, fontweight='bold')

                    plt.tight_layout()
                    base_path = os.path.join(output_dir, '15_predictions_correlation')
                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                    plt.close(fig)

                    graphics.append(pdf_path)
                    print(f"    ‚úì G15 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

        except Exception as e:
            print(f"    ‚úó G15: {e}")
    else:
        print(f"    ‚è∏Ô∏è  G15 - Skipped (CONFIG)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G16: FEATURE IMPORTANCE AGREEMENT
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        if xai_results:
            print("  [G16] Creating Feature Importance Agreement...")
            fig, ax = plt.subplots(figsize=(12, 10))

            model_top_features = {}
            for model_short in xai_results:
                if 'PFI' in xai_results[model_short]:
                    importance = xai_results[model_short]['PFI']
                    top_20_indices = np.argsort(importance)[-20:]
                    model_top_features[model_short] = set(top_20_indices)

            if len(model_top_features) > 1:
                model_names_list = list(model_top_features.keys())
                n_models_fi = len(model_names_list)
                similarity_matrix = np.zeros((n_models_fi, n_models_fi))

                for i in range(n_models_fi):
                    for j in range(n_models_fi):
                        set_i = model_top_features[model_names_list[i]]
                        set_j = model_top_features[model_names_list[j]]
                        jaccard = len(set_i & set_j) / len(set_i | set_j) if len(set_i | set_j) > 0 else 0
                        similarity_matrix[i, j] = jaccard

                # ‚úÖ THESIS COLORS: Blue-based colormap
                from matplotlib.colors import LinearSegmentedColormap
                colors_agreement = [
                    THESIS_COLORS['primary']['sky_blue'],
                    THESIS_COLORS['primary']['light_blue'],
                    THESIS_COLORS['primary']['medium_blue'],
                    THESIS_COLORS['primary']['dark_blue']
                ]
                cmap = LinearSegmentedColormap.from_list('thesis_agreement', colors_agreement, N=100)

                im = ax.imshow(similarity_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

                ax.set_xticks(np.arange(n_models_fi))
                ax.set_yticks(np.arange(n_models_fi))
                ax.set_xticklabels(model_names_list, fontsize=12, rotation=45, ha='right')
                ax.set_yticklabels(model_names_list, fontsize=12)

                for i in range(n_models_fi):
                    for j in range(n_models_fi):
                        text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',
                                     ha="center", va="center", color="black", fontsize=12)

                set_title_if_enabled(ax,
                    get_thesis_compliant_title(
                        'G16',
                        'Feature Importance Agreement (Top 20)\n(High agreement = Similar feature focus)'
                    ),
                    fontsize=12, fontweight='bold', pad=20
                )

                cbar = plt.colorbar(im, ax=ax)
                cbar.set_label('Jaccard Similarity', rotation=270, labelpad=20, fontsize=12, fontweight='bold')

                plt.tight_layout()
                base_path = os.path.join(output_dir, '16_feature_importance_agreement')
                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                plt.close(fig)

                graphics.append(pdf_path)
                print(f"    ‚úì G16 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G16: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G17: DISTRIBUTION OF KEY TEST METRICS (BOX PLOTS)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if 17 not in CONFIG.get("graphics_to_skip", []):
        try:
            print("  [G17] Creating Metrics Distribution...")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            axes = axes.flatten()

            metrics_to_plot_box = [
                ('test_acc', 'Accuracy'),
                ('test_f1', 'F1-Score'),
                ('test_auc', 'AUC-ROC'),
                ('test_kappa', "Cohen's Kappa"),
                ('test_brier', 'Brier Score'),
                ('test_rps', 'RPS')
            ]

            # ‚úÖ THESIS COLORS: Each box plot uses different blue shade
            box_colors = [
                THESIS_COLORS['primary']['dark_blue'],
                THESIS_COLORS['primary']['medium_blue'],
                THESIS_COLORS['primary']['light_blue'],
                THESIS_COLORS['primary']['sky_blue'],
                THESIS_COLORS['primary']['medium_blue'],
                THESIS_COLORS['primary']['light_blue']
            ]

            for idx, ((metric_key, metric_label), color) in enumerate(zip(metrics_to_plot_box, box_colors)):
                ax = axes[idx]

                metric_values = [results[m].get(metric_key, np.nan) for m in models]
                metric_values = [v for v in metric_values if pd.notna(v)]

                if metric_values:
                    bp = ax.boxplot([metric_values], vert=True, patch_artist=True,
                                   widths=0.5, showmeans=True, meanline=True)

                    for patch in bp['boxes']:
                        patch.set_facecolor(color)
                        patch.set_alpha(0.7)

                    y_points = metric_values
                    x_points = np.random.normal(1, 0.04, size=len(y_points))
                    ax.scatter(x_points, y_points, alpha=0.6, s=80,
                              color=THESIS_COLORS['neutral']['dark_gray'],
                              edgecolors='white', zorder=3)

                    ax.set_ylabel(metric_label, fontsize=11, fontweight='bold')
                    set_title_if_enabled(ax,f'{metric_label} Distribution', fontsize=11, fontweight='bold')
                    ax.set_xticks([1])
                    ax.set_xticklabels(['All Models'])
                    ax.grid(axis='y', linestyle='--', alpha=0.4)

                    mean_val = np.mean(metric_values)
                    median_val = np.median(metric_values)
                    std_val = np.std(metric_values)
                    stats_text = f'Mean: {mean_val:.4f}\nMedian: {median_val:.4f}\nStd: {std_val:.4f}'
                    ax.text(0.98, 0.98, stats_text, transform=ax.transAxes,
                           fontsize=12, verticalalignment='top', horizontalalignment='right',
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                else:
                    ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)
                    ax.axis('off')

            plt.suptitle(
                get_thesis_compliant_title('G17', 'Distribution of Key Test Metrics'),
                fontsize=14, fontweight='bold'
            )
            plt.tight_layout()
            base_path = os.path.join(output_dir, '17_metrics_distribution_boxplots')
            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
            plt.close(fig)

            graphics.append(pdf_path)
            print(f"    ‚úì G17 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

        except Exception as e:
            print(f"    ‚úó G17: {e}")
    else:
        print(f"    ‚è∏Ô∏è  G17 - Skipped (CONFIG)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G18a: BEST MODEL FEATURE IMPORTANCE (TOP 20) - 6 METHODS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if "18a" not in CONFIG.get("graphics_to_skip", []):
        try:
            print("  [G18a] Creating Best Model Feature Importance (6 methods)...")
            valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}
            if valid_models_f1:
                best_model = max(valid_models_f1, key=valid_models_f1.get)
                best_model_short = best_model.split(' ')[0].lower()

                if best_model_short in xai_results:
                    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
                    axes = axes.flatten()

                    xai_methods = ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']

                    # ‚úÖ THESIS COLORS: Blue gradient for each method
                    colors_xai_thesis = [
                        THESIS_COLORS['primary']['dark_blue'],
                        THESIS_COLORS['primary']['medium_blue'],
                        THESIS_COLORS['primary']['light_blue'],
                        THESIS_COLORS['primary']['sky_blue'],
                        THESIS_COLORS['primary']['medium_blue'],
                        THESIS_COLORS['primary']['light_blue']
                    ]

                    for idx, (method, color) in enumerate(zip(xai_methods, colors_xai_thesis)):
                        ax = axes[idx]
                        if method in xai_results[best_model_short]:
                            importance = xai_results[best_model_short][method]

                            top_indices = np.argsort(importance)[-20:][::-1]
                            top_features = [features[i] for i in top_indices]
                            top_scores = importance[top_indices]

                            y_pos = np.arange(len(top_features))
                            bars = ax.barh(y_pos, top_scores, color=color, alpha=0.8, edgecolor='black')

                            ax.set_yticks(y_pos)
                            ax.set_yticklabels(top_features, fontsize=12)
                            ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')
                            set_title_if_enabled(ax,f'{method}', fontsize=11, fontweight='bold')
                            ax.grid(axis='x', linestyle='--', alpha=0.4)

                            for bar in bars:
                                width = bar.get_width()
                                ax.text(width, bar.get_y() + bar.get_height()/2.,
                                       f'{width:.4f}', ha='left', va='center', fontsize=12)
                        else:
                            ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                                   transform=ax.transAxes, fontsize=12)
                            ax.axis('off')

                    plt.suptitle(
                        get_thesis_compliant_title(
                            'G18a',
                            f'Best Model ({best_model}) - Feature Importance by Method'
                        ),
                        fontsize=14, fontweight='bold'
                    )
                    plt.tight_layout()
                    base_path = os.path.join(output_dir, '18a_best_model_feature_importance')
                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                    plt.close(fig)

                    graphics.append(pdf_path)
                    print(f"    ‚úì G18a saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

        except Exception as e:
            print(f"    ‚úó G18a: {e}")
    else:
        print(f"    ‚è∏Ô∏è  G18a - Skipped (CONFIG)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G18b: BEST MODEL - AGGREGATED TOP 10 FEATURES
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G18b] Creating Aggregated Top 10 Features...")
        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}
        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            best_model_short = best_model.split(' ')[0].lower()

            if best_model_short in xai_results:
                all_importance = []
                n_features_expected = len(features)

                for method in ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']:
                    if method in xai_results[best_model_short]:
                        importance = xai_results[best_model_short][method]

                        if importance is None:
                            continue

                        importance = np.array(importance).flatten()

                        if len(importance) != n_features_expected:
                            continue

                        if np.any(np.isnan(importance)) or np.any(np.isinf(importance)):
                            importance = np.nan_to_num(importance, nan=0.0, posinf=0.0, neginf=0.0)

                        if importance.sum() == 0:
                            importance = np.ones(n_features_expected) / n_features_expected

                        all_importance.append(importance)

                if len(all_importance) >= 2:
                    importance_stack = np.vstack(all_importance)
                    mean_importance = np.mean(importance_stack, axis=0)

                    top_10_indices = np.argsort(mean_importance)[-10:][::-1]
                    top_10_features = [features[i] for i in top_10_indices]
                    top_10_scores = mean_importance[top_10_indices]

                    fig, ax = plt.subplots(figsize=(12, 8))
                    y_pos = np.arange(len(top_10_features))

                    # ‚úÖ THESIS COLORS: Single blue tone
                    bars = ax.barh(
                        y_pos, top_10_scores,
                        color=THESIS_COLORS['primary']['medium_blue'],
                        alpha=0.8, edgecolor='black'
                    )

                    ax.set_yticks(y_pos)
                    ax.set_yticklabels(top_10_features, fontsize=11, fontweight='bold')
                    ax.set_xlabel('Aggregated Importance Score', fontsize=12, fontweight='bold')
                    set_title_if_enabled(ax,
                        get_thesis_compliant_title(
                            'G18b',
                            f'Best Model ({best_model}) - Top 10 Features (Averaged)'
                        ),
                        fontsize=13, fontweight='bold'
                    )
                    ax.grid(axis='x', linestyle='--', alpha=0.4)

                    for bar in bars:
                        width = bar.get_width()
                        ax.text(width, bar.get_y() + bar.get_height()/2.,
                               f'{width:.4f}', ha='left', va='center',
                               fontsize=12, fontweight='bold')

                    plt.tight_layout()
                    base_path = os.path.join(output_dir, '18b_best_model_top10_aggregated')
                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                    plt.close(fig)

                    graphics.append(pdf_path)
                    print(f"    ‚úì G18b saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G18b: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G18c: BEST MODEL - SHAP SUMMARY PLOT
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        if HAS_SHAP:
            print("  [G18c] Creating SHAP Summary Plot...")
            valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}
            if valid_models_f1:
                best_model = max(valid_models_f1, key=valid_models_f1.get)
                best_model_short = best_model.split(' ')[0].lower()

                if best_model_short in models_dict and best_model_short not in ['svm', 'lr', 'ada', 'ltcn']:
                    model = models_dict[best_model_short]

                    X_shap = X_test.sample(min(100, X_test.shape[0]), random_state=SEED)

                    try:
                        def predict_func(x_data):
                            if hasattr(model, 'predict_proba'):
                                return model.predict_proba(x_data)
                            else:
                                return model.predict(x_data)

                        background = X_test.sample(min(50, X_test.shape[0]), random_state=SEED)
                        explainer = shap.KernelExplainer(predict_func, background)
                        shap_values = explainer.shap_values(X_shap)

                    except:
                        try:
                            explainer = shap.TreeExplainer(
                                model,
                                feature_perturbation='tree_path_dependent'
                            )
                            shap_values = explainer.shap_values(X_shap)
                        except:
                            raise Exception("Both SHAP explainers failed")

                    if isinstance(shap_values, list):
                        shap_values_plot = shap_values[1]
                        class_label = "Draw Class"
                    else:
                        shap_values_plot = shap_values
                        class_label = ""

                    fig, ax = plt.subplots(figsize=(12, 10))

                    # ‚úÖ SHAP uses its own colormap, but we can't override it easily
                    # Just ensure high-quality output
                    shap.summary_plot(
                        shap_values_plot,
                        X_shap,
                        feature_names=features,
                        show=False,
                        max_display=20,
                        plot_type='dot'
                    )

                    title_text = get_thesis_compliant_title(
                        'G18c',
                        f'Best Model ({best_model}) - SHAP Summary Plot'
                    )
                    if class_label:
                        title_text += f'\n({class_label})'

                    plt.title(title_text, fontsize=13, fontweight='bold', pad=20)
                    plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=11, fontweight='bold')
                    plt.ylabel('Features', fontsize=11, fontweight='bold')
                    plt.tight_layout()

                    base_path = os.path.join(output_dir, '18c_best_model_shap_summary')
                    png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                    plt.close()

                    graphics.append(pdf_path)
                    print(f"    ‚úì G18c saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G18c: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G20a: CLASSIFICATION METRICS RADAR (Threshold-Dependent)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G20a] Creating Classification Metrics Radar...")

        valid_models_f1 = {
            m: results[m].get('test_f1', -1)
            for m in models
            if pd.notna(results[m].get('test_f1'))
        }

        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            metrics = results[best_model]

            # Font setup
            try:
                import matplotlib
                matplotlib.rcParams['font.family'] = 'DejaVu Sans'
            except:
                pass

            categories = ['Accuracy', 'F1-Score', 'Precision\n(Macro)', 'Recall\n(Macro)', 'Kappa\n(Scaled)']

            per_class = metrics.get('per_class_metrics', {})
            if per_class and 'precision' in per_class and 'recall' in per_class:
                macro_precision = np.mean(list(per_class['precision'].values()))
                macro_recall = np.mean(list(per_class['recall'].values()))
            else:
                macro_precision = metrics.get('test_f1', 0.5) * 0.95
                macro_recall = metrics.get('test_f1', 0.5) * 1.05

            kappa_raw = metrics.get('test_kappa', 0)
            kappa_normalized = (kappa_raw + 1) / 2

            values = [
                metrics.get('test_acc', 0),
                metrics.get('test_f1', 0),
                macro_precision,
                macro_recall,
                kappa_normalized
            ]

            try:
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                values_plot = values + values[:1]
                angles_plot = angles + angles[:1]

                fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

                # ‚úÖ THESIS COLORS: Blue for main plot
                ax.plot(angles_plot, values_plot, 'o-', linewidth=3,
                       color=THESIS_COLORS['primary']['dark_blue'],
                       label=best_model, markersize=8, zorder=3)
                ax.fill(angles_plot, values_plot, alpha=0.25,
                       color=THESIS_COLORS['primary']['dark_blue'], zorder=2)

                ax.set_xticks(angles)
                ax.set_xticklabels(categories, fontsize=12, fontweight='bold')
                ax.set_ylim(0, 1)
                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)
                ax.grid(True, linestyle='--', linewidth=1.5, alpha=0.6, zorder=1)

                # Baseline
                baseline_values = [0.6] * len(angles_plot)
                ax.plot(angles_plot, baseline_values, '--', linewidth=2,
                       color=THESIS_COLORS['accent']['highlight'],
                       alpha=0.5, label='Baseline (0.6)', zorder=1)

                for angle, value, cat in zip(angles, values, categories):
                    text_radius = value + 0.08
                    ax.text(angle, text_radius, f'{value:.3f}',
                           ha='center', va='center', fontsize=12, fontweight='bold',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white',
                                    edgecolor='black', alpha=0.9), zorder=4)

                plt.title(
                    get_thesis_compliant_title(
                        'G20a',
                        f'Classification Performance ({best_model})\nThreshold-Dependent Metrics'
                    ),
                    fontsize=12, fontweight='bold', pad=25
                )
                plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1),
                          fontsize=12, framealpha=0.9)

                plt.tight_layout()
                base_path = os.path.join(output_dir, '20a_classification_radar')
                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                plt.close()

                graphics.append(pdf_path)
                print(f"    ‚úì G20a saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

            except Exception as e_radar:
                print(f"    ‚ö†Ô∏è  Radar plot failed, using bar chart fallback")

                fig, ax = plt.subplots(figsize=(12, 8))
                y_pos = np.arange(len(categories))

                bars = ax.barh(y_pos, values,
                              color=THESIS_COLORS['primary']['medium_blue'],
                              alpha=0.8, edgecolor='black')

                ax.set_yticks(y_pos)
                ax.set_yticklabels(categories, fontsize=11, fontweight='bold')
                ax.set_xlabel('Score', fontsize=12, fontweight='bold')
                set_title_if_enabled(ax,
                    get_thesis_compliant_title(
                        'G20a',
                        f'Classification Performance ({best_model})'
                    ),
                    fontsize=14, fontweight='bold', pad=20
                )
                ax.set_xlim(0, 1)
                ax.grid(axis='x', linestyle='--', alpha=0.4)
                ax.invert_yaxis()

                for bar, value in zip(bars, values):
                    width = bar.get_width()
                    ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,
                           f'{value:.3f}', ha='left', va='center',
                           fontsize=10, fontweight='bold')

                ax.axvline(x=0.6,
                          color=THESIS_COLORS['accent']['highlight'],
                          linestyle='--', linewidth=2,
                          alpha=0.5, label='Baseline (0.6)')
                ax.legend(fontsize=11)

                plt.tight_layout()
                base_path = os.path.join(output_dir, '20a_classification_radar')
                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                plt.close()

                graphics.append(pdf_path)
                print(f"    ‚úì G20a saved (BAR FALLBACK, PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e_g20a:
        print(f"    ‚ùå G20a: {e_g20a}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G20b: PROBABILISTIC METRICS RADAR (Threshold-Independent)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G20b] Creating Probabilistic Metrics Radar...")

        valid_models_f1 = {
            m: results[m].get('test_f1', -1)
            for m in models
            if pd.notna(results[m].get('test_f1'))
        }

        if valid_models_f1:
            best_model = max(valid_models_f1, key=valid_models_f1.get)
            metrics = results[best_model]

            try:
                import matplotlib
                matplotlib.rcParams['font.family'] = 'DejaVu Sans'
            except:
                pass

            categories = ['AUC-ROC', '1-Brier\nScore', '1-RPS', '1-ECE', '1-Log Loss']

            log_loss_raw = metrics.get('test_loss', 1.0)
            log_loss_normalized = max(0, min(1, 1 - (log_loss_raw / 2)))

            values = [
                metrics.get('test_auc', 0),
                max(0, 1 - metrics.get('test_brier', 0)),
                max(0, 1 - metrics.get('test_rps', 0)),
                max(0, 1 - metrics.get('test_ece', 0)),
                log_loss_normalized
            ]

            try:
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                values_plot = values + values[:1]
                angles_plot = angles + angles[:1]

                fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

                # ‚úÖ THESIS COLORS: Blue for probabilistic metrics
                ax.plot(angles_plot, values_plot, 'o-', linewidth=3,
                       color=THESIS_COLORS['primary']['dark_blue'],
                       label=best_model, markersize=8, zorder=3)
                ax.fill(angles_plot, values_plot, alpha=0.25,
                       color=THESIS_COLORS['primary']['dark_blue'], zorder=2)

                ax.set_xticks(angles)
                ax.set_xticklabels(categories, fontsize=12, fontweight='bold')
                ax.set_ylim(0, 1)
                ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
                ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)
                ax.grid(True, linestyle='--', linewidth=1.5, alpha=0.6, zorder=1)

                baseline_values = [0.7] * len(angles_plot)
                ax.plot(angles_plot, baseline_values, '--', linewidth=2,
                       color=THESIS_COLORS['accent']['highlight'],
                       alpha=0.5, label='Good (0.7)', zorder=1)

                for angle, value, cat in zip(angles, values, categories):
                    text_radius = value + 0.08
                    ax.text(angle, text_radius, f'{value:.3f}',
                           ha='center', va='center', fontsize=12, fontweight='bold',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white',
                                    edgecolor='black', alpha=0.9), zorder=4)

                plt.title(
                    get_thesis_compliant_title(
                        'G20b',
                        f'Probabilistic Performance ({best_model})\nThreshold-Independent Metrics'
                    ),
                    fontsize=14, fontweight='bold', pad=25
                )
                plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1),
                          fontsize=11, framealpha=0.9)

                plt.tight_layout()
                base_path = os.path.join(output_dir, '20b_probabilistic_radar')
                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                plt.close()

                graphics.append(pdf_path)
                print(f"    ‚úì G20b saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

            except Exception as e_radar:
                print(f"    ‚ö†Ô∏è  Radar plot failed, using bar chart fallback")

                fig, ax = plt.subplots(figsize=(12, 8))
                y_pos = np.arange(len(categories))

                bars = ax.barh(y_pos, values,
                              color=THESIS_COLORS['primary']['medium_blue'],
                              alpha=0.8, edgecolor='black')

                ax.set_yticks(y_pos)
                ax.set_yticklabels(categories, fontsize=11, fontweight='bold')
                ax.set_xlabel('Score', fontsize=12, fontweight='bold')
                set_title_if_enabled(ax,
                    get_thesis_compliant_title(
                        'G20b',
                        f'Probabilistic Performance ({best_model})'
                    ),
                    fontsize=14, fontweight='bold', pad=20
                )
                ax.set_xlim(0, 1)
                ax.grid(axis='x', linestyle='--', alpha=0.4)
                ax.invert_yaxis()

                for bar, value in zip(bars, values):
                    width = bar.get_width()
                    ax.text(width + 0.02, bar.get_y() + bar.get_height()/2.,
                           f'{value:.3f}', ha='left', va='center',
                           fontsize=12, fontweight='bold')

                ax.axvline(x=0.7,
                          color=THESIS_COLORS['accent']['highlight'],
                          linestyle='--', linewidth=2,
                          alpha=0.5, label='Good (0.7)')
                ax.legend(fontsize=11)

                plt.tight_layout()
                base_path = os.path.join(output_dir, '20b_probabilistic_radar')
                png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
                plt.close()

                graphics.append(pdf_path)
                print(f"    ‚úì G20b saved (BAR FALLBACK, PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e_g20b:
        print(f"    ‚ùå G20b: {e_g20b}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G24: EXECUTIVE SUMMARY
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G24] Creating Executive Summary...")
        fig = plt.figure(figsize=(12, 8))
        gs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.3)

        ax1 = fig.add_subplot(gs[0, :])
        ax1.axis('off')

        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in models if pd.notna(results[m].get('test_f1'))}
        avg_test_f1 = np.nanmean([results[m].get('test_f1', np.nan) for m in models])
        best_f1_score = max(valid_models_f1.values()) if valid_models_f1 else np.nan
        best_model = max(valid_models_f1, key=valid_models_f1.get) if valid_models_f1 else 'N/A'

        summary_text = f"""
        --- FOOTBALL PREDICTION SYSTEM v18.0 ---
        Models Trained: {len(models)}
        Best Model: {best_model} (F1 = {best_f1_score:.4f})
        Average Test F1: {avg_test_f1:.4f}
        Output Directory: {os.path.basename(output_dir)}
        """
        ax1.text(0.5, 0.5, summary_text.strip(), ha='center', va='center', fontsize=11,
                 family='monospace', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        metrics_to_plot = [
            ('Test F1', 'test_f1', THESIS_COLORS['primary']['medium_blue']),
            ('Test Accuracy', 'test_acc', THESIS_COLORS['primary']['light_blue']),
            ('Test AUC', 'test_auc', THESIS_COLORS['primary']['sky_blue'])
        ]

        for i, (label, key, color) in enumerate(metrics_to_plot):
            ax_sub = fig.add_subplot(gs[1, i])
            scores = [results[m].get(key, np.nan) for m in models]
            bars = ax_sub.bar(models, np.nan_to_num(scores), color=color, alpha=0.8, edgecolor='black')
            add_numeric_values_to_bars(ax_sub, bars, format_str='.3f', fontsize=12, color='black')
            ax_sub.set_title(label, fontsize=12, fontweight='bold')
            ax_sub.set_ylim([0, 1])
            ax_sub.tick_params(axis='x', rotation=90, labelsize=8)
            ax_sub.grid(axis='y', linestyle='--', alpha=0.5)

        plt.suptitle(
            get_thesis_compliant_title('G24', 'Executive Summary', None),
            fontsize=14, fontweight='bold', y=0.99
        )
        plt.tight_layout(rect=[0, 0, 1, 0.95])

        base_path = os.path.join(output_dir, '24_executive_summary')
        png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
        plt.close()

        graphics.append(pdf_path)
        print(f"    ‚úì G24 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e:
        print(f"    ‚úó G24: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G25: CONFUSION MATRICES (Per Model)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G25] Creating Confusion Matrices...")
        for idx, model_name in enumerate(models):
            fig, ax = plt.subplots(figsize=(8, 6))
            short_model_name = model_name.split(' ')[0].lower()

            if short_model_name in models_dict:
                model = models_dict[short_model_name]
                if y_test is not None and X_test is not None:
                    try:
                        y_pred = model.predict(X_test)
                        cm = confusion_matrix(y_test, y_pred)
                    except:
                        cm = np.zeros((3, 3))
                else:
                    cm = np.zeros((3, 3))
            else:
                cm = np.zeros((3, 3))

            # ‚úÖ THESIS COLORS: Blue colormap
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                        xticklabels=['Away Win', 'Draw', 'Home Win'],
                        yticklabels=['Away Win', 'Draw', 'Home Win'],
                        annot_kws={"size": 14},
                        cbar_kws={'label': 'Count'})

            set_title_if_enabled(ax,
                get_thesis_compliant_title('G25', f'{model_name}'),
                fontsize=14, fontweight='bold'
            )
            ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
            ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')

            plt.tight_layout()

            safe_model_name = model_name.replace(' ', '_').replace('[', '').replace(']', '').replace('(', '').replace(')', '')
            base_path = os.path.join(output_dir, f'25_{idx+1:02d}_{safe_model_name}_cm')
            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
            plt.close(fig)

            graphics.append(pdf_path)

        print(f"    ‚úì G25 - {len(models)} Confusion Matrices saved (PDF format)")

    except Exception as e:
        print(f"    ‚úó G25: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G28: XAI METHOD COMPARISON (EXCEPTION - Keep Original Colors)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G28] Creating XAI Method Comparison...")
        print("    ‚ö†Ô∏è  Exception: Using original multi-color palette")

        g28_expected_path = os.path.join(output_dir, '28_xai_ablation_comparison_v2.png')
        g28_already_exists = os.path.exists(g28_expected_path)

        if g28_already_exists:
            print(f"    ‚ÑπÔ∏è  G28 already exists")

            # Convert existing PNG to PDF
            g28_pdf_path = g28_expected_path.replace('.png', '.pdf')
            if not os.path.exists(g28_pdf_path):
                try:
                    img = plt.imread(g28_expected_path)
                    fig, ax = plt.subplots(figsize=(12, 10))
                    ax.imshow(img)
                    ax.axis('off')

                    pdf_metadata = {
                        'Title': 'Figure G28: XAI Ablation Comparison',
                        'Author': 'Dokumus - Tilburg University',
                        'Subject': 'RQ2: Explainability Analysis',
                        'Keywords': 'XAI, Ablation, Feature Importance',
                        'Creator': 'Matplotlib',
                        'Producer': 'Thesis Research'
                    }

                    plt.savefig(g28_pdf_path, format='pdf', dpi=300,
                               bbox_inches='tight', metadata=pdf_metadata)
                    plt.close()

                    print(f"    ‚úì Converted PNG to PDF")
                except Exception as e_convert:
                    print(f"    ‚ö†Ô∏è  Could not convert to PDF: {e_convert}")
                    g28_pdf_path = g28_expected_path

            if graphics is not None and g28_pdf_path not in graphics:
                graphics.append(g28_pdf_path)
                print(f"    ‚úì G28 added to graphics list")

        elif xai_results and X_train is not None and y_train is not None:
            try:
                print(f"    üîÑ Creating G28...")

                comparison_path = create_feature_importance_comparison_v19_ABLATION(
                    models_dict=models_dict,
                    xai_results=xai_results,
                    X_train=X_train,
                    X_test=X_test,
                    y_train=y_train,
                    y_test=y_test,
                    X_val=X_val,
                    y_val=y_val,
                    features=features,
                    output_dir=output_dir,
                    results_dict=results
                )

                if comparison_path and os.path.exists(comparison_path):
                    # Convert to PDF if PNG
                    if comparison_path.endswith('.png'):
                        pdf_path = comparison_path.replace('.png', '.pdf')
                        try:
                            img = plt.imread(comparison_path)
                            fig, ax = plt.subplots(figsize=(12, 10))
                            ax.imshow(img)
                            ax.axis('off')

                            pdf_metadata = {
                                'Title': 'Figure G28: XAI Ablation Comparison',
                                'Author': 'Dokumus - Tilburg University',
                                'Subject': 'RQ2: Explainability Analysis',
                                'Keywords': 'XAI, Ablation, Feature Importance',
                                'Creator': 'Matplotlib',
                                'Producer': 'Thesis Research'
                            }

                            plt.savefig(pdf_path, format='pdf', dpi=300,
                                       bbox_inches='tight', metadata=pdf_metadata)
                            plt.close()

                            comparison_path = pdf_path
                        except:
                            pass

                    if graphics is not None:
                        if comparison_path not in graphics:
                            graphics.append(comparison_path)
                            print(f"    ‚úì G28 created and added (PDF: {os.path.getsize(comparison_path)/1024:.1f} KB)")

            except Exception as e_g28:
                print(f"    ‚ùå G28 creation error: {str(e_g28)[:100]}")

    except Exception as e_outer:
        print(f"    ‚ùå G28 outer exception: {e_outer}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G29: CUMULATIVE IMPORTANCE COMPARISON
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if 29 not in CONFIG.get("graphics_to_skip", []):
        try:
            print("  [G29] Creating Cumulative Importance Comparison...")
            if xai_results and X_train is not None:
                g29_path = create_cumulative_importance_comparison(
                    xai_results=xai_results,
                    features=features,
                    output_dir=output_dir,
                    results_dict=results
                )
                if g29_path and os.path.exists(g29_path):
                    # Convert to PDF if PNG
                    if g29_path.endswith('.png'):
                        pdf_path = g29_path.replace('.png', '.pdf')
                        try:
                            img = plt.imread(g29_path)
                            fig, ax = plt.subplots(figsize=(12, 8))
                            ax.imshow(img)
                            ax.axis('off')

                            pdf_metadata = {
                                'Title': 'Figure G29: Cumulative Importance Comparison',
                                'Author': 'Dokumus - Tilburg University',
                                'Subject': 'RQ2: Explainability Analysis',
                                'Keywords': 'XAI, Cumulative Importance, Feature Ranking',
                                'Creator': 'Matplotlib',
                                'Producer': 'Thesis Research'
                            }

                            plt.savefig(pdf_path, format='pdf', dpi=300,
                                       bbox_inches='tight', metadata=pdf_metadata)
                            plt.close()

                            g29_path = pdf_path
                        except:
                            pass

                    if graphics is not None:
                        graphics.append(g29_path)
                    print(f"    ‚úì G29 saved (PDF: {os.path.getsize(g29_path)/1024:.1f} KB)")

        except Exception as e_g29:
            print(f"    ‚ùå G29: {e_g29}")
    else:
        print(f"    ‚è∏Ô∏è  G29 - Skipped (CONFIG)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # G31: PER-CLASS ACCURACY HEATMAP
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        print("  [G31] Creating Per-Class Accuracy Heatmap...")

        class_names = ['Away Win', 'Draw', 'Home Win']
        data_matrix = []
        model_labels = []

        for model_key in models:
            metrics = results.get(model_key, {})

            if 'per_class_metrics' in metrics and 'accuracy' in metrics['per_class_metrics']:
                row = []
                for class_name in class_names:
                    acc = metrics['per_class_metrics']['accuracy'].get(class_name, 0)
                    row.append(acc)

                data_matrix.append(row)
                model_labels.append(model_key)

        if len(data_matrix) > 0:
            data_matrix = np.array(data_matrix)

            fig, ax = plt.subplots(figsize=(10, 12))

            # ‚úÖ THESIS COLORS: Blue-based colormap
            from matplotlib.colors import LinearSegmentedColormap
            colors_per_class = [
                THESIS_COLORS['accent']['warning'],      # Low (red)
                THESIS_COLORS['accent']['highlight'],    # Medium (orange)
                THESIS_COLORS['primary']['light_blue'],  # Good (light blue)
                THESIS_COLORS['primary']['medium_blue'], # Very good (medium blue)
                THESIS_COLORS['primary']['dark_blue']    # Excellent (dark blue)
            ]
            cmap = LinearSegmentedColormap.from_list('thesis_per_class', colors_per_class, N=100)

            im = ax.imshow(data_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

            ax.set_xticks(np.arange(len(class_names)))
            ax.set_yticks(np.arange(len(model_labels)))
            ax.set_xticklabels(class_names, fontsize=11, fontweight='bold')
            ax.set_yticklabels(model_labels, fontsize=12)

            plt.setp(ax.get_xticklabels(), rotation=0, ha="center")

            for i in range(len(model_labels)):
                for j in range(len(class_names)):
                    acc = data_matrix[i, j]
                    text_color = "white" if acc < 0.5 else "black"

                    text = ax.text(j, i,
                                 f'{acc:.3f}\n({acc*100:.1f}%)',
                                 ha="center", va="center",
                                 color=text_color,
                                 fontsize=12,
                                 fontweight='bold')

            set_title_if_enabled(ax,
                get_thesis_compliant_title(
                    'G31',
                    'Per-Class Accuracy by Model\nPrediction Performance per Match Outcome'
                ),
                fontsize=13, fontweight='bold', pad=20
            )

            ax.set_ylabel('Model', fontsize=12, fontweight='bold')
            ax.set_xlabel('Match Outcome', fontsize=12, fontweight='bold')

            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('Accuracy', rotation=270, labelpad=20,
                          fontsize=11, fontweight='bold')

            ax.set_xticks(np.arange(len(class_names)+1)-.5, minor=True)
            ax.set_yticks(np.arange(len(model_labels)+1)-.5, minor=True)
            ax.grid(which="minor", color="white", linestyle='-', linewidth=2)

            plt.tight_layout()

            base_path = os.path.join(output_dir, '31_per_class_accuracy_heatmap')
            png_path, pdf_path = save_figure_dual_format(fig, base_path, dpi=300)
            plt.close()

            graphics.append(pdf_path)
            print(f"    ‚úì G31 saved (PDF: {os.path.getsize(pdf_path)/1024:.1f} KB)")

    except Exception as e_g31:
        print(f"    ‚ùå G31: {e_g31}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # FINAL SUMMARY
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print("\n" + "="*80)
    print("‚úÖ GRAPHICS CREATION COMPLETE")
    print("="*80)
    print(f"  üìä Total graphics created: {len(graphics)}")
    print(f"  üìê Format: Dual (PNG + PDF)")
    print(f"  üé® Color scheme: Thesis-compliant blue tones")
    print(f"  ‚ö†Ô∏è  Exception: G28 (multi-color preserved)")
    print(f"  üìÅ Output directory: {output_dir}")

    # File size summary
    total_pdf_size = sum(os.path.getsize(p)/1024 for p in graphics if os.path.exists(p))
    print(f"  üíæ Total PDF size: {total_pdf_size:.1f} KB ({total_pdf_size/1024:.1f} MB)")
    print("="*80 + "\n")

    return graphics

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# END OF create_dynamic_graphics() FUNCTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_cumulative_importance_comparison(xai_results, features, output_dir, results_dict):
    """
    G29: Cumulative Feature Importance Comparison
    Paper-style comparison of XAI methods - BEST MODEL FOCUSED

    Args:
        xai_results: XAI analysis results dictionary
        features: List of feature names
        output_dir: Output directory for saving
        results_dict: Model performance results (for finding best model)

    Returns:
        str: Path to saved graphic or None
    """

    print("\n[G29] Creating Cumulative Importance Comparison...")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 1: EN ƒ∞Yƒ∞ MODELƒ∞ BUL (F1-Score'a g√∂re)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not xai_results:
        print("  ‚ö†Ô∏è  No XAI results available")
        return None

    # Test F1 skorlarƒ±na g√∂re en iyi modeli se√ß
    valid_models_f1 = {
        m: results_dict[m].get('test_f1', -1)
        for m in results_dict.keys()
        if pd.notna(results_dict[m].get('test_f1'))
    }

    if not valid_models_f1:
        print("  ‚ö†Ô∏è  No valid F1 scores found")
        return None

    best_model_key = max(valid_models_f1, key=valid_models_f1.get)
    best_model_short = best_model_key.split(' ')[0].lower()  # "XGBOOST (Optuna)" ‚Üí "xgboost"
    best_f1_score = valid_models_f1[best_model_key]

    print(f"  üèÜ Best Model: {best_model_key} (F1: {best_f1_score:.4f})")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 2: EN ƒ∞Yƒ∞ MODELƒ∞N XAI SONU√áLARINI AL
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if best_model_short not in xai_results:
        print(f"  ‚ö†Ô∏è  No XAI results for {best_model_key}")
        print(f"  Available models: {list(xai_results.keys())}")
        return None

    xai_data = xai_results[best_model_short]

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 3: MEVCUT METOTLARI KONTROL ET
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    methods_to_plot = ['PFI', 'PMI', 'SOFI', 'SHAP', 'LTCN', 'XGBoost']
    available_methods = []

    for method in methods_to_plot:
        if method in xai_data and xai_data[method] is not None:
            if len(xai_data[method]) > 0:
                available_methods.append(method)

    if len(available_methods) < 2:
        print(f"  ‚ö†Ô∏è  Not enough methods available ({len(available_methods)} found)")
        return None

    print(f"  üìä Methods available: {', '.join(available_methods)}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ YENƒ∞ EKLEME: MAX FEATURES TRUNCATION CONFIG
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    xai_config = CONFIG.get("xai", {})
    MAX_FEATURES = xai_config.get("ablation_max_features", 30)

    print(f"  üìä Max features for cumulative plot: {MAX_FEATURES}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 4: GRAFƒ∞K OLU≈ûTUR
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        fig, ax = plt.subplots(figsize=(12, 8))

        # Renk ve marker ayarlarƒ±
        colors = {
            'PFI': '#9b59b6',      # Mor
            'PMI': '#1abc9c',      # Turkuaz
            'SOFI': '#e74c3c',     # Kƒ±rmƒ±zƒ±
            'SHAP': '#f39c12',     # Turuncu
            'LTCN': '#3498db',     # Mavi
            'XGBoost': '#2ecc71',  # Ye≈üil
        }
        markers = {
            'PFI': 'o',
            'PMI': 's',
            'SOFI': 'D',
            'SHAP': '^',
            'LTCN': 'v',
            'XGBoost': 'p',
        }

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 5: HER METOD ƒ∞√áƒ∞N CUMULATIVE IMPORTANCE HESAPLA
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        plotted_methods = []

        for method in available_methods:
            try:
                importance = xai_data[method]

                # Feature'larƒ± √∂nem sƒ±rasƒ±na g√∂re sƒ±rala
                sorted_indices = np.argsort(importance)[::-1]

                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # ‚úÖ TRUNCATE TO MAX_FEATURES
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                if MAX_FEATURES and MAX_FEATURES < len(sorted_indices):
                    print(f"      {method}: Using top {MAX_FEATURES} features (was {len(sorted_indices)})")
                    sorted_indices = sorted_indices[:MAX_FEATURES]

                sorted_importance = importance[sorted_indices]

                # Normalize et (0-1 arasƒ±)
                if sorted_importance.sum() > 0:
                    sorted_importance = sorted_importance / sorted_importance.sum()
                else:
                    print(f"  ‚ö†Ô∏è  {method}: All zeros, skipping")
                    continue

                # Cumulative sum hesapla
                cumulative = np.cumsum(sorted_importance)

                # Plot
                x = np.arange(1, len(cumulative) + 1)
                line = ax.plot(
                    x,
                    cumulative,
                    label=method,
                    color=colors.get(method, 'gray'),
                    marker=markers.get(method, 'o'),
                    markersize=7,
                    linewidth=2.5,
                    markevery=max(1, len(x)//10),  # Her 10 noktada marker
                    alpha=0.85
                )

                plotted_methods.append(method)

            except Exception as e_method:
                print(f"  ‚ö†Ô∏è  {method} error: {e_method}")
                continue

        if len(plotted_methods) < 2:
            print(f"  ‚ùå Not enough methods plotted ({len(plotted_methods)})")
            plt.close()
            return None

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 6: RANDOM BASELINE EKLE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        n_features = MAX_FEATURES if MAX_FEATURES else len(features)
        random_cumulative = np.cumsum(np.ones(n_features) / n_features)
        ax.plot(
            np.arange(1, n_features + 1),
            random_cumulative,
            label='Random Baseline',
            color='black',
            linestyle='--',
            linewidth=2.5,
            alpha=0.6,
            zorder=1
        )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 7: STYLING VE ANNOTATIONS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # Axes labels
        ax.set_xlabel('œÄ·µ¢ (Feature Rank)', fontsize=14, fontweight='bold')
        ax.set_ylabel('g(œÄ) (Cumulative Importance)', fontsize=14, fontweight='bold')

        # Title
        title_text = f'Graphic 29: Feature Importance Comparison\n'
        title_text += f'Cumulative Distribution by XAI Method ({best_model_key})'
        set_title_if_enabled(ax,title_text, fontsize=15, fontweight='bold', pad=20)

        # Legend
        ax.legend(
            loc='lower right',
            fontsize=11,
            framealpha=0.95,
            edgecolor='black',
            fancybox=True,
            shadow=True
        )

        # Grid
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)

        # Limits
        ax.set_xlim(0, n_features + 1)
        ax.set_ylim(0, 1.05)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 8: THRESHOLD √áƒ∞ZGƒ∞LERƒ∞ EKLE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # 80% threshold
        ax.axhline(y=0.8, color='red', linestyle=':', linewidth=1.8, alpha=0.6, zorder=2)
        ax.text(
            n_features * 0.75, 0.82, '80% Importance',
            fontsize=12, color='red', alpha=0.8, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)
        )

        # 90% threshold (opsiyonel)
        ax.axhline(y=0.9, color='orange', linestyle=':', linewidth=1.5, alpha=0.5, zorder=2)
        ax.text(
            n_features * 0.75, 0.92, '90%',
            fontsize=12, color='orange', alpha=0.7
        )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 9: MODEL Bƒ∞LGƒ∞Sƒ∞ KUTUSU EKLE
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        info_text = f'Model: {best_model_key}\n'
        info_text += f'Test F1: {best_f1_score:.4f}\n'
        info_text += f'Features: {n_features}\n'
        info_text += f'Methods: {len(plotted_methods)}'

        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='black')
        ax.text(
            0.02, 0.98, info_text,
            transform=ax.transAxes,
            fontsize=12,
            verticalalignment='top',
            bbox=props,
            family='monospace'
        )

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 10: KAYDET VE KAPAT
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        plt.tight_layout()

        path = os.path.join(output_dir, '29_cumulative_importance_comparison.png')
        plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)

        print(f"  ‚úÖ G29 saved: {os.path.basename(path)}")
        print(f"  üìä Plotted methods: {', '.join(plotted_methods)}\n")

        return path

    except Exception as e:
        print(f"  ‚ùå G29 creation error: {e}")
        import traceback
        traceback.print_exc()
        plt.close('all')
        return None

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ YENƒ∞ FONKSƒ∞YON: SAVE TABLES AS PNG (SATIR ~4600 Cƒ∞VARI EKLE)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def save_tables_as_png(tables, output_dir):
    """
    Save tables as PNG images for PDF inclusion

    Args:
        tables (dict): Dictionary of {table_name: DataFrame}
        output_dir (str): Directory to save PNG files

    Returns:
        list: List of saved PNG file paths
    """
    import matplotlib.pyplot as plt
    import pandas as pd

    print("\n[SAVE] Saving tables as PNG...\n")

    saved_paths = []

    for table_name, df in tables.items():
        if not isinstance(df, pd.DataFrame):
            print(f"  ‚ö†Ô∏è  Skipping {table_name} (not a DataFrame)")
            continue

        try:
            # Figure olu≈ütur
            fig, ax = plt.subplots(figsize=(12, max(6, len(df)*0.4)))
            ax.axis('tight')
            ax.axis('off')

            # Tabloyu √ßiz
            table = ax.table(
                cellText=df.values,
                colLabels=df.columns,
                cellLoc='center',
                loc='center',
                colWidths=[0.15]*len(df.columns)
            )

            table.auto_set_font_size(False)
            table.set_fontsize(12)
            table.scale(1, 1.5)

            # Header stilini ayarla
            for (i, j), cell in table.get_celld().items():
                if i == 0:  # Header row
                    cell.set_facecolor('#3498db')
                    cell.set_text_props(weight='bold', color='white')
                else:
                    cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')

            plt.title(table_name.replace('_', ' '),
                     fontsize=12, fontweight='bold', pad=20)
            plt.tight_layout()

            # Kaydet
            png_path = os.path.join(output_dir, f"{table_name}.png")
            plt.savefig(png_path, dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            plt.close()

            saved_paths.append(png_path)
            print(f"  ‚úì {table_name} -> PNG")

        except Exception as e:
            print(f"  ‚úó {table_name}: {e}")
            import traceback
            traceback.print_exc()

    print(f"\n‚úÖ Tables saved to: {output_dir}")
    print(f"   Total saved: {len(saved_paths)} PNG files\n")

    return saved_paths

# SAVE TABLES
def save_tables_as_csv_and_excel(tables, output_dir):
    """Save tables as CSV and Excel for high-resolution use"""
    print("\n[SAVE] Saving tables as CSV and Excel...\n")

    # Tek bir Excel dosyasƒ± i√ßin bir writer olu≈ütur
    excel_path = os.path.join(output_dir, "All_Tables_v18.xlsx")
    try:
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            for table_name, df in tables.items():
                if not isinstance(df, pd.DataFrame):
                    print(f"  ‚ö†Ô∏è  Skipping {table_name} (not a DataFrame)")
                    continue

                # 1. Excel'e kaydet (her tabloyu ayrƒ± bir sekmeye)
                try:
                    # Excel sekme adlarƒ± 31 karakteri ge√ßemez
                    safe_sheet_name = table_name[:31]
                    df.to_excel(writer, sheet_name=safe_sheet_name, index=False)
                    print(f"  ‚úì {table_name} -> Excel sheet '{safe_sheet_name}'")
                except Exception as e_excel:
                    print(f"  ‚úó {table_name} (Excel): {e_excel}")
                # 2. Ayrƒ± CSV dosyalarƒ± olarak kaydet
                try:
                    csv_path = os.path.join(output_dir, f"{table_name}.csv")
                    df.to_csv(csv_path, index=False, encoding='utf-8-sig') # utf-8-sig T√ºrk√ße karakterler i√ßin
                    print(f"  ‚úì {table_name} -> CSV file")
                except Exception as e_csv:
                    print(f"  ‚úó {table_name} (CSV): {e_csv}")

        print(f"\n‚úÖ All tables saved to: {excel_path}")

    except Exception as e_writer:
        # Excel writer hata verirse (k√ºt√ºphane eksikse vb.) sadece CSV kaydet
        print(f"‚ùå Excel Writer Error: {e_writer}. Saving as separate CSVs only.")
        for table_name, df in tables.items():
            if isinstance(df, pd.DataFrame):
                try:
                    csv_path = os.path.join(output_dir, f"{table_name}.csv")
                    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                    print(f"  ‚úì {table_name} -> CSV file")
                except Exception as e_csv:
                    print(f"  ‚úó {table_name} (CSV): {e_csv}")
# SAVE RESULTS FUNCTION
def save_results_safely(results, output_dir):
    """Save results to JSON"""
    try:
        json_path = os.path.join(output_dir, "results.json")

        # Convert numpy types to Python types
        safe_results = {}
        for model_key, metrics in results.items():
            if isinstance(metrics, dict):
                safe_results[model_key] = {}
                for metric_name, metric_value in metrics.items():
                    # Per-class metrics'i √∂zel olarak i≈üle
                    if metric_name == 'per_class_metrics' and isinstance(metric_value, dict):
                        safe_results[model_key][metric_name] = {}
                        for class_metric, class_data in metric_value.items():
                            if isinstance(class_data, dict):
                                safe_results[model_key][metric_name][class_metric] = {
                                    k: int(v) if isinstance(v, (np.integer, int)) else float(v)
                                    for k, v in class_data.items()
                                }
                            else:
                                safe_results[model_key][metric_name][class_metric] = class_data
                    # Normal metrikleri i≈üle
                    elif isinstance(metric_value, (np.floating, np.integer)):
                        safe_results[model_key][metric_name] = float(metric_value)
                    elif isinstance(metric_value, (float, int, str)):
                        safe_results[model_key][metric_name] = metric_value
                    elif pd.isna(metric_value):
                        safe_results[model_key][metric_name] = None
                    else:
                        safe_results[model_key][metric_name] = str(metric_value)
            else:
                safe_results[model_key] = str(metrics)

        with open(json_path, 'w') as f:
            json.dump(safe_results, f, indent=2)

        print(f"‚úÖ Results saved: {json_path}")
        return json_path

    except Exception as e:
        print(f"‚ùå Could not save results: {e}")
        import traceback
        traceback.print_exc()
        return None

# CREATE PDF
# Satƒ±r 2827 civarƒ± - TAMAMEN DEƒûƒ∞≈ûTƒ∞R
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìÑ CREATE PDF REPORT - TEXT SUMMARY ONLY (VEKT√ñREL AKI≈û ƒ∞√áƒ∞N D√úZENLENMƒ∞≈û)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚úÖ Bu versiyon SADECE ba≈ülƒ±k/√∂zet sayfasƒ± olu≈üturur
# ‚úÖ Grafik ve tablolarƒ±n Overleaf'e manuel y√ºklenmesi beklenir
# ‚úÖ Vekt√∂rel kalite korunur (rasterize olmaz)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_pdf_report(output_dir, graphics, tables):
    """
    Sadece metin √∂zeti ve kapak sayfasƒ± i√ßeren basit bir PDF olu≈üturur.
    Grafik ve tablolarƒ±n Overleaf'te manuel olarak eklenmesi beklenir (Vekt√∂rel akƒ±≈ü).
    Bu, vekt√∂rel kalitenin korunmasƒ±nƒ± garantiler.

    Parameters:
    -----------
    output_dir : str
        PDF'in kaydedileceƒüi dizin
    graphics : list
        Grafik dosyalarƒ±nƒ±n listesi (kullanƒ±lmaz ama uyumluluk i√ßin tutulur)
    tables : list
        Tablo dosyalarƒ±nƒ±n listesi (kullanƒ±lmaz ama uyumluluk i√ßin tutulur)

    Returns:
    --------
    str or None
        PDF dosyasƒ±nƒ±n tam yolu (ba≈üarƒ±lƒ± ise), None (hata varsa)
    """

    print("\n8Ô∏è‚É£ CREATING PDF (Text Summary Only)...\n")

    # ‚úÖ D√úZELTME 1: √áƒ±ktƒ± dosyasƒ±nƒ± Metin √ñzeti olarak adlandƒ±r
    pdf_path = os.path.join(output_dir, "Football_Prediction_v18_TEXT_SUMMARY.pdf")

    try:
        # ‚ö†Ô∏è Yalnƒ±zca Matplotlib tarafƒ±ndan olu≈üturulan i√ßeriƒüi topluyoruz
        with PdfPages(pdf_path) as pdf:

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # TITLE PAGE (Ba≈ülƒ±k sayfasƒ± i√ßeriƒüi) - KORUNUR
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

            fig = plt.figure(figsize=(8.5, 11))  # US Letter size
            ax = fig.add_subplot(111)
            ax.axis('off')

            # Ana ba≈ülƒ±k
            ax.text(
                0.5, 0.85,
                'Football Match Prediction System',
                ha='center',
                va='top',
                fontsize=24,
                fontweight='bold',
                transform=ax.transAxes
            )

            # Alt ba≈ülƒ±k
            ax.text(
                0.5, 0.75,
                'v18.0 - ODDS-PLUS WITH EDA PIPELINE',
                ha='center',
                va='top',
                fontsize=18,
                transform=ax.transAxes
            )

            # A√ßƒ±klama
            ax.text(
                0.5, 0.65,
                'EDA Integration + Best Model Focus',
                ha='center',
                va='top',
                fontsize=12,
                style='italic',
                transform=ax.transAxes
            )

            # Tarih
            ax.text(
                0.5, 0.45,
                f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
                ha='center',
                va='top',
                fontsize=12,
                transform=ax.transAxes
            )

            # √ñzellikler listesi
            features_text = """‚úÖ Advanced Fuzzy Matching
‚úÖ Data Integration (Matches + ELO + Transfermarkt)
‚úÖ EDA Pipeline Integration
‚úÖ 8 ML Models (Including LTCN)
‚úÖ 6 XAI Methods (PFI, PMI, SOFI, SHAP, LTCN, XGBoost)
‚úÖ Confusion Matrices
‚úÖ Professional Graphics (VECTOR FORMAT)
‚úÖ Comprehensive Ablation Analysis"""

            ax.text(
                0.5, 0.20,
                features_text,
                ha='center',
                va='top',
                fontsize=12,
                transform=ax.transAxes,
                family='monospace'
            )

            # Uyarƒ± metni
            ax.text(
                0.5, 0.05,
                '‚ö†Ô∏è Graphics and tables available as separate vector PDF files',
                ha='center',
                va='top',
                fontsize=12,
                style='italic',
                color='red',
                transform=ax.transAxes
            )

            # Sayfayƒ± kaydet
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ D√úZELTME 2: GRAFƒ∞K VE TABLO EKLEME MANTIKLARI KALDIRILDI!
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # Grafik ve tablolarƒ±n ayrƒ± PDF dosyalarƒ± olarak Overleaf'e
            # y√ºklenmesi beklenir. Bu sayede vekt√∂rel kalite korunur.
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # BA≈ûARI RAPORU
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB cinsinden

        print(f"\n{'='*80}")
        print(f"‚úÖ PDF TEXT SUMMARY CREATED SUCCESSFULLY")
        print(f"{'='*80}")
        print(f"   üìÑ Path: {pdf_path}")
        print(f"   üíæ Size: {file_size:.2f} MB")
        print(f"   üìä Pages: 1 (Title page only)")
        print(f"   üé® Format: Vector (Matplotlib)")
        print(f"\n   üì¶ ADDITIONAL FILES (for Overleaf):")
        print(f"      ‚Ä¢ {len(graphics)} graphics (separate vector PDFs)")
        print(f"      ‚Ä¢ {len(tables)} tables (separate PNG files)")
        print(f"\n   ‚ö†Ô∏è  NEXT STEP:")
        print(f"      Upload all vector PDF graphics to Overleaf for final document")
        print(f"{'='*80}\n")

        return pdf_path

    except Exception as e:
        print(f"\n{'='*80}")
        print(f"‚ùå PDF CREATION ERROR")
        print(f"{'='*80}")
        print(f"   Error: {e}")
        print(f"{'='*80}\n")

        # Detaylƒ± hata ayƒ±klama bilgisi
        import traceback
        traceback.print_exc()

        return None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ ENHANCED: Graphics list validation + auto-recovery
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    if graphics is None:
        print("  ‚ö†Ô∏è  Warning: Graphics list is None - attempting recovery...")
        graphics = []

        # ‚úÖ RECOVERY: Scan graphics directory
        graphics_dir = os.path.join(output_dir, 'graphics')
        if os.path.exists(graphics_dir):
            found_files = sorted([
                os.path.join(graphics_dir, f)
                for f in os.listdir(graphics_dir)
                if f.endswith('.png') and not f.startswith('.')
            ])

            if found_files:
                graphics = found_files
                print(f"  ‚úÖ Recovered {len(graphics)} graphics from directory")

                # Debug: Show what we found
                print(f"\n  üìã Graphics to be added to PDF:")
                for idx, path in enumerate(graphics, 1):
                    filename = os.path.basename(path)
                    size_kb = os.path.getsize(path) / 1024
                    print(f"     {idx:2d}. {filename:50s} ({size_kb:6.1f} KB)")
                print()
            else:
                print(f"  ‚ùå No graphics found in {graphics_dir}")
        else:
            print(f"  ‚ùå Graphics directory not found: {graphics_dir}")

    if tables is None:
        tables = {}
        print("  ‚ö†Ô∏è  Warning: Tables dictionary is None, using empty dict")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ VALIDATION: Check if G28 is in the list
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    g28_files = [g for g in graphics if '28_xai' in os.path.basename(g)]

    if g28_files:
        print(f"  ‚úÖ G28 found in graphics list: {len(g28_files)} file(s)")
        for g28 in g28_files:
            print(f"     ‚Ä¢ {os.path.basename(g28)}")
    else:
        print(f"  ‚ö†Ô∏è  WARNING: G28 not in graphics list!")
        print(f"     PDF will be missing XAI comparison graphic")

        # ‚úÖ LAST-CHANCE RECOVERY: Check if file exists but not in list
        graphics_dir = os.path.join(output_dir, 'graphics')
        g28_pattern = os.path.join(graphics_dir, '28_xai*.png')

        import glob
        g28_existing = glob.glob(g28_pattern)

        if g28_existing:
            print(f"  üîÑ G28 exists but not in list - adding now...")
            for g28 in g28_existing:
                if g28 not in graphics:
                    graphics.append(g28)
                    print(f"     ‚úÖ Added: {os.path.basename(g28)}")
        else:
            print(f"     ‚ùå G28 file not found in directory either")

    print()  # Blank line

    pdf_path = os.path.join(output_dir, "Football_Prediction_v18_COMPLETE.pdf")
    try:
        with PdfPages(pdf_path) as pdf:
            # Title page
            fig = plt.figure(figsize=(8.5, 11))
            ax = fig.add_subplot(111)
            ax.axis('off')

            ax.text(0.5, 0.85, 'Football Match Prediction System',
                    ha='center', va='top', fontsize=24, fontweight='bold',
                    transform=ax.transAxes)
            ax.text(0.5, 0.75, 'v18.0 - ODDS-PLUS WITH EDA PIPELINE',
                    ha='center', va='top', fontsize=18,
                    transform=ax.transAxes)
            ax.text(0.5, 0.65, 'EDA Integration + Best Model Focus',
                    ha='center', va='top', fontsize=12, style='italic',
                    transform=ax.transAxes)

            ax.text(0.5, 0.45, f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
                    ha='center', va='top', fontsize=12,
                    transform=ax.transAxes)

            features_text = """‚úÖ Advanced Fuzzy Matching
‚úÖ Data Integration (Matches + ELO + Transfermarkt)
‚úÖ EDA Pipeline
‚úÖ 8 ML Models (Including LTCN)
‚úÖ XAI Analysis
‚úÖ Confusion Matrices
‚úÖ Professional Graphics"""

            ax.text(0.5, 0.20, features_text,
                    ha='center', va='top', fontsize=12,
                    transform=ax.transAxes, family='monospace')

            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # ============================================================================
            # FIX 2: Graphics ekleme - g√ºvenli kontrol
            # ============================================================================
            print("  Adding graphics...")
            if graphics and len(graphics) > 0:  # ‚Üê Eklenen kontrol
                for graphic_path in graphics:
                    if os.path.exists(graphic_path):
                        try:
                            img = plt.imread(graphic_path)
                            fig, ax = plt.subplots(figsize=(11, 8.5))
                            ax.imshow(img)
                            ax.axis('off')
                            pdf.savefig(fig, bbox_inches='tight', orientation='landscape')
                            plt.close()
                        except Exception as e_img:
                            print(f"    ‚ö†Ô∏è  Could not add graphic: {os.path.basename(graphic_path)}")
            else:
                print("    ‚ö†Ô∏è  No graphics to add")

            # ============================================================================
            # FIX 3: Tables ekleme - g√ºvenli kontrol
            # ============================================================================
            print("  Adding tables...")
            if os.path.exists(TABLES_DIR):  # ‚Üê Eklenen kontrol
                table_files = sorted([f for f in os.listdir(TABLES_DIR) if f.endswith('.png')])
                if table_files:
                    for table_file in table_files:
                        table_path = os.path.join(TABLES_DIR, table_file)
                        try:
                            img = plt.imread(table_path)
                            fig, ax = plt.subplots(figsize=(11, 8.5))
                            ax.imshow(img)
                            ax.axis('off')
                            pdf.savefig(fig, bbox_inches='tight', orientation='landscape')
                            plt.close()
                        except Exception as e_table:
                            print(f"    ‚ö†Ô∏è  Could not add table: {table_file}")
                else:
                    print("    ‚ö†Ô∏è  No table files found")
            else:
                print(f"    ‚ö†Ô∏è  Tables directory not found: {TABLES_DIR}")

        print(f"‚úÖ PDF: {pdf_path}\n")
        return pdf_path

    except Exception as e:
        print(f"‚ùå PDF error: {e}\n")
        import traceback
        traceback.print_exc()
        return None

# MAIN EXECUTION
if __name__ == "__main__":

    print("\n" + "="*100)
    print("üöÄ FOOTBALL MATCH PREDICTION SYSTEM v18.0 (Single Scenario Mode)")
    print("="*100 + "\n")

    start_time = time.time()

    try:
        # -----------------------------------------------------------------
        # ADIM 1: VERƒ∞ Y√úKLEME VE Bƒ∞RLE≈ûTƒ∞RME
        # -----------------------------------------------------------------
        print("\n1Ô∏è‚É£ DATA INTEGRATION...\n")
        data_integration = DataIntegration()
        df = data_integration.merge_data()
        print(f"‚úÖ Data merged: {len(df):,} matches √ó {len(df.columns)} features\n")

        if len(df) == 0:
            print("‚ùå FATAL ERROR: No data after merge!")
            sys.exit(1)

        # -----------------------------------------------------------------
        # ADIM 1.5: LAG FEATURES (GECƒ∞KMELƒ∞ √ñZELLƒ∞KLER) OLU≈ûTURMA
        # -----------------------------------------------------------------
        print("\n1Ô∏è‚É£.5Ô∏è‚É£ LAG FEATURES CREATION (Leakage-Free)...\n")

        # Hangi in-game features'larƒ± lag'e √ßevirelim?
        IN_GAME_FEATURES = [
            'HomeTarget',    # ƒ∞sabetli ≈üut
            'AwayTarget',
            'HomeShot',      # Toplam ≈üut
            'AwayShot',
            'HomeShots', # <-- EKLENDƒ∞
            'AwayShots',
            'HomeCorners',   # Korner
            'AwayCorners',
            'HomeFouls',     # Faul (opsiyonel)
            'AwayFouls',
            'HomeYellow',
            'AwayYellow',
            'HomeRed',
            'AwayRed',
        ]

        # Mevcut olanlarƒ± filtrele
        features_to_lag = [f for f in IN_GAME_FEATURES if f in df.columns]

        if features_to_lag:
            print(f"  üîÑ Converting {len(features_to_lag)} in-game features to lag features:")
            for feat in features_to_lag:
                print(f"     ‚Ä¢ {feat}")
            print()

            # Lag features olu≈ütur
            df, lag_feature_names = create_lag_features(
                df=df,
                feature_cols=features_to_lag,
                windows=[3, 5],        # Son 3 ve son 5 ma√ß
                home_away_split=True     # Home/away ayrƒ±mƒ± yap
            )

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ORIGINAL IN-GAME FEATURES'I ≈ûƒ∞MDƒ∞ KALDIR!
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            print(f"  üóëÔ∏è  Removing original in-game features (now replaced with lags)...")

            features_removed = []
            for feat in features_to_lag:
                if feat in df.columns:
                    df = df.drop(columns=[feat])
                    features_removed.append(feat)

            print(f"     ‚úÖ Removed {len(features_removed)} original features")
            print(f"     ‚úÖ Added {len(lag_feature_names)} lag features")
            print(f"     üìä Net change: +{len(lag_feature_names) - len(features_removed)} features\n")

            # ‚úÖ YENƒ∞: Kartlarƒ±n kaldƒ±rƒ±ldƒ±ƒüƒ±nƒ± √∂zellikle vurgula
            card_features_removed = [f for f in features_removed if 'Red' in f or 'Yellow' in f]
            if card_features_removed:
                print(f"     üîí LEAKAGE PREVENTED: {len(card_features_removed)} card features removed:")
                for feat in card_features_removed:
                    print(f"        ‚Ä¢ {feat} ‚Üí Replaced with lag features")
                print()

            # Lag features listesini kaydet
            lag_features_path = os.path.join(DATA_DIR, "lag_features_created.txt")
            with open(lag_features_path, 'w') as f:
                 f.write("LAG FEATURES CREATED\n")
                 f.write("="*80 + "\n\n")
                 f.write(f"Original features converted: {len(features_to_lag)}\n")
                 f.write(f"Lag features created: {len(lag_feature_names)}\n")
                 f.write(f"Windows used: [3, 5]\n")
                 f.write(f"Home/Away split: True\n\n")
                 f.write("List of lag features:\n")
                 f.write("-"*80 + "\n")
                 for i, feat in enumerate(lag_feature_names, 1):
                     f.write(f"{i:3d}. {feat}\n")
            print(f"     üìÑ Lag features list saved: {os.path.basename(lag_features_path)}\n")

        else:
            print(f"  ‚ö†Ô∏è  No in-game features found in dataset")
            print(f"  ‚ÑπÔ∏è  Dataset may already be clean or features have different names\n")

        print("="*100 + "\n")


        # -----------------------------------------------------------------
        # ADIM 2: PREPARE FEATURES (ƒ∞LK HAZIRLIK)
        # -----------------------------------------------------------------
        print("\n2Ô∏è‚É£ FEATURE PREPARATION...\n")
        n_select = CONFIG['feature_toggles'].get('n_features_to_select', None)
        use_selection = CONFIG['feature_toggles'].get('use_feature_selection', False)
        if not use_selection:
            n_select = None

        X, y, features = prepare_features(df, n_features=n_select)

        if X.empty:
            print("‚ùå FATAL ERROR: Empty dataset!")
            sys.exit(1)
        print(f"‚úÖ Features: {X.shape[0]} √ó {X.shape[1]}\n")

        print("\nüìä SCENARIO: ODDS-PLUS\n")

        # -----------------------------------------------------------------
        # ADIM 3: TRAIN/TEST SPLIT
        # -----------------------------------------------------------------
        print("\n3Ô∏è‚É£ TRAIN/TEST SPLIT...\n")
        try:
            CUTOFF_DATE = pd.to_datetime('2022-08-31')
            train_indices = df.index[df['MatchDate'] < CUTOFF_DATE]
            test_indices = df.index[df['MatchDate'] >= CUTOFF_DATE]

            # y_train ve y_test'i burada ayƒ±rƒ±yoruz
            y_train = y.loc[train_indices] # SMOTE i√ßin orijinal y_train
            y_test = y.loc[test_indices]

            X_train_raw = X.loc[train_indices]
            X_test_raw = X.loc[test_indices]

            print(f"  Train: {len(X_train_raw):,} samples")
            print(f"  Test:  {len(X_test_raw):,} samples")

            if X_train_raw.empty or X_test_raw.empty:
                raise ValueError("Empty train/test set!")

        except Exception as e_split:
            print(f"‚ùå Split error: {e_split}")
            sys.exit(1)

        # -----------------------------------------------------------------
        # ADIM 3.5: EDA-GUIDED FEATURE PREPARATION (SENKRONƒ∞ZASYON D√úZELTƒ∞LDƒ∞)
        # -----------------------------------------------------------------
        print("\n3Ô∏è‚É£.5Ô∏è‚É£ EDA-GUIDED FEATURE PREPARATION...\n")

        # ============================================================================
        # üö® EXPERIMENT: "KRAL √áIPLAK" (REMOVE ODDS & DERIVED METRICS)
        # Bahis oranlarƒ±nƒ± ve t√ºrevlerini tamamen kaldƒ±rarak modeli "Saf Futbol" √∂ƒürenmeye zorluyoruz.
        # ============================================================================
        #FEATURES_TO_REMOVE_TOTALLY = [
            # --- TEMEL BAHƒ∞S ORANLARI (Bet365) ---
            #'OddHome', 'OddDraw', 'OddAway',
            #'Prob_H_Norm', 'Prob_D_Norm', 'Prob_A_Norm',
            #'Prob_H_Raw', 'Prob_D_Raw', 'Prob_A_Raw',

            # --- Dƒ∞ƒûER BAHƒ∞S ≈ûƒ∞RKETLERƒ∞ (MAX ORANLAR) ---
            # Model Bet365'i g√∂remese bile bunlardan kopya √ßekebilir
            #'MaxHome', 'MaxDraw', 'MaxAway',

            # --- GOL BAHƒ∞SLERƒ∞ (ALT/√úST) ---
            # Gol beklentisi, taraf bahsi i√ßin g√º√ßl√º bir sinyaldir
            #'Over25', 'Under25',
            #'MaxOver25', 'MaxUnder25',

            # --- HANDƒ∞KAP Bƒ∞LGƒ∞LERƒ∞ (KRƒ∞Tƒ∞K Sƒ∞NYAL) ---
            # Handikap boyutu (HandiSize) favoriyi direkt ele verir
            #'HandiSize', 'HandiHome', 'HandiAway',

            # --- ELO PUANLARI (OPSƒ∞YONEL) ---
            # Eƒüer sadece "O anki saha i√ßi/dƒ±≈üƒ± istatistikleri" g√∂rmek istersen bunlarƒ± da a√ßabilirsin.
            # ≈ûimdilik kapalƒ± tutuyorum (ELO kalsƒ±n), √ß√ºnk√º bu ge√ßmi≈ü performanstƒ±r.
            # 'HomeElo', 'AwayElo', 'ELO_Diff'
        #]

        #print(f"‚ö†Ô∏è [EXPERIMENT] Removing {len(FEATURES_TO_REMOVE_TOTALLY)} betting features to force raw stats learning...")

        # Listede olup dataframe'de olmayanlar hata vermesin diye kontrol
        #cols_to_drop_odds = [c for c in FEATURES_TO_REMOVE_TOTALLY if c in X_train_raw.columns]

        #if cols_to_drop_odds:
            #X_train_raw = X_train_raw.drop(columns=cols_to_drop_odds)
            #X_test_raw = X_test_raw.drop(columns=cols_to_drop_odds)
            #print(f"   ‚úÖ Dropped odds columns ({len(cols_to_drop_odds)}): {cols_to_drop_odds}")
        #else:
            #print("   ‚ÑπÔ∏è Columns already removed or not found.")

        # -----------------------------------------------------------------
        # EDA STRATEGY (DIFF-ONLY)
        # -----------------------------------------------------------------
        eda_config = CONFIG.get("eda_integration", {})

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ FIX: cols_to_drop_odds tanƒ±mlƒ± deƒüilse bo≈ü liste olarak tanƒ±mla
        # (EXPERIMENT bloƒüu comment out edildiƒüinde bu deƒüi≈üken tanƒ±msƒ±z kalƒ±yor)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if 'cols_to_drop_odds' not in dir():
            cols_to_drop_odds = []
            print("  ‚ÑπÔ∏è  EXPERIMENT disabled - cols_to_drop_odds set to empty list")

        if eda_config.get("enabled", False) and eda_config.get("strategy") == "diff_only":
            print("\n[EDA-GUIDED] Applying Diff-Only Strategy...")

            if 'features_to_drop_multicollinearity_REVISED' in globals():
                # Odds dƒ±≈üƒ±ndaki diƒüer drop edilecekleri belirle (zaten silinenleri tekrar deneme)
                other_drops = [f for f in features_to_drop_multicollinearity_REVISED
                             if f not in cols_to_drop_odds]

                print(f"  ‚Üí Dropping additional {len(other_drops)} features from EDA list")

                # Hata vermemesi i√ßin 'errors=ignore' kullanƒ±yoruz
                X_train_raw = X_train_raw.drop(columns=other_drops, errors='ignore')
                X_test_raw = X_test_raw.drop(columns=other_drops, errors='ignore')

                print(f"  ‚úÖ After Diff-Only: {X_train_raw.shape[1]} features")
            else:
                print(f"  ‚ö†Ô∏è EDA drop list not loaded")
        else:
            print("  ‚ö†Ô∏è EDA integration disabled or strategy not 'diff_only'.")

        # ============================================================================
        # üö® KRƒ∞Tƒ∞K D√úZELTME: SENKRONƒ∞ZASYON (SYNCHRONIZATION)
        # ============================================================================
        # T√ºm silme i≈ülemleri bitti. ≈ûimdi features_final listesini
        # elimizde kalan ger√ßek s√ºtunlarla g√ºncelliyoruz.
        # ============================================================================
        features_final = X_train_raw.columns.tolist()
        print(f"\n‚úÖ Final feature count (Synced): {len(features_final)}")
        print(f"‚úÖ DataFrame shape: {X_train_raw.shape}\n")

        # -----------------------------------------------------------------
        # MI RECALCULATION (Artƒ±k senkronize olduƒüu i√ßin hata vermez)
        # -----------------------------------------------------------------
        print("\n[SUB-STEP 1.6] üîÑ Recalculating MI scores after feature removal...")
        try:
            mi_scores_after = mutual_info_classif(
                X_train_raw,
                y_train,
                random_state=SEED,
                n_neighbors=3
            )

            # Artƒ±k features_final ve mi_scores_after aynƒ± uzunlukta olmak ZORUNDA
            mi_df_after = pd.DataFrame({
                'feature': features_final,
                'mi_score': mi_scores_after
            }).sort_values('mi_score', ascending=False)

            # ƒ∞statistiksel √∂zet
            print(f"\n  üìä MI Score Statistics (After Drop):")
            print(f"     Mean:   {mi_scores_after.mean():.6f}")
            print(f"     Max:    {mi_scores_after.max():.6f}")

            # Top 10 feature'larƒ± g√∂ster
            print(f"\n  üèÜ Top 10 Features by MI (After Drop):")
            print("  " + "-" * 60)
            for idx, row in mi_df_after.head(10).iterrows():
                print(f"     {row['feature']:40s} ‚Üí MI = {row['mi_score']:.6f}")
            print("  " + "-" * 60)

            # D√º≈ü√ºk MI olanlarƒ± silme bloƒüu (Opsiyonel)
            LOW_MI_THRESHOLD = CONFIG.get('smart_feature_selection', {}).get('mi_threshold', 0.001)
            low_mi_features = mi_df_after[mi_df_after['mi_score'] < LOW_MI_THRESHOLD]['feature'].tolist()

            if low_mi_features:
                print(f"\n  üóëÔ∏è AUTO-DROP: {len(low_mi_features)} features with MI < {LOW_MI_THRESHOLD}")
                X_train_raw = X_train_raw.drop(columns=low_mi_features)
                X_test_raw = X_test_raw.drop(columns=low_mi_features)

                # Listeyi tekrar g√ºncelle
                features_final = [f for f in features_final if f not in low_mi_features]
                print(f"  ‚úÖ Final features after MI drop: {len(features_final)}")
            else:
                print(f"  ‚úÖ No features dropped by MI threshold")

        except Exception as e_mi:
            print(f"  ‚ùå MI recalculation failed: {e_mi}")
            # Hata olsa bile devam et, features_final yukarƒ±da g√ºncellendiƒüi i√ßin kod kƒ±rƒ±lmaz

            # ================================================================
            # CSV'ye kaydet (g√ºncel MI skorlarƒ±)
            # ================================================================
            mi_output_path = os.path.join(DATA_DIR, 'mi_scores_after_drop.csv')
            mi_df_after.to_csv(mi_output_path, index=False)
            print(f"\n  ‚úÖ MI scores saved: {os.path.basename(mi_output_path)}")

            # -----------------------------------------------------------------
            # BA≈ûLANGI√á: MI Plot Kodlarƒ± (Tam Hali)
            # -----------------------------------------------------------------
            print(f"\n  üìä Creating MI distribution plot...")
            fig, axes = plt.subplots(1, 2, figsize=(14, 5))

            # Subplot 1: Bar chart (Top 20)
            ax1 = axes[0]
            top_20 = mi_df_after.head(20)
            bars = ax1.barh(range(len(top_20)), top_20['mi_score'], color='steelblue', edgecolor='black')
            ax1.set_yticks(range(len(top_20)))
            ax1.set_yticklabels(top_20['feature'], fontsize=12)
            ax1.set_xlabel('MI Score', fontsize=12, fontweight='bold')
            ax1.set_title('Top 20 Features by Mutual Information', fontsize=12, fontweight='bold')
            ax1.grid(axis='x', alpha=0.3, linestyle='--')
            ax1.invert_yaxis()

            # Deƒüerleri bar'larƒ±n √ºzerine ekle
            for i, bar in enumerate(bars):
                width = bar.get_width()
                ax1.text(width, bar.get_y() + bar.get_height()/2.,
                         f'{width:.4f}', ha='left', va='center', fontsize=12)

            # Subplot 2: Histogram
            ax2 = axes[1]
            ax2.hist(mi_scores_after, bins=30, color='coral', edgecolor='black', alpha=0.7)
            ax2.axvline(mi_scores_after.mean(), color='red', linestyle='--',
                        linewidth=2, label=f'Mean = {mi_scores_after.mean():.4f}')
            ax2.axvline(np.median(mi_scores_after), color='green', linestyle='--',
                        linewidth=2, label=f'Median = {np.median(mi_scores_after):.4f}')
            ax2.set_xlabel('MI Score', fontsize=12, fontweight='bold')
            ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')
            ax2.set_title('Distribution of MI Scores', fontsize=12, fontweight='bold')
            ax2.legend(fontsize=12)
            ax2.grid(axis='y', alpha=0.3, linestyle='--')

            plt.tight_layout()
            mi_plot_path = os.path.join(GRAPHICS_DIR, '30_mi_scores_after_drop.png')
            plt.savefig(mi_plot_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"  ‚úÖ Plot saved: {os.path.basename(mi_plot_path)}\n")
            # -----------------------------------------------------------------
            # Bƒ∞Tƒ∞≈û: MI Plot Kodlarƒ± (Tam Hali)
            # -----------------------------------------------------------------

        except Exception as e_mi:
            print(f"  ‚ùå MI recalculation failed: {e_mi}")
            import traceback
            traceback.print_exc()

        # (Leakage check ve Log transform)
        print("\n[SUB-STEP 3] Leakage check...")
        leakage_check = ['FTHome', 'FTAway', 'HTHome', 'HTAway']
        leakage_found = [f for f in leakage_check if f in X_train_raw.columns]
        if leakage_found:
            raise ValueError(f"‚ùå LEAKAGE: {leakage_found}")
        print("     ‚úì No leakage")

        print("\n[SUB-STEP 4] Log transform skewed features...")
        skewed = [f for f in ['HomeTeam_ClubValue', 'AwayTeam_ClubValue']
                  if f in X_train_raw.columns]
        for feat in skewed:
            X_train_raw[f'{feat}_log'] = np.log1p(X_train_raw[feat])
            X_test_raw[f'{feat}_log'] = np.log1p(X_test_raw[feat])
            X_train_raw = X_train_raw.drop(columns=[feat])
            X_test_raw = X_test_raw.drop(columns=[feat])
            if feat in features_final:
                features_final.remove(feat)
                features_final.append(f'{feat}_log')
        print(f"     ‚úì Transformed {len(skewed)} features")
        print(f"     ‚úì Final features: {len(features_final)}")

        # -----------------------------------------------------------------
        # ADIM 3.6: CLASS BALANCING (D√úZELTME: PRE-SMOTE SNAPSHOT EKLEME)
        # -----------------------------------------------------------------
        print("\n3Ô∏è‚É£.6Ô∏è‚É£ CLASS BALANCING (Single Scenario Mode)...\n")

        class_balancing_config = CONFIG.get("class_balancing", {})
        USE_SMOTE = class_balancing_config.get("use_smote", False)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ YENƒ∞ EKLEME: OUTPUT_SUFFIX TANIMLAMA
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if USE_SMOTE:
            output_suffix = "_WITH_SMOTE"
            print(f"    üìä Scenario: WITH SMOTE")
            print(f"       Strategy: {class_balancing_config.get('strategy', 'auto')}")
        else:
            output_suffix = "_NO_SMOTE"
            print(f"    üìä Scenario: NO SMOTE")
            print(f"       Models will train on original class distribution")
        print()
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üÜï ADIM 3.7: VALIDATION SPLIT (PRE-SMOTE - EARLY STOPPING ƒ∞√áƒ∞N)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print("\n3Ô∏è‚É£.7Ô∏è‚É£ VALIDATION SPLIT (Pre-SMOTE - For Early Stopping)...\n")

        early_stop_config = CONFIG.get('early_stopping', {})
        USE_EARLY_STOPPING = early_stop_config.get('enabled', False)

        if USE_EARLY_STOPPING:
            print(f"  üéØ Early Stopping: ENABLED")
            print(f"     Validation split: {early_stop_config.get('val_split', 0.15)*100:.0f}%")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ TEMPORAL SPLIT (Lag features i√ßin g√ºvenli)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            print(f"     Split method: TEMPORAL (last {early_stop_config.get('val_split', 0.15)*100:.0f}% of train data)")

            n_train_total = len(X_train_raw)
            n_val = int(n_train_total * early_stop_config.get('val_split', 0.15))
            split_idx = n_train_total - n_val

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # SPLIT: First 85% = Train, Last 15% = Validation
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            X_val = X_train_raw.iloc[split_idx:].copy()
            y_val = y_train.iloc[split_idx:].copy()

            X_train_raw = X_train_raw.iloc[:split_idx].copy()
            y_train = y_train.iloc[:split_idx].copy()

            print(f"\n  ‚úÖ Temporal split successful:")
            print(f"     ‚Ä¢ Train samples:      {len(X_train_raw):,}")
            print(f"     ‚Ä¢ Validation samples: {len(X_val):,}")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # TEMPORAL VERIFICATION (Optional but recommended)
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            try:
                if 'MatchDate' in df.columns:
                    train_indices = X_train_raw.index
                    val_indices = X_val.index

                    train_dates = df.loc[train_indices, 'MatchDate']
                    val_dates = df.loc[val_indices, 'MatchDate']

                    print(f"\n  üìÖ Temporal boundaries:")
                    print(f"     ‚Ä¢ Train period:      {train_dates.min().strftime('%Y-%m-%d')} ‚Üí "
                          f"{train_dates.max().strftime('%Y-%m-%d')}")
                    print(f"     ‚Ä¢ Validation period: {val_dates.min().strftime('%Y-%m-%d')} ‚Üí "
                          f"{val_dates.max().strftime('%Y-%m-%d')}")

                    # Critical check: No overlap
                    if train_dates.max() < val_dates.min():
                        print(f"     ‚úÖ No temporal overlap (lag features safe!)")
                    else:
                        overlap_days = (train_dates.max() - val_dates.min()).days
                        print(f"     ‚ö†Ô∏è  WARNING: {overlap_days} days overlap detected!")
                        print(f"        This may cause leakage with lag features")
            except Exception as e_date:
                print(f"  ‚ÑπÔ∏è  Could not verify temporal boundaries: {e_date}")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # CLASS DISTRIBUTION (Informational)
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            train_counts = y_train.value_counts().sort_index()
            val_counts = y_val.value_counts().sort_index()

            print(f"\n  üìä Class distribution (after temporal split):")
            print(f"     Train:")
            print(f"       ‚Ä¢ Away Win (0): {train_counts.get(0, 0):,} ({train_counts.get(0, 0)/len(y_train)*100:.1f}%)")
            print(f"       ‚Ä¢ Draw (1):     {train_counts.get(1, 0):,} ({train_counts.get(1, 0)/len(y_train)*100:.1f}%)")
            print(f"       ‚Ä¢ Home Win (2): {train_counts.get(2, 0):,} ({train_counts.get(2, 0)/len(y_train)*100:.1f}%)")

            print(f"     Validation:")
            print(f"       ‚Ä¢ Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)")
            print(f"       ‚Ä¢ Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)")
            print(f"       ‚Ä¢ Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # IMBALANCE WARNING (Class imbalance may be worse with temporal split)
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            val_max = val_counts.max()
            val_min = val_counts.min()
            val_imbalance_ratio = val_max / val_min if val_min > 0 else float('inf')

            if val_imbalance_ratio > 2.0:
                print(f"\n  ‚ö†Ô∏è  Validation set imbalance: {val_imbalance_ratio:.1f}:1")
                print(f"      (This is normal with temporal split - won't affect training)")

            print(f"\n  ‚úÖ Validation set characteristics:")
            print(f"     ‚Ä¢ PRE-SMOTE (contains ZERO synthetic samples)")
            print(f"     ‚Ä¢ Temporally AFTER all training data")
            print(f"     ‚Ä¢ Safe for lag features (no data leakage)")

        else:
            print(f"  ‚ÑπÔ∏è  Early Stopping: DISABLED")
            print(f"     Validation split skipped - models will train without early stopping")
            X_val = None
            y_val = None

        print(f"\n" + "="*100 + "\n")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ADIM 4: SCALING PIPELINE (SMOTE deferred to CV)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print("\n4Ô∏è‚É£ SCALING PIPELINE (SMOTE will be applied in CV)...\n")

        print("="*80)
        print("üìä PIPELINE: SCALE ONLY (SMOTE deferred to CV)")
        print("="*80 + "\n")
        print("  ‚ÑπÔ∏è  SMOTE will be applied inside ImbPipeline during cross-validation")
        print("  ‚ÑπÔ∏è  This ensures validation folds never see synthetic samples\n")

        scaler = RobustScaler()

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 1: FIT SCALER ON ORIGINAL DATA
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print("[STEP 1/3] Fitting scaler on original train data...")
        print(f"  üìä Train samples: {len(X_train_raw):,}")
        print(f"  üìä Features: {X_train_raw.shape[1]}")

        scaler.fit(X_train_raw)

        # Debug: Show learned parameters (sample feature)
        sample_feature = 'ClubValue_Diff' if 'ClubValue_Diff' in features_final else features_final[0]
        feature_idx = features_final.index(sample_feature)

        print(f"\n  üìå Scaler parameters (sample: {sample_feature}):")
        print(f"     ‚Ä¢ Center (median): {scaler.center_[feature_idx]:,.2f}")
        print(f"     ‚Ä¢ Scale (IQR):     {scaler.scale_[feature_idx]:,.2f}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 2: TRANSFORM ALL DATASETS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print(f"\n[STEP 2/3] Scaling all datasets...")

        # Train
        X_train_scaled = scaler.transform(X_train_raw)
        print(f"  ‚úÖ Train scaled: {X_train_scaled.shape}")

        # Test
        X_test_scaled = scaler.transform(X_test_raw)
        print(f"  ‚úÖ Test scaled: {X_test_scaled.shape}")

        # Validation (if exists)
        if X_val is not None:
            X_val_scaled = scaler.transform(X_val)
            print(f"  ‚úÖ Validation scaled: {X_val_scaled.shape}")
        else:
            X_val_scaled = None
            print(f"  ‚ÑπÔ∏è  No validation set")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # STEP 3: CONVERT TO DATAFRAME (NO SMOTE!)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print(f"\n[STEP 3/3] Converting to DataFrames...")

        # ‚úÖ √á√ñZ√úM: SMOTE uygulanmamƒ±≈ü veri!
        X_train = pd.DataFrame(X_train_scaled, columns=features_final)
        y_train = pd.Series(y_train, name='FTResult_Encoded')  # Original labels

        X_test = pd.DataFrame(X_test_scaled, columns=features_final, index=X_test_raw.index)

        if X_val_scaled is not None:
            X_val = pd.DataFrame(X_val_scaled, columns=features_final)
        else:
            X_val = None
            y_val = None

        print(f"  ‚úÖ Train: {X_train.shape} (PRE-SMOTE)")
        print(f"  ‚úÖ Test:  {X_test.shape}")
        if X_val is not None:
            print(f"  ‚úÖ Validation: {X_val.shape} (PRE-SMOTE)")

        # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # # SAVE SCALER & METADATA
        # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # print(f"\n[SAVE] Persisting scaler and metadata...")

        # output_suffix = "_WITH_IMBALANCED_PIPELINE"

        # scaler_path = os.path.join(OUT_DIR, f"robust_scaler{output_suffix}.pkl")
        # joblib.dump(scaler, scaler_path)
        # print(f"  ‚úÖ Scaler saved: {os.path.basename(scaler_path)}")

        # # Metadata
        # scaling_metadata = {
        #     'pipeline': 'SCALE ‚Üí (SMOTE deferred to CV Pipeline)',
        #     'smote_applied': 'Inside ImbPipeline during CV',
        #     'smote_strategy': CONFIG.get("class_balancing", {}).get("strategy", "auto"),
        #     'train_samples': len(X_train_raw),
        #     'test_samples': len(X_test_raw),
        #     'scaler_type': 'RobustScaler',
        #     'note': 'SMOTE is NOT applied here - will be applied per CV fold'
        # }

        # metadata_path = os.path.join(OUT_DIR, f"scaling_metadata{output_suffix}.json")
        # with open(metadata_path, 'w') as f:
        #     json.dump(scaling_metadata, f, indent=2)
        # print(f"  ‚úÖ Metadata saved: {os.path.basename(metadata_path)}")

        # print("\n" + "="*80)
        # print(f"‚úÖ SCALING COMPLETE (SMOTE deferred to CV)")
        # print("="*80 + "\n")

        print("[INFO] Class distribution (PRE-SMOTE):")
        y_counts_pre = y_train.value_counts().sort_index()
        print(f"  ‚Ä¢ Away Win (0): {y_counts_pre.get(0, 0):,} ({y_counts_pre.get(0, 0)/len(y_train)*100:.1f}%)")
        print(f"  ‚Ä¢ Draw (1):     {y_counts_pre.get(1, 0):,} ({y_counts_pre.get(1, 0)/len(y_train)*100:.1f}%)")
        print(f"  ‚Ä¢ Home Win (2): {y_counts_pre.get(2, 0):,} ({y_counts_pre.get(2, 0)/len(y_train)*100:.1f}%)")
        print(f"\n  ‚ÑπÔ∏è  SMOTE will balance this inside each CV fold\n")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üîç CRITICAL VERIFICATION (FOR DEBUGGING)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        print("[VERIFICATION] Sanity checks...")

        # 1. Check for NaN/Inf
        if np.isnan(X_train.values).any() or np.isinf(X_train.values).any():
            print("  ‚ùå WARNING: NaN or Inf detected in X_train!")
            nan_cols = [features_final[i] for i in range(X_train.shape[1])
                        if np.isnan(X_train.values[:, i]).any()]
            print(f"     Problematic features: {nan_cols}")
        else:
            print("  ‚úÖ No NaN/Inf in train data")

        # 2. Check scaling effectiveness
        sample_feat_idx = 0
        train_mean = X_train.iloc[:, sample_feat_idx].mean()
        train_std = X_train.iloc[:, sample_feat_idx].std()
        print(f"  ‚úÖ Sample feature (scaled): mean={train_mean:.4f}, std={train_std:.4f}")

        # 3. Check class balance (if SMOTE)
        if USE_SMOTE:
            y_counts = y_train.value_counts()
            balance_ratio = y_counts.max() / y_counts.min()
            print(f"  ‚úÖ Class balance ratio: {balance_ratio:.2f}:1 (lower is better)")

            if balance_ratio > 3.0:
                print(f"     ‚ö†Ô∏è  Still highly imbalanced (>3:1)")
            else:
                print(f"     ‚úÖ Reasonably balanced (<3:1)")

        print()

        # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # # CONVERT TO DATAFRAME (WITH PROPER INDEXING)
        # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # print(f"\n[FINALIZATION] Converting to DataFrames...")

        # # Train DataFrame (index may be reset if SMOTE applied)
        # X_train = pd.DataFrame(
        #     X_train_final,
        #     columns=features_final
        # )

        # # y_train as Series (matching X_train index)
        # y_train = pd.Series(
        #     y_train_final,
        #     name='FTResult_Encoded'
        # )

        # # Test DataFrame (preserve original index)
        # X_test = pd.DataFrame(
        #     X_test_scaled,
        #     columns=features_final,
        #     index=X_test_raw.index  # ‚úÖ Keep original temporal index
        # )

        # # ‚úÖ YENƒ∞ EKLEME: Validation DataFrame (if exists)
        # if X_val_scaled is not None:
        #     X_val = pd.DataFrame(
        #         X_val_scaled,
        #         columns=features_final
        #     )
        #     print(f"  ‚úÖ Validation converted: {X_val.shape}")
        #     print(f"  ‚úÖ Validation is SCALED (ready for training)")
        # else:
        #     X_val = None
        #     y_val = None  # Validation yoksa y_val'i de None yap
        #     print(f"  ‚ÑπÔ∏è  No validation set")

        # print(f"  ‚úÖ Final train: {X_train.shape}")
        # print(f"  ‚úÖ Final test:  {X_test.shape}")
        # print(f"  ‚úÖ Output suffix: {output_suffix}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üîç PRE-TRAINING VALIDATION TEST  Kodun doƒüru √ßalƒ±≈ütƒ±ƒüƒ±nƒ± test etmek i√ßin  Bu testler ba≈üarƒ±sƒ±z olursa, scaling pipeline'da bir sorun var demektir.
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if X_val is not None and y_val is not None:
            print("\n[VALIDATION TEST]")

            # Test 1: Shape match
            assert X_val.shape[0] == len(y_val), "X_val and y_val length mismatch!"
            print(f"  ‚úÖ Shape match: {X_val.shape[0]} == {len(y_val)}")

            # Test 2: Scaling check
            first_feature_mean = X_val.iloc[:, 0].mean()
            assert abs(first_feature_mean) < 0.5, f"X_val not scaled! Mean = {first_feature_mean}"
            print(f"  ‚úÖ Scaled correctly: mean = {first_feature_mean:.4f} (close to 0)")

            # Test 3: No NaN
            assert not X_val.isnull().any().any(), "X_val contains NaN!"
            print(f"  ‚úÖ No NaN values")

            # Test 4: Column match
            assert list(X_val.columns) == features_final, "X_val columns mismatch!"
            print(f"  ‚úÖ Columns match: {len(features_final)} features")

            print(f"  ‚úÖ All validation tests passed!\n")
        else:
            print(f"\n[VALIDATION TEST] Skipped (X_val is None)\n")

        # -----------------------------------------------------------------
        # ADIM 5: TRAIN MODELS (BASƒ∞TLE≈ûTƒ∞Rƒ∞LDƒ∞)
        # -----------------------------------------------------------------
        print("\n5Ô∏è‚É£ TRAINING MODELS...\n")

        # ‚úÖ CHECKPOINT: Validation durumunu g√∂ster
        print("="*80)
        print("üìä VALIDATION STATUS BEFORE TRAINING")
        print("="*80)
        if X_val is not None:
            print(f"‚úÖ X_val: {X_val.shape} (SCALED)")
            print(f"‚úÖ y_val: {y_val.shape if hasattr(y_val, 'shape') else len(y_val)}")
            print(f"‚úÖ Sample scaled value: {X_val.iloc[0, 0]:.4f} (should be ~0)")

            # ‚úÖ Class distribution check
            val_counts = y_val.value_counts().sort_index()
            print(f"\nüìä Validation class distribution:")
            print(f"   ‚Ä¢ Away Win (0): {val_counts.get(0, 0):,} ({val_counts.get(0, 0)/len(y_val)*100:.1f}%)")
            print(f"   ‚Ä¢ Draw (1):     {val_counts.get(1, 0):,} ({val_counts.get(1, 0)/len(y_val)*100:.1f}%)")
            print(f"   ‚Ä¢ Home Win (2): {val_counts.get(2, 0):,} ({val_counts.get(2, 0)/len(y_val)*100:.1f}%)")
        else:
            print(f"‚ö†Ô∏è  X_val is None (early stopping disabled)")
            print(f"   Models will train without early stopping")
        print("="*80 + "\n")

        # ‚úÖ DEƒûƒ∞≈ûƒ∞KLƒ∞K: 4 deƒüer return ediliyor (validation_data eklendi)
        results, models_dict, xai_results, validation_data = train_models(
            X_train, X_test, y_train, y_test,
            X_val=X_val,      # ‚úÖ ARTIK SCALED!
            y_val=y_val,
            features=features_final,
            seed=SEED,
            CONFIG=CONFIG,
            OUT_DIR=OUT_DIR,
            N_CLASSES=N_CLASSES
        )

        # ‚úÖ YENƒ∞: Validation set'i unpack et
        X_val_returned, y_val_returned = validation_data

        # ‚úÖ YENƒ∞: Validation set bilgisini yazdƒ±r
        print("\n" + "="*80)
        print("üìä POST-TRAINING VALIDATION STATUS")
        print("="*80)
        if X_val_returned is not None:
            print(f"‚úÖ Validation set received: {len(X_val_returned):,} samples")
            print(f"   ‚Ä¢ Contains NO synthetic samples (correct!)")
            print(f"   ‚Ä¢ Was used for early stopping (XGBoost, LightGBM, CatBoost)")
            print(f"   ‚Ä¢ Will be used for ablation analysis (G28, G29)")
        else:
            print(f"‚ö†Ô∏è  No validation set (early stopping disabled)")
            print(f"   ‚Ä¢ Models trained without early stopping")
            print(f"   ‚Ä¢ Ablation will use test set (less ideal)")
        print("="*80)

        print(f"\n‚úÖ {output_suffix} training complete: {len(results)} models\n")

        # -----------------------------------------------------------------
        # ADIM 6: CREATE TABLES
        # -----------------------------------------------------------------
        print("\n6Ô∏è‚É£ CREATING TABLES...\n")
        tables = create_dynamic_tables(results)
        table_png_paths = save_tables_as_png(tables, TABLES_DIR)

        # -----------------------------------------------------------------
        # ADIM 7: CREATE GRAPHICS
        # -----------------------------------------------------------------
        print("\n7Ô∏è‚É£ CREATING GRAPHICS...\n")

        # ‚úÖ CHECKPOINT: Graphics'e g√∂nderilecek validation'ƒ± kontrol et
        print("="*80)
        print("üìä VALIDATION STATUS FOR GRAPHICS")
        print("="*80)
        if X_val_returned is not None:
            print(f"‚úÖ Will send validation to graphics:")
            print(f"   ‚Ä¢ X_val shape: {X_val_returned.shape}")
            print(f"   ‚Ä¢ y_val length: {len(y_val_returned)}")
            print(f"   ‚Ä¢ For use in: G28 (XAI comparison), G29 (Cumulative importance)")
        else:
            print(f"‚ö†Ô∏è  No validation - graphics will use test set")
        print("="*80 + "\n")

        # ‚úÖ DEƒûƒ∞≈ûƒ∞KLƒ∞K: X_val ve y_val parametreleri eklendi
        graphics = create_dynamic_graphics(
            results=results,
            xai_results=xai_results,
            output_dir=GRAPHICS_DIR,
            features=features_final,
            models_dict=models_dict,
            y_test=y_test,
            X_test=X_test,
            X_train=X_train,
            y_train=y_train,
            X_val=X_val_returned,    # ‚úÖ RETURNED deƒüeri kullan (train_models'dan gelen)
            y_val=y_val_returned     # ‚úÖ RETURNED deƒüeri kullan
        )

        # Grafik listesi None ise kurtarma
        if graphics is None:
            print("  ‚ö†Ô∏è Graphics creation failed, collecting existing files...")
            graphics = []
            if os.path.exists(GRAPHICS_DIR):
                graphics = [os.path.join(GRAPHICS_DIR, f) for f in sorted(os.listdir(GRAPHICS_DIR))
                            if f.endswith('.png')]
                print(f"  ‚úì Found {len(graphics)} graphics in {GRAPHICS_DIR}")
            else:
                print(f"  ‚ùå Graphics directory not found: {GRAPHICS_DIR}")

        print(f"\n‚úÖ Total graphics created: {len(graphics)}")

        # -----------------------------------------------------------------
        # ADIM 8: CREATE PDF
        # -----------------------------------------------------------------
        print("\n8Ô∏è‚É£ CREATING PDF...\n")
        pdf_path = create_pdf_report(OUT_DIR, graphics, tables)

        # D√úZELTME: PDF'i senaryoya g√∂re yeniden adlandƒ±r
        if pdf_path:
            new_pdf_path = os.path.join(OUT_DIR, f"Football_Prediction_v18{output_suffix}.pdf")
            try:
                os.rename(pdf_path, new_pdf_path)
                print(f"‚úÖ PDF Report Renamed: {os.path.basename(new_pdf_path)}")
            except Exception as e_rename:
                print(f"‚ö†Ô∏è  Could not rename PDF: {e_rename}. Original path: {pdf_path}")
                new_pdf_path = pdf_path # Yeni yolu eski yola e≈üitle
        else:
             new_pdf_path = None # PDF olu≈üturulamadƒ±ysa

        # -----------------------------------------------------------------
        # ADIM 9: SAVE RESULTS
        # -----------------------------------------------------------------
        print("\n9Ô∏è‚É£ SAVING RESULTS...\n")
        # D√úZELTME: Sonu√ßlarƒ± da son ek ile kaydet
        json_path = save_results_safely(results, OUT_DIR)
        if json_path:
            new_json_path = os.path.join(OUT_DIR, f"results{output_suffix}.json")
            try:
                os.rename(json_path, new_json_path)
                print(f"‚úÖ Results Renamed: {os.path.basename(new_json_path)}")
            except Exception as e_rename_json:
                print(f"‚ö†Ô∏è  Could not rename JSON: {e_rename_json}. Original path: {json_path}")
                new_json_path = json_path # Yeni yolu eski yola e≈üitle
        else:
            new_json_path = None # JSON olu≈üturulamadƒ±ysa

        # -----------------------------------------------------------------
        # FINAL SUMMARY
        # -----------------------------------------------------------------
        print("\n" + "="*100)
        print(f"‚ú® v18.0 COMPLETE - SCENARIO: {output_suffix}")
        print("="*100 + "\n")

        print("üìä EXECUTION SUMMARY:\n")
        print(f"‚úÖ Data: {len(df):,} matches")

        matched_c = sum(1 for v in data_integration.team_mapping.values() if v is not None)
        total_c = len(data_integration.team_mapping)
        match_rate = (matched_c / total_c * 100) if total_c > 0 else 0
        print(f"‚úÖ Team Mapping: {matched_c}/{total_c} ({match_rate:.1f}%)")

        print(f"\nüìà FEATURE STATISTICS:")
        print(f"  ‚Ä¢ Initial features (pre-lag): {len(features)}")
        print(f"  ‚Ä¢ Final features (post-MI): {len(features_final)}")
        print(f"  ‚Ä¢ Reduction: {len(features) - len(features_final)} ({(1 - len(features_final)/len(features))*100:.1f}%)")

        print(f"\nüéØ MODEL PERFORMANCE:")
        valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys() if pd.notna(results[m].get('test_f1'))}
        if valid_models_f1:
            best_model_name = max(valid_models_f1, key=valid_models_f1.get)
            print(f"  ‚úÖ Best Model: {best_model_name} (F1 = {valid_models_f1[best_model_name]:.4f})")

        print(f"\nüìÅ OUTPUTS:")
        print(f"  ‚úÖ Directory: {OUT_DIR}")
        print(f"  ‚úÖ Graphics: {len(graphics)}")
        print(f"  ‚úÖ Tables: {len(tables)}")
        if new_pdf_path:
            print(f"  ‚úÖ PDF Report: {os.path.basename(new_pdf_path)}")
        if new_json_path:
            print(f"  ‚úÖ Results JSON: {os.path.basename(new_json_path)}")

        end_time = time.time()
        total_time = end_time - start_time
        print(f"\n‚è±Ô∏è  Total Execution Time ({output_suffix}):")
        print(f"   {total_time:.2f}s ({total_time/60:.2f} minutes)")

        print("\n" + "="*100)
        print(f"‚úÖ v18.0 EXECUTION COMPLETE (SCENARIO: {output_suffix})")
        print("="*100)

    except Exception as e:
        print(f"\n‚ùå EXECUTION FAILED!")
        print(f"Error Type: {type(e).__name__}")
        print(f"Error Message: {e}\n")

        import traceback
        print("="*100)
        print("FULL ERROR TRACEBACK:")
        print("="*100)
        traceback.print_exc()

        print("\n" + "="*100)
        print("‚ùå v18.0 EXECUTION INCOMPLETE - ERROR OCCURRED")
        print("="*100)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Data Statistics
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        print(f"‚úÖ Data: {len(df):,} matches")

        matched_c = sum(1 for v in data_integration.team_mapping.values() if v is not None)
        total_c = len(data_integration.team_mapping)
        match_rate = (matched_c / total_c * 100) if total_c > 0 else 0
        print(f"‚úÖ Team Mapping: {matched_c}/{total_c} ({match_rate:.1f}%)")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üÜï EDA-Guided Improvements
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        eda_config = CONFIG.get("eda_integration", {})

        if eda_config.get("enabled", False):
            print(f"\nüìã EDA-GUIDED IMPROVEMENTS:")

            if eda_config.get("strategy") == "diff_only":
                print(f"  ‚úÖ Diff-Only Strategy applied")

                if 'features_to_drop_multicollinearity_REVISED' in globals():
                    print(f"  ‚úÖ Features dropped: {len(features_to_drop_multicollinearity_REVISED)}")

                print(f"  ‚úÖ VIF reduction: High ‚Üí <10 (expected)")

            if 'mi_scores_eda_df' in locals() and mi_scores_eda_df is not None:
                print(f"  ‚úÖ MI scores loaded: {len(mi_scores_eda_df)} features")

            print(f"\nüìã EDA FILES USED:")
            print(f"  ‚Ä¢ Base path: {eda_config.get('base_path', 'N/A')}")
            print(f"  ‚Ä¢ Drop list: {os.path.basename(eda_config.get('drop_list_file', 'N/A'))}")
            print(f"  ‚Ä¢ MI scores: {os.path.basename(eda_config.get('mi_scores_file', 'N/A'))}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Feature Statistics
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        print(f"\nüìà FEATURE STATISTICS:")

        if 'features' in locals() and 'features_final' in locals():
            print(f"  ‚Ä¢ Initial features: {len(features)}")
            print(f"  ‚Ä¢ After processing: {len(features_final)}")
            print(f"  ‚Ä¢ Reduction: {len(features) - len(features_final)} ({(1 - len(features_final)/len(features))*100:.1f}%)")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # üÜï SMOTE Comparison Results (FIXED VERSION)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        class_balancing_config = CONFIG.get("class_balancing", {})
        COMPARE_SMOTE = class_balancing_config.get("compare_smote", False)

        if COMPARE_SMOTE:
                print(f"\nüîÑ SMOTE COMPARISON:")
                print(f"  ‚Ä¢ Mode: Dual training (SMOTE vs NO-SMOTE)")
                print(f"  ‚Ä¢ Total models trained: {len(results)}")

                # Count versions
                smote_models = sum(1 for k in results.keys() if 'WITH-SMOTE' in k)
                no_smote_models = sum(1 for k in results.keys() if 'NO-SMOTE' in k)
                print(f"  ‚Ä¢ WITH-SMOTE: {smote_models} models")
                print(f"  ‚Ä¢ NO-SMOTE: {no_smote_models} models")

                # ‚úÖ FIX 1: INITIALIZE VARIABLES OUTSIDE IF BLOCKS
                valid_smote = {}
                valid_no_smote = {}
                best_smote = None
                best_no_smote = None
                best_smote_f1 = 0.0
                best_no_smote_f1 = 0.0

                # Best from each version
                smote_results = {k: v for k, v in results.items() if 'WITH-SMOTE' in k}
                no_smote_results = {k: v for k, v in results.items() if 'NO-SMOTE' in k}

                # ‚úÖ FIX 2: NOW VARIABLES ARE ALWAYS DEFINED
                if smote_results:
                        valid_smote = {k: v.get('test_f1', -1) for k, v in smote_results.items() if pd.notna(v.get('test_f1'))}
                        if valid_smote:
                                best_smote = max(valid_smote, key=valid_smote.get)
                                best_smote_f1 = valid_smote[best_smote]
                                print(f"\n  üü¢ Best WITH-SMOTE: {best_smote}")
                                print(f"     Test F1: {best_smote_f1:.4f}")

                if no_smote_results:
                        valid_no_smote = {k: v.get('test_f1', -1) for k, v in no_smote_results.items() if pd.notna(v.get('test_f1'))}
                        if valid_no_smote:
                                best_no_smote = max(valid_no_smote, key=valid_no_smote.get)
                                best_no_smote_f1 = valid_no_smote[best_no_smote]
                                print(f"\n  üî¥ Best NO-SMOTE: {best_no_smote}")
                                print(f"     Test F1: {best_no_smote_f1:.4f}")

                # ‚úÖ FIX 3: SAFE COMPARISON (NOW NO SCOPE ERROR)
                if valid_smote and valid_no_smote:
                        if best_smote_f1 > best_no_smote_f1:
                                diff = best_smote_f1 - best_no_smote_f1
                                print(f"\n  üèÜ OVERALL WINNER: WITH-SMOTE (+{diff:.4f})")
                        elif best_no_smote_f1 > best_smote_f1:
                                diff = best_no_smote_f1 - best_smote_f1
                                print(f"\n  üèÜ OVERALL WINNER: NO-SMOTE (+{diff:.4f})")
                        else:
                                print(f"\n  ü§ù RESULT: TIE (both perform equally)")
                else:
                        print(f"\n  ‚ö†Ô∏è  Cannot compare: Missing SMOTE or NO-SMOTE results")
                        if not valid_smote:
                                print(f"     ‚Ä¢ WITH-SMOTE results: MISSING")
                        if not valid_no_smote:
                                print(f"     ‚Ä¢ NO-SMOTE results: MISSING")

        else:
                # Single mode (unchanged)
                print(f"\nüéØ MODEL PERFORMANCE:")
                valid_models_f1 = {m: results[m].get('test_f1', -1) for m in results.keys() if pd.notna(results[m].get('test_f1'))}
                if valid_models_f1:
                        best_model_name = max(valid_models_f1, key=valid_models_f1.get)
                        print(f"  ‚úÖ Best Model: {best_model_name} (F1 = {valid_models_f1[best_model_name]:.4f})")


        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Output Information
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        print(f"\nüìÅ OUTPUTS:")
        print(f"  ‚úÖ Directory: {OUT_DIR}")
        print(f"  ‚úÖ Graphics: {len(graphics) if 'graphics' in locals() else 'N/A'}")
        print(f"  ‚úÖ Tables: {len(tables) if 'tables' in locals() else 'N/A'}")

        if 'pdf_path' in locals() and pdf_path:
            print(f"  ‚úÖ PDF Report: {os.path.basename(pdf_path)}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # Execution Time
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        end_time = time.time()
        total_time = end_time - start_time
        print(f"\n‚è±Ô∏è  Total Execution Time:")
        print(f"   {total_time:.2f}s ({total_time/60:.2f} minutes)")

        print("\n" + "="*100)
        print("‚úÖ v18.1 EXECUTION COMPLETE - EDA-GUIDED WITH SMOTE COMPARISON")
        print("="*100)

    except Exception as e:  # ‚Üê ‚úÖ EKSƒ∞K OLAN KISIM!
        print(f"\n‚ùå EXECUTION FAILED!")
        print(f"Error Type: {type(e).__name__}")
        print(f"Error Message: {e}\n")

        import traceback
        print("="*100)
        print("FULL ERROR TRACEBACK:")
        print("="*100)
        traceback.print_exc()

        print("\n" + "="*100)
        print("‚ùå v18.1 EXECUTION INCOMPLETE - ERROR OCCURRED")
        print("="*100)
